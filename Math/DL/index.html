
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../SL/">
      
      
        <link rel="next" href="../SageMath/">
      
      
      <link rel="icon" href="../../imgs/zxr.ico">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.16">
    
    
      
        <title>深度学习 - zoeminus</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.bcfcd587.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      

  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 7v2h4l-4 6v2h6v-2h-4l4-6V7H9m3-5a10 10 0 0 1 10 10 10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2Z"/></svg>');}</style>


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="zoeminus" class="md-header__button md-logo" aria-label="zoeminus" data-md-component="logo">
      
  <img src="../../imgs/zx.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            zoeminus
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              深度学习
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/zoeplus/zoeminus" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    zoeminus
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="zoeminus" class="md-nav__button md-logo" aria-label="zoeminus" data-md-component="logo">
      
  <img src="../../imgs/zx.png" alt="logo">

    </a>
    zoeminus
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/zoeplus/zoeminus" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    zoeminus
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    一
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            一
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Set/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    集合论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../R/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    实数理论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../MA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数学分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../LAlg/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    高等代数
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../GTopo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一般拓扑学
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RF/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    实变函数
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../MAlg/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    矩阵代数
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    九
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            九
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Prob/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    概率论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Stat/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数理统计
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    最优化理论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DSA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据结构与算法
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CLT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    计算学习理论
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    二十二
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            二十二
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据挖掘
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SL/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    统计学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    深度学习
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    深度学习
  </span>
  

      </a>
      
        

  

<nav class="md-nav md-nav--secondary" aria-label="本文组织：">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      本文组织：
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      线性神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="线性神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      线性神经网络用于回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="线性神经网络用于回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      线性神经网络用于监督学习：分类
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机
    </span>
  </a>
  
    <nav class="md-nav" aria-label="多层感知机">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      感知机
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      从感知机到多层感知机
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      卷积神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      循环神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="循环神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      关于序列数据预测的一个简单介绍
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      循环神经网络初步
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      自然语言处理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      残差网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      注意力机制
    </span>
  </a>
  
    <nav class="md-nav" aria-label="注意力机制">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      图神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      生成对抗神经网络
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#part-ii-advanced-topics" class="md-nav__link">
    <span class="md-ellipsis">
      Part II: Advanced Topics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Part II: Advanced Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#theory" class="md-nav__link">
    <span class="md-ellipsis">
      Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application" class="md-nav__link">
    <span class="md-ellipsis">
      Application
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-select-hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      How to select hyperparameters?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      附录
    </span>
  </a>
  
    <nav class="md-nav" aria-label="附录">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      数学
    </span>
  </a>
  
    <nav class="md-nav" aria-label="数学">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      微积分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      概率论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      信息论
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    二十
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            二十
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SageMath/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SageMath
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Coding/C.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Coding/CPP/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Coding/Python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    十九
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            十九
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Cookbooks/DLCB/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Cookbooks/PyDCB/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python DM & ML
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    零零五五
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            零零五五
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Divisadero/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    遥望
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Senses/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    感官刺激
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/LucidDreaming.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    清醒的梦
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Falling.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    落
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/WithLLM.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对话
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_6" >
        
          
          <label class="md-nav__link" for="__nav_7_6" id="__nav_7_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    存在主义
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_6">
            <span class="md-nav__icon md-icon"></span>
            存在主义
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Existential/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    存在 I
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/B%26N/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对存在的追求
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  

<nav class="md-nav md-nav--secondary" aria-label="本文组织：">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      本文组织：
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      线性神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="线性神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      线性神经网络用于回归
    </span>
  </a>
  
    <nav class="md-nav" aria-label="线性神经网络用于回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      线性神经网络用于监督学习：分类
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      多层感知机
    </span>
  </a>
  
    <nav class="md-nav" aria-label="多层感知机">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      感知机
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      从感知机到多层感知机
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      卷积神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      循环神经网络
    </span>
  </a>
  
    <nav class="md-nav" aria-label="循环神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      关于序列数据预测的一个简单介绍
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      循环神经网络初步
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      自然语言处理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      残差网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      注意力机制
    </span>
  </a>
  
    <nav class="md-nav" aria-label="注意力机制">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      图神经网络
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      生成对抗神经网络
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#part-ii-advanced-topics" class="md-nav__link">
    <span class="md-ellipsis">
      Part II: Advanced Topics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Part II: Advanced Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#theory" class="md-nav__link">
    <span class="md-ellipsis">
      Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application" class="md-nav__link">
    <span class="md-ellipsis">
      Application
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-select-hyperparameters" class="md-nav__link">
    <span class="md-ellipsis">
      How to select hyperparameters?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      附录
    </span>
  </a>
  
    <nav class="md-nav" aria-label="附录">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      数学
    </span>
  </a>
  
    <nav class="md-nav" aria-label="数学">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      微积分
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      概率论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      信息论
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/zoeplus/zoeminus/commits/main/docs/Math/DL.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


  <h1>深度学习</h1>

<p>从浅显的角度上来说，神经网络是复合映射，可认为每一个神经网络层为一映射，多个网络层对输入值进行多次线性和非线性映射，最终得到输出. </p>
<p>以网络的角度看有很多层，因此称为“深度”；根据数据集，以某指标为方向以某种优化方法不断更新神经网络的模型参数，称为“学习”.</p>
<details class="quote">
<summary>GAN中的一句话，深度学习是<strong>表示学习</strong>（representation learning）</summary>
<p>The promise of deep learning is to discover rich, hierachical models that represent probability distributions over the kinds of data encountered in artificial intelligence applications.</p>
</details>
<h2 id="_1">线性神经网络<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>首先关注<a href="../SL/#监督学习假设及概念">监督学习</a>中预测的两个场景：<strong>回归</strong>和<strong>分类</strong>. </p>
<h3 id="_2">线性神经网络用于回归<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<p><strong>预测器</strong>：神经网络中的预测器是网络，由层和块构成. 其中层（layer）、块（module，由层和块构成）或者网络（net，各种层和块构成）中包含模型参数.</p>
<details class="note">
<summary>PyTorch：加载数据集并将其分割为训练集、验证集和测试集</summary>
<p>读取一个数组对象作为数据集. <a href="../../Cookbooks/DLCB/#^TensorDataset">Check here</a><br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="o">.../.../</span><span class="n">train</span><span class="o">.</span><span class="n">csv</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="o">.../.../</span><span class="n">test</span><span class="o">.</span><span class="n">csv</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">train_data</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 访问数据集中第一个样本的特征</span>

<span class="c1"># 创建迭代器</span>
<span class="n">data_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span> <span class="c1"># 迭代5次，输出样本后停止</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The </span><span class="si">{</span><span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">th batch samples</span><span class="se">\&#39;</span><span class="s1"> features are </span><span class="si">{</span><span class="n">X</span><span class="si">}</span><span class="s1">,</span><span class="se">\n</span><span class="s1"> targets are </span><span class="si">{</span><span class="n">Y</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">break</span>
</code></pre></div><br />
下面将数据集随机分割为训练集、测试集和验证集. <a href="../../Cookbooks/DLCB/#^RandomSplit">Check here</a><br />
^PyTorchDataset</p>
</details>
<details class="abstract">
<summary>概念：回归和线性回归的假设</summary>
<ul>
<li><strong>回归</strong>（regression）：对一个或者多个<u>自变量</u>与一个或者多个<u>因变量</u>之间的关系建模的一类方法. 线性回归（linear regression）最简单并且最流行.</li>
<li><strong>线性回归的假设</strong>：1) 自变量<span class="arithmatex">\(x\)</span>和因变量<span class="arithmatex">\(y\)</span>之间的关系是<strong>线性</strong>的，即<span class="arithmatex">\(y\)</span>可以表示为<span class="arithmatex">\(x\)</span>中元素的<u>加权和</u>，通常允许包含观测值的一些<strong>噪声</strong>（noise）；2) 假设任何噪声都比较“正常”，如噪声遵循正态分布. 3) 关于噪声：大量随机因素的干扰.</li>
</ul>
</details>
<details class="abstract">
<summary>机器学习中的通用概念</summary>
<p>机器学习中有三个比较通用的概念：<strong>模型</strong>、<strong>策略</strong>和<strong>算法</strong>.</p>
<p>简单来说，<u>模型</u>建立的是输入与输出之间的映射；<u>策略</u>检验模型表现的性能，决定模型应该朝什么方向优化；<u>算法</u>则是如何基于策略进行优化的问题.</p>
<p>线性神经网络使用线性回归模型，初始化模型参数之后，通常采用（小批量）随机梯度下降方法优化均方损失函数，从而不断更新模型参数.相应的三个概念为：</p>
<ul>
<li>模型（module, model, net）：线性回归模型</li>
<li>策略（strategy, schedule, criterion）：均方损失函数</li>
<li>算法（optimizer, algorithm, updater）：随机梯度下降</li>
</ul>
</details>
<details class="abstract">
<summary>泛化与过拟合</summary>
<p>在监督学习中，如果一个在训练集上表现良好（计算损失较低）的模型应用于从未进行过训练的数据计算得到的损失也较低，这种现象称为<strong>泛化（generalization）</strong></p>
<p>为理解泛化的概念，首先考虑<br />
^GeneralizationOverfitting</p>
</details>
<details class="abstract">
<summary>线性回归模型</summary>
<p>线性回归模型是一个仿射变换.</p>
<ul>
<li><strong>仿射变换</strong>（affine transformation）：对特征进行<strong>线性变换</strong>（linear transformation，根据权重加权求和），并通过偏置进行<strong>平移</strong>（translation），这里给出的是对于一个二维变量的处理，对张量（三维及以上）的仿射变换类似<span class="arithmatex">\(<span class="arithmatex">\(\hat{y}=w^Tx+b,\quad w\in\mathbb{R}^d,x\in\mathbb{R}^d\)</span>\)</span><br />
或者对于一个大小为<span class="arithmatex">\(n\)</span>的数据集进行作用：<span class="arithmatex">\(<span class="arithmatex">\(\hat{y}=Xw+b,\quad \hat{y}\in\mathbb{R}^n,X\in\mathbb{R}^{n\times d},w\in\mathbb{R}^d\)</span>\)</span></li>
</ul>
</details>
<details class="note">
<summary>PyTorch: 线性模型</summary>
<p><div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;在PyTorch框架下从头搭建一个&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">LinearModule1</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>


<span class="sd">&#39;&#39;&#39;使用PyTorch的nn模块&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">LinearModule2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_nums</span><span class="p">,</span> <span class="n">output_nums</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_nums</span><span class="p">,</span> <span class="n">output_nums</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># 正态分布初始化权重</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="n">LM2</span> <span class="o">=</span> <span class="n">LinearModule2</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 用PyTorch自身的框架创建模型</span>
<span class="n">LM2</span> <span class="c1"># 返回一个模型的简单介绍 LinearModule2( (net): Linear(in_features=4, out_features=2, bias=True) )</span>
<span class="n">LM2</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">LM2</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bias</span> <span class="c1"># 查看模型的参数</span>
<span class="n">LM2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">LM1</span> <span class="o">=</span> <span class="n">LinearModule1</span><span class="p">(</span><span class="n">LM2</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">LM2</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="c1"># 将LM2的模型参数传到LM1中，注意，使用 .data方法传入的Tensor不再设置requires_grad=True</span>
<span class="n">LM1</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">LM1</span><span class="o">.</span><span class="n">weight</span>

<span class="n">LM1</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">==</span> <span class="n">LM2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;nn.Linear(in_features, out_features)是对输入数据的最后一个维度进行计算（与矩阵运算不同）&#39;&#39;&#39;</span>
<span class="n">Seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">LN</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">LN</span><span class="p">(</span><span class="n">Seq</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">LN</span><span class="p">(</span><span class="n">Seq</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div><br />
^PyTorchLinearModel</p>
</details>
<details class="abstract">
<summary>概念：损失函数</summary>
<p>损失函数（loss function, cost function）是度量模型表现好坏的标准，输出数值越大损失越大</p>
</details>
<details class="abstract">
<summary>均方误差损失函数</summary>
<p><strong>均方误差损失函数</strong>（或损失均值）：最常使用，对于真实标签<span class="arithmatex">\(y^{(i)}\)</span>和预测值<span class="arithmatex">\(\hat{y}^{(i)}\)</span>，定义<strong>损失值</strong>为：<span class="arithmatex">\(<span class="arithmatex">\(l^{(i)}(w,b)=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2\)</span>\)</span>其中常数<span class="arithmatex">\(\frac{1}{2}\)</span>能够使得损失函数在求导后常数系数为1，便于计算. 定义均方误差损失函数：<span class="arithmatex">\(<span class="arithmatex">\(L(w,b)=\frac{1}{n}\sum\limits_{i=1}^{n}l^{(i)}(w,b)=\frac{1}{n}\sum\limits_{i=1}^{n}\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2\)</span>\)</span></p>
</details>
<details class="note">
<summary>PyTorch: 均方误差损失函数 <a href="../../Cookbooks/DLCB/#^MSELoss">Check here</a></summary>
<p><div class="highlight"><pre><span></span><code><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span> <span class="c1"># 返回损失平均值</span>
<span class="n">criterion_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span> <span class="c1"># 返回每个预测值对应的损失值</span>
<span class="n">criterion_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span> <span class="c1"># 总损失</span>
</code></pre></div><br />
^PyTorchMSE</p>
</details>
<details class="note">
<summary>理论：损失函数与概率的关系，极大似然估计</summary>
</details>
<p>到这里可以给出从深度学习角度对线性回归（亦称为单层线性神经网络）的解释：</p>
<p>给定训练数据的特征<span class="arithmatex">\(X\)</span>和对应的真实标签<span class="arithmatex">\(y\)</span>，线性回归的目标是找到一组权重向量<span class="arithmatex">\(w\)</span>和偏置<span class="arithmatex">\(b\)</span>：对于从<span class="arithmatex">\(X\)</span>的概率同分布中抽取的新的样本的特征，这组权重向量和偏置能使得新样本预测标签的误差尽可能小.</p>
<p>下面就是怎么选（如何优化损失函数）.</p>
<details class="abstract">
<summary>概念：优化方法（学习策略，或更新器）</summary>
<ul>
<li><strong>解析解</strong>（analytical solution）：线性回归具有解析解：<span class="arithmatex">\(<span class="arithmatex">\(w^*=(X^TX)^{-1}X^Ty\)</span>\)</span></li>
</ul>
<p>在其他情况下，例如引入了非线性函数（如ReLU函数）的[[1-多层感知机]]就没有解析解，这个时候对于损失函数的优化需要使用其他方法，通常用梯度下降，有以下方法.</p>
</details>
<details class="abstract">
<summary>批量梯度下降、随机梯度下降、小批量随机梯度下降</summary>
<ul>
<li>
<p><strong>批量梯度下降</strong>（Batch Gradient Descent, BGD）：计算整个数据集的损失函数<span class="arithmatex">\(L\)</span>的梯度，完全确定性（deterministic）优化.</p>
</li>
<li>
<p><strong>随机梯度下降</strong>（Stochastic gradient descent, SGD）：随机指每次打乱训练集进行训练（具有不确定性，或者称引入噪声），每一次计算一个样本的损失函数<span class="arithmatex">\(L\)</span>的梯度<span class="arithmatex">\(\nabla_{(w,b)} L\)</span>，然后更新参数：<span class="arithmatex">\((w,b)\leftarrow (w,b)-\eta \nabla_{(w,b)} L\)</span>（称为梯度下降，gradient descent），反复重复. </p>
</li>
<li>其中<span class="arithmatex">\(\eta\)</span>为一个提前设定的参数，称为<strong>超参数</strong>（hyper parameter）</li>
<li>注：根据<span class="arithmatex">\(n\)</span>元函数（标量）的<a href="#^Taylor">Taylor公式</a>，在局部沿着负梯度的方向函数值的下降最快，但是“局部”在定义域中的不同点是不同的，设置<span class="arithmatex">\(\eta\)</span>（或称学习率）的目的即为保证局部近似有<span class="arithmatex">\(f(x)=f(x_0)+\nabla f(x_0)\Delta x\)</span>，从而更好地优化.</li>
</ul>
<p>对于大型数据集，随机梯度下降不适用，太慢. 把批量梯度下降和随机梯度下降做一结合：小批量随机梯度下降</p>
<ul>
<li><strong>小批量随机梯度下降</strong>（Batch gradient descent, BSGD 一般把它称为SGD，PyTorch中的SGD也是这个）：在每一次迭代中，随机抽取一个小批量<span class="arithmatex">\(B\)</span>（由固定数量的训练样本组成）进行以下更新：<span class="arithmatex">(<span class="arithmatex">\((w,b)\leftarrow(w,b)-\frac{\eta}{\lvert B \rvert}\sum\limits_{i\in B}\nabla_{(w,b)}l^{(i)}(w,b)\)</span>\)</span>其中：<ul>
<li><span class="arithmatex">\(\lvert B\rvert\)</span>表示批量大小（batch_size，一个批量中包含的样本数量）；<span class="arithmatex">\(\eta\)</span>是提前选定的一个正数，称为学习率（learning rate）.</li>
<li>对于线性回归，可以明确的将随机梯度更新写为以下形式：$$\begin{aligned}</li>
</ul>
</li>
</ul>
</details>
<p>&amp;w\leftarrow w-\frac{\eta}{\lvert B \rvert}\sum\limits_{i\in B}\partial_{w}l^{(i)}(w,b)=w-\frac{\eta}{\lvert B\rvert}\sum\limits_{i\in B}x<sup>{(i)}(w</sup>Tx<sup _i_="(i)">{(i)}+b-y</sup>)\<br />
&amp;b\leftarrow b-\frac{\eta}{\lvert B\rvert}\partial_{b}l^{(i)}(w,b)=b-\frac{\eta}{\lvert B\rvert}\sum\limits_{i\in B}(w<sup _i_="(i)">Tx</sup>)}+b-y^{(i)<br />
\end{aligned}$$</p>
<blockquote>
<ul>
<li>学习率<span class="arithmatex">\(\eta\)</span>和批量大小<span class="arithmatex">\(\lvert B\rvert\)</span>可以调整，但（一般）并不在训练过程中更新. 这种不在训练过程中更新的参数称为<strong>超参数（hyperparameter）</strong>，<strong>调参（hyperparameter tuning）</strong> 是选择超参数的过程. 超参数通常是根据数据迭代结果来调整的，训练迭代结果是在独立的<strong>验证数据集（validation dataset）</strong> 上评估得到的。</li>
</ul>
</blockquote>
<details class="note">
<summary>PyTorch: SGD <a href="../../Cookbooks/DLCB/#^SGD">Check Here</a></summary>
<p>我们在上面已经介绍了PyTorch中的均方损失函数，PyTorch中可以使用<code>torch.optim.SGD</code>来实现随机梯度下降方法，下面给出的一个例子使用一个函数生成了一个噪声玩具数据集，然后用SGD对真实函数进行学习.<br />
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">true_function</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">toy_dataset</span><span class="p">(</span><span class="n">sample_num</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">sample_num</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 这里使用了一个均值为0，方差为3的分布生成噪声</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">sample_num</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">5</span> <span class="c1"># 使用(5,15)上的均匀分布生成样本特征</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">true_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 真值</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">true_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span> <span class="c1"># 标签（观测值）</span>
    <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">TD</span> <span class="o">=</span> <span class="n">toy_dataset</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">6.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span> <span class="c1"># 初始化参数</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span> <span class="c1"># 学习率0.0001（这个学习率并不是随便设置的，我事先打印了一些梯度，观察后设置学习率，否则可能会产生梯度爆炸，或者梯度消失）</span>

<span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">TD</span><span class="p">:</span> <span class="c1"># 训练</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 查看模型参数，我这次返回的是tensor([2.7492, 4.2948], requires_grad=True)，而如果设置样本数量为5000，有一次返回了tensor([2.9938, 4.0046], requires_grad=True)，因为真实函数是凸性的，所以很好拟合.</span>
</code></pre></div><br />
^PyTorchSGD</p>
</details>
<details class="abstract">
<summary>概念：权重衰减（<span class="arithmatex">\(L_2\)</span>正则化）</summary>
<p><strong>权重衰减</strong>（Weight decay），或被称为<span class="arithmatex">\(L_2\)</span>正则化，是一项很常见的技术. 为了提高模型的泛化能力，一般会<u>降低模型的复杂度</u>.</p>
<p>假设函数的模型参数为<span class="arithmatex">\(\mathbf{w}\in \mathbb{R}^{m\times 1}\)</span>，权重衰减使用<strong>惩罚项</strong>（penalty）<span class="arithmatex">\(\lVert \mathbf{w}\rVert^2=\mathbf{w}^T \mathbf{w}\)</span>来度量模型的复杂性，这样做的原因是直观上<span class="arithmatex">\(\lVert \mathbf{w}\rVert^2\)</span>越大，特征在发生微小变化时对结果的影响程度就越大，通常并不符合预期. </p>
<p>在进行优化时，将惩罚项加入损失函数中：<span class="arithmatex">\(<span class="arithmatex">\(L(w,b)+\lambda\lVert w\rVert^2\)</span>\)</span>其中<span class="arithmatex">\(\lambda\)</span>为需要设定的非负超参数.</p>
<ul>
<li>与<span class="arithmatex">\(L_2\)</span>正则化相对应的还有<span class="arithmatex">\(L_1\)</span>正则化<span class="arithmatex">\(\lvert \mathbf{w}\rvert=\sum\limits_{i=1}^{m}\mathbf{w}_i\)</span>，<span class="arithmatex">\(L_2\)</span>相比于<span class="arithmatex">\(L_1\)</span>而言对权重向量<span class="arithmatex">\(\mathbf{w}\)</span>中的大分量的惩罚更大，算法更偏向于选择在大量特征上权重分布均匀的模型. 而<span class="arithmatex">\(L_1\)</span>正则化有时导致模型将权重集中在一小部分特征上，发生<strong>特征选择</strong>（feature selection）<br />
^WeightDecay</li>
</ul>
</details>
<details class="note">
<summary>Momentum</summary>
<p>使用小批量随机梯度下降可以加速计算（计算梯度的次数减少了），而且通过平均梯度还能够减少方差（<strong>???</strong>）</p>
<p><strong>动量法</strong>（Momentum）也能够减少方差，并且在这种情况下计算得到的梯度可以大于小批量随机梯度下降的平均梯度，这样在针对梯度“平缓”或者是局部最优的情况下更容易摆脱.</p>
<p><strong>Momentum</strong>（动量）在计算梯度时使用了过去的梯度，在<span class="arithmatex">\(t+1\)</span>次更新时对第<span class="arithmatex">\(t\)</span>次更新时使用的梯度加权，再加上当前批量计算得到的梯度（也加权）：$$\begin{aligned}</p>
</details>
<p>&amp; m_{t+1}\leftarrow \beta \cdot m_t + (1-\beta)\sum\limits_{i\in B_t}\nabla_{(w,b)}l^{(i)}(w,b)\<br />
&amp; (w,b)\leftarrow (w,b)-\alpha \cdot m_{t-1}<br />
\end{aligned}$$</p>
<blockquote>
<ul>
<li><span class="arithmatex">\(\beta,\alpha\)</span>均为超参数，其中<span class="arithmatex">\(\beta\in[0,1)\)</span> 其中<span class="arithmatex">\(<span class="arithmatex">\(m_{t+1}\leftarrow \beta \cdot m_t+(1-\beta)\sum\limits_{i\in B_t}^{}\nabla{(w,b)}l^{(i)}(w,b)\)</span>\)</span>可以视为平均梯度的一种弱化，这种计算方法也称为<strong>漏平均值</strong>（Leaky average），Momentum还有其他的写法，如：<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
&amp;m_{t+1}\leftarrow \beta m_t+\sum\limits_{i\in B_t}^{}\nabla_{(w,b)}l^{(i)}(w,b)\\
&amp;m_{t+1}\leftarrow \beta m_t+\alpha\sum\limits_{i\in B_t}^{}\nabla_{(w,b)}l^{(i)}(w,b)
\end{aligned}\)</span>\)</span></li>
<li>直观上理解Momentum：首先每一次计算的批量梯度都加权<span class="arithmatex">\(1-\beta\)</span>，其实可以视为学习率，考虑<span class="arithmatex">\(\beta m_t\)</span>这一部分，如果连续的迭代中<span class="arithmatex">\(\beta m_t\)</span>的正负一致，那么梯度更新速度实际上在不断加快，反之，如果在连续的迭代中<span class="arithmatex">\(m_t\)</span>的符号不能确定，那么将会表现为模型的梯度变化幅度缓慢（减小或者增大）.</li>
<li>关于超参数的设置，如果<span class="arithmatex">\(\beta\)</span>设置较小，那么对于过去的依赖弱，反之越接近于平均（这时学习率要设得更小）.</li>
</ul>
<p><strong>Nesterov accelerated momentum</strong>: <span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
&amp;m_{t+1}\leftarrow \beta m_t + (1-\beta)\sum\limits_{i\in B_t}^{}\nabla_{(w,b)}\frac{\partial{l^{(i)}}[(w,b)-\alpha \cdot m_t]}{(w,b)}\\
&amp;(w,b)\leftarrow (w,b)-\alpha\cdot m_{t+1}
\end{aligned}\)</span>\)</span>Nesterov acclerated momentum相对于momentum的区别是，直接使用了momentum中梯度更新之后的参数来计算损失函数的梯度（<strong>???</strong> 有用吗，计算梯度的结果改变吗？）</p>
<p>下面用一段伪代码说明带Momentum的SGD算法（参照PyTorch框架），假设函数为<span class="arithmatex">\(f\)</span>，其第<span class="arithmatex">\(t\)</span>次迭代后的参数为<span class="arithmatex">\(\theta_t\)</span>，第<span class="arithmatex">\(t\)</span>次计算得到的梯度为<span class="arithmatex">\(g_t\)</span>，<span class="arithmatex">\(\lambda\)</span>为权重衰减所置超参数，<span class="arithmatex">\(m_t\)</span>为第<span class="arithmatex">\(t\)</span>次迭代时的Momentum（在未使用Nesterov时也即更新梯度），<span class="arithmatex">\(\mu,\tau\)</span>为应用Momentum方法时设置的两个超参数，<span class="arithmatex">\(\gamma\)</span>为在不使用Momentum下设置的学习率：<br />
<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
&amp;\text{for }t=1\ \text{to }T \text{ do}\\
&amp;\quad g_t\leftarrow \nabla_\theta f(\theta_{t-1})\\
&amp;\quad \text{if } \lambda\neq 0\\
&amp;\quad \quad g_t\leftarrow g_t+\lambda \theta_{t-1}\\
&amp;\quad \text{if } \mu\neq0\\
&amp;\quad \quad \text{if }t&gt;1\\
&amp;\quad \quad \quad m_t\leftarrow \mu m_{t-1}+(1-\tau)g_t\\
&amp;\quad \quad \text{else }\\
&amp;\quad \quad \quad m_t\leftarrow g_t\\
&amp;\quad \quad \text{if } nesterov\\
&amp;\quad \quad \quad g_t\leftarrow g_t+\mu m_t\\
&amp;\quad \quad \text{else }\\
&amp;\quad \quad \quad g_t\leftarrow m_t\\
&amp;\text{else}\\
&amp;\quad \theta_t\leftarrow \theta_{t-1}-\gamma g_t
\end{aligned}\)</span>\)</span><br />
^Momentum</p>
</blockquote>
<details class="note">
<summary>Adam</summary>
<p>有时计算得到的梯度太大（太小），我们希望更新时能更中庸一些. 一种直观的想法是对于梯度进行标准化，假设计算得到的梯度为<span class="arithmatex">\(\text{grad} = \nabla_{(w,b)}L(w,b)\)</span>，标准化得到：<span class="arithmatex">\(<span class="arithmatex">\((w,b)\leftarrow (w,b)-\alpha \frac{\text{grad}}{\sqrt{\lvert \text{grad}\rvert^2}+\epsilon}\)</span>\)</span></p>
<ul>
<li>其中<span class="arithmatex">\(\epsilon\)</span>为一个很小的常数项，用于防止数值上溢或者返回Non值（当分母为<span class="arithmatex">\(0\)</span>时）.</li>
<li>使用这种正则化方法能够限制梯度的移动，但是也带来了一些问题：在接近最优点（或者局部最优时）计算得到的原始梯度可能是小的，但是由于标准化，梯度可能仍然维持在较大的水平上，造成模型参数的更新来回震荡.</li>
</ul>
<p>所以我们仍然需要可调整的梯度，实现方式可以是参照过去的梯度.</p>
<p><strong>Adam</strong>（Adaptive moment estimation）结合了上述提到的Momentum和梯度标准化的概念，首先，基于之前的梯度更新结果，按照Momentum方法计算<span class="arithmatex">\(m_{t+1}\)</span>，并更新标准化需要用到的方差<span class="arithmatex">\(v_{t+1}\)</span>：$$\begin{aligned}</p>
</details>
<p>&amp;m_{t+1}\leftarrow \beta\cdot m_t + (1- \beta)\nabla L_{(w,b)}\<br />
&amp;v_{t+1}\leftarrow \gamma v_t + (1-\gamma) (\nabla_{(w,b)})^2<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(在开始的时候，迭代次数少，因此不需要过小的估计（加权可能是不必要的），随着迭代次数的增加越是需要之前的梯度计算数据作为依据调整当前计算梯度，因此有：\)</span>\)</span>\begin{aligned}<br />
&amp; \widetilde{m}<em t_1="t+1">{t+1}\leftarrow \frac{m</em>\}}{1-\beta^{t+1}<br />
&amp; \widetilde{v}<em t_1="t+1">{t+1}\leftarrow \frac{v</em>}}{1-\gamma^{t+1}<br />
\end{aligned}$$</p>
<blockquote>
<p>最后对梯度（momentum）进行标准化，更新模型参数<span class="arithmatex">\(<span class="arithmatex">\((w,b)\leftarrow (w,b)-\alpha \frac{\widetilde{m}_{t+1}}{\sqrt{\widetilde{v}_{t+1}}+\epsilon}\)</span>\)</span><br />
- 以上<span class="arithmatex">\(\gamma,\beta,\alpha\)</span>均为超参数（<strong>???</strong> 实践中怎么设置?）在Adam相关的论文中，推荐设置<span class="arithmatex">\(\epsilon=1e-9,\beta=0.9,\gamma=0.999\)</span><br />
^Adam</p>
</blockquote>
<details class="note">
<summary>PyTorch: Weight decay, Momentum <a href="../../Cookbooks/DLCB/">Check here</a></summary>
<p>尝试使用一个数据集，用同样的网络和初始参数，MSE作为损失函数，分别使用SGD，SGD + weight dacay, SGD + Momentum优化，更具体的应用具体见这个<a href="#^ExampleCovidCasesPrediction">例子</a>，注意这里没有使用小批量（而且也没有对数据集进行随机打乱，但是由于每次模型初始参数不同，结果还是有差距的），但是在直观上已经能看出优化算法之间的差异</p>
<p>这里用到的数据集<code>Dataset</code>从<a href="#^PyTorchSGD">PyTorch SGD</a>中写的函数<code>toy_dataset()</code>生成的<br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">copy</span> <span class="c1"># 用于克隆模型（不同的内存空间）</span>

<span class="k">def</span> <span class="nf">come_my_SGDs</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="n">SGD</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">SGD_wd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">SGD_m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">SGD_wd</span><span class="p">,</span> <span class="n">SGD_m</span>

<span class="n">net_0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Dataset</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">td</span><span class="p">)</span> <span class="k">for</span> <span class="n">td</span> <span class="ow">in</span> <span class="n">toy_dataset</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">net_0</span><span class="p">)</span> <span class="c1"># 每次复制最开始的网络，保证初始化参数完全相同</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">come_my_SGDs</span><span class="p">(</span><span class="n">net</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Dataset</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># print(net.weight.grad, net.bias.grad)</span>
        <span class="c1"># print(net.weight.data, net.bias.data)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># print(net.weight.data, net.bias.data, &#39;\n&#39;)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 务必注意，否则梯度将会积累，结果将错到离谱</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> next one :)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># 最后我的返回是：</span>
<span class="c1"># tensor([[0.5320, 0.4991]]) tensor([-0.4939])</span>
<span class="c1"># tensor([[3.2596, 3.7796]]) tensor([-0.2124])</span>

<span class="c1">#  next one :)</span>

<span class="c1"># tensor([[0.5320, 0.4991]]) tensor([-0.4939])</span>
<span class="c1"># tensor([[3.2576, 3.7735]]) tensor([-0.2019])</span>

<span class="c1">#  next one :)</span>

<span class="c1"># tensor([[0.5320, 0.4991]]) tensor([-0.4939])</span>
<span class="c1"># tensor([[3.0263, 4.0161]]) tensor([-0.2356])</span>

<span class="c1">#  next one :)</span>
</code></pre></div><br />
需要注意的细节：</p>
<ul>
<li>超参数的设置，这里其实的三个超参中最重要的一个超参是学习率<code>lr</code>，我在设置其为<span class="arithmatex">\(0.1,0.01\)</span>都会出现很大的问题，最后的结果严重偏离真实值（而且这个模型一点都不复杂）</li>
<li>从上方的表现来看似乎是Momentum胜出，但是实际上Momentum也受到学习率的影响，事实上在学习率为<span class="arithmatex">\(0.001\)</span>时，有时三者中Momentum的效果反而不如前两者.</li>
<li>模型参数的初始化也会造成结果上很大的变化</li>
<li>我这里使用的数据集在<span class="arithmatex">\(f=3x_1+4x_2\)</span>上加上以<span class="arithmatex">\(0\)</span>为均值，以<span class="arithmatex">\(3\)</span>为方差的正态分布的噪声得到的，注意真实函数<span class="arithmatex">\(f\)</span>不含参数，返回的结果中，模型的偏置也在减小，但是一个有趣的现象是，即使初始化参数中偏置很小（i.e.<span class="arithmatex">\(0.0160\)</span>），经过训练之后模型的权重很接近<span class="arithmatex">\(f\)</span>的权重，但是偏置却并不如此，反而增大（<span class="arithmatex">\(0.3131, 0.3137, 0.2614\)</span>），<u>思考这是为什么</u>.</li>
</ul>
</details>
<details class="note">
<summary>PyTorch: Adam <a href="../../Cookbooks/DLCB/#^Adam">Check here</a></summary>
<p>Adam中涉及到的超参数有学习率<span class="arithmatex">\(\alpha\)</span>，更新Momentum时设置的<span class="arithmatex">\(\beta\)</span>，更新方差设置的<span class="arithmatex">\(\gamma\)</span>，Adam也可以设置权重衰减所用到的参数<span class="arithmatex">\(\lambda\)</span>.<br />
^PyTorchAdam</p>
</details>
<details class="note">
<summary>Dropout</summary>
<p><mark>State: 需要更多完善</mark></p>
<p>和之前所提到的<a href="#^WeightDecay">权重衰减</a>的思想相同，Dropout也倾向于降低模型的复杂度. 一种观点认为复杂性较低的模型对于输入的微小特征的敏感程度应该低一些，<u>加入一些随机噪声将不会对结果造成很大影响</u>，具备这种性质的模型也称其具有<strong>平滑性</strong>（smoothness）.</p>
<p><strong>Dropout</strong>（暂退法）的想法是，在网络进行前向传播时，<u>在计算后面的层之前向其输入中加入一些噪声</u>，而在神经网络的实际训练过程中，实现注入噪声的方法为丢弃一些神经元（所以称为Dropout）</p>
<p>具体算法如下：首先假设上一层（<span class="arithmatex">\(l\)</span>）的输出值为<span class="arithmatex">\(\mathbf{y}^{(l)}\)</span>，在进行计算之前，从Bernoulli分布中随机抽样一个随机向量<span class="arithmatex">\(\mathbf{r}^{(l)}\)</span>：<span class="arithmatex">\(<span class="arithmatex">\(r_j^{l}\sim \text{Bernoulli}(p)\)</span>\)</span>然后对于输入进行丢弃（置<span class="arithmatex">\(0\)</span>，从而等同于丢弃神经元）：<span class="arithmatex">\(<span class="arithmatex">\(\widetilde{\mathbf{y}}^{(l)}=\mathbf{r}^{(l)}\circ \mathbf{y}^{(l)}\)</span>\)</span>其中<span class="arithmatex">\(\circ\)</span>为按元素计算，接着进行传到下一层（<span class="arithmatex">\(l+1\)</span>）中计算.</p>
<ul>
<li>上面所提到的注入噪声的方法，最直观的方法是在输出上添加一个从均值为<span class="arithmatex">\(0\)</span>的分布中采样的噪声，i.e.一个高斯分布：<span class="arithmatex">\(<span class="arithmatex">\(x'=x+\epsilon, \epsilon\sim \mathcal{N}(0, \sigma^2)\)</span>\)</span>这样做的考虑是使得<span class="arithmatex">\(E[x']=x\)</span>，而在Dropout中，我们也希望实现这种效果，因此会将未被Dropout的值缩放，于是Dropout操作可以写作：$$h'=\left{\begin{aligned}</li>
</ul>
</details>
<p>&amp;0,\quad prob=p (dropout)\<br />
&amp;\frac{h}{1-p}\quad prob=1-p (not dropout)<br />
\end{aligned}\right.$$因为每一层的Dropout是独立进行的，所以每一层的输出值的期望都等同于输入值. 只不过这种方法有一种显然的问题，参数梯度也被放缩了. 为了解决这一问题需要调整学习率. <strong>!!!</strong> 这里还需要理论上的进一步计算：在链式法则下，这样做具体会对梯度的优化造成什么影响，是否在多次Dropout的情况下一个参数的梯度更新的期望等同于未Dropout的情况？</p>
<blockquote>
<ul>
<li>关于Dropout，还有一个问题是是否需要每次迭代都Dropout，如果不Dropout，或者间隔几次迭代Dropout一次会对结果产生什么影响?</li>
</ul>
</blockquote>
<details class="note">
<summary>PyTorch: Dropout <a href="../../Cookbooks/DLCB/#^Dropout">Check here</a></summary>
<p><div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">dropout_net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span> <span class="c1"># 这里设置了两个参数分别表示两个Dropout操作对应的概率</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">d1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">d2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># PyTorch的Dropout操作会在每一次计算的时候进行执行一次（每次被Drop的都是不同的神经元）</span>
<span class="n">dn</span> <span class="o">=</span> <span class="n">dropout_net</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span> <span class="c1"># 为了简便起见设置的概率较低，实际实践中一般设置0.5以上，并且接近1</span>
<span class="n">cn</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># children of the net</span>
<span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">dn</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">children</span><span class="p">():</span> <span class="c1"># 这里要使用dn.net 是因为在写dropout_net类时其.children()只有net，访问的是dn的孙子们</span>
    <span class="n">cn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
<span class="n">cn</span> <span class="c1"># 这里把网络中的各个子层取出来单独分析</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="n">cn</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">X</span><span class="p">)</span> <span class="c1"># 在没有经过Dropout之前，返回的$8$个值是稳定的</span>
<span class="n">cn</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">cn</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">X</span><span class="p">))</span> <span class="c1"># Dropout层会依照概率随机丢弃一些神经元的输出，也就相当于丢弃神经元，多次运行，返回的结果并不相同.</span>

<span class="c1"># 下面查看梯度</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">dn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cn</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">cn</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">cn</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">dn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 多次运行，观察梯度置零的行和列，理解Dropout对链式法则求导的影响.</span>
</code></pre></div><br />
^PyTorchDropout</p>
</details>
<details class="example">
<summary>例子：线性回归通用，但缺陷很大</summary>
<ul>
<li>图像、自然语言、语音、DNA序列都可以转换为向量表示，从而可以用于线性回归.</li>
<li>但是，这样可能会破坏数据本身的结构，比如，如果使用向量表示图像，那么图像原本的<u>空间结构</u>（例如：边缘）就会被破坏，训练效果会很差. 见[[V-Deep Learning#卷积神经网络]].</li>
</ul>
<p>还可以从其他方面看到线性本身就存在的问题：</p>
<ul>
<li><strong>XOR</strong>：线性模型无法拟合XOR<br />
^LinearFlaws</li>
</ul>
</details>
<p>目前介绍了线性神经网络中的所有元素：数据集，预测器（模型），损失函数，优化方法，这些概念是通用的，因而可以给出下面的深度学习（监督学习）一般算法.</p>
<details class="note">
<summary>PyTorch: 训练模型并测试</summary>
<p>神经网络训练和测试的流程基本相同，在PyTorch框架下：<br />
1) <strong>加载数据集</strong>（torch.utils.data）：包括将原数据转化为PyTorch中的Dataset对象，然后将其分割为train_data, validation_data, test_data，将train_data传入DataLoader中，根据已经确定的批量大小，返回一个可迭代对象train_iter用于训练；<br />
2) <strong>定义神经网络模型</strong>（torch.nn.Module）：选择要用到的网络层，定义的网络需要继承torch.nn.Module，并进行初始化，定义forward()方法；<br />
3) <strong>定义损失函数</strong>（torch.nn）：根据问题选择相应的损失函数（例如对于回归问题用MSE，对于分类问题用CrossEntropy）<br />
4) <strong>定义优化方法</strong>（torch.optim.Optimizer）；<br />
5) <strong>训练</strong>：确定训练周期，在每一个训练周期内，网络首先进行前向传播：计算train_iter中每一个批量输出结果，损失函数随后根据数据结果计算网络损失，然后用优化方法对于网络模型进行更新，重复多次；<br />
6) <strong>验证</strong>：用验证集对于模型的性能进行评估，可能会再次返回到训练阶段继续训练，或者进行数据集的特征选取，网络模型、损失函数、优化方法的修改，然后再进行验证，注意验证集不是固定的，会使用交叉验证的方法确保这点；<br />
7) <strong>测试</strong>：测试集上的数据不用于训练，模型从来没有见过，用于测试模型的实际性能.</p>
<p>注意：对于每一批（batch）数据，首先需要清零优化器的梯度<code>optimizer.zero_grad()</code>(<a href="../../Cookbooks/DLCB/#^whyzerograd">Check here</a>)，然后使用<code>loss.backward</code>计算梯度，最后使用<code>optimizer.step()</code>调整模型参数</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>

<span class="n">dataset</span> <span class="o">=</span> 
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">train_iter</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span> <span class="p">,</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># training the model</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="n">train_iter</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">Y_hat</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">Y_hat</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[:]),</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># validate the model</span>
<span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># test the model: the final output</span>
</code></pre></div>
</details>
<details class="example">
<summary>Example: Regreesion with PyTorch：From <a href="https://www.kaggle.com/competitions/ml2023spring-hw1/overview">Lhy_ML 2023-Spring HW01</a></summary>
<ul>
<li><strong>Task Description</strong>: COVID-19 Cases Prediction. Given survey results in the past 3 days in a specific staet in U.S., then <u>predict the percentage of new tested postitive cases in the 3rd day</u></li>
<li><strong>Data</strong>:<ul>
<li>Feature:<ul>
<li>State (35): encoded to one-hot vectors</li>
<li>COVID-like illness (5): cli, ili ...</li>
<li>Behavior indicators (5): wearing_mask ...</li>
<li>Belief indicators (2): belief_mask_effective</li>
<li>Mental indicators (2): worrier_catch_covid ...</li>
<li>Environmental indicators (3): other_masked_public</li>
</ul>
</li>
<li>Target:<ul>
<li><mark>Tested Positive Cases</mark> (1): tested_postitive</li>
</ul>
</li>
</ul>
</li>
<li><strong>Evaluation Metric</strong>: MSE</li>
<li><strong>Hints</strong>:<ul>
<li>Simple: sample code</li>
<li>Medium: Feature selection</li>
<li>Strong: Different optimizers and L2 regularization</li>
<li>Boss: Better feature selection, different model architectures and try more hyperparameters<br />
^ExampleCovidCasesPrediction</li>
</ul>
</li>
</ul>
</details>
<details class="abstract">
<summary>概念：线性回归到神经网络</summary>
<p>下面是一个单层单输出神经网络</p>
<p>![[单层神经网络（单输出）]]</p>
<ul>
<li>输入层中的<strong>输入数</strong>（或者称为<strong>特征维度</strong>，feature dimensionality）为<span class="arithmatex">\(d\)</span>.</li>
<li>网络的<strong>输出</strong>为<span class="arithmatex">\(o_1\)</span>，这里<strong>输出数</strong>为1.</li>
<li>计算神经元</li>
<li><strong>层数</strong>：注意，模型重点放在发生计算的地方，所以通常在计算层数时不考虑输入层。图中层数为1.</li>
<li>每个输入都与每个输出相连，这种变换称为<strong>全连接层（fully-connected layer）</strong>，或者<strong>稠密层（dense layer）</strong>.</li>
</ul>
</details>
<details class="abstract">
<summary>概念：神经元角度看神经网络</summary>
<p>![[线性单元表示]]</p>
</details>
<hr />
<h4 id="_3">线性神经网络用于监督学习：分类<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h4>
<p>分类问题也是监督学习的一个重要问题，样本标签为离散值（回归的样本标签为连续值），但事实上我们可以将分类问题当成回归问题.</p>
<details class="example">
<summary>例子：分类问题</summary>
<ul>
<li>二分类：患病，不患病？</li>
<li>多分类：糖尿病？或心血管疾病？或脑部疾病？...</li>
<li>多标签分类：既有糖尿病，又有心血管疾病？...</li>
</ul>
</details>
<details class="abstract">
<summary>概念：硬类别，软类别</summary>
<ul>
<li>通常，关注<strong>硬类别</strong>，即属于哪个类别. 但有时分析的是<strong>软硬别</strong>的模型：属于每个类别的概率. 具体到判断属于何种类别时，可以取概率值最大的作为预测类别输出（多分类），或者选择概率值前几个最大的作为预测类别输出（多标签分类）.</li>
<li>在这种情况下，分类就可以当作回归来做，预测的是一个概率值（连续值）</li>
</ul>
</details>
<details class="abstract">
<summary>概念：类别的自然顺序，独热编码</summary>
<ul>
<li>类别间有一些<strong>自然顺序</strong>. 例如：<span class="arithmatex">\(<span class="arithmatex">\(\{total noob, noob, amateur, producer, genius\}\)</span>\)</span>似乎，可以对离散值分配这样的值：<span class="arithmatex">\(<span class="arithmatex">\(\{1,2,3,4,5\}\)</span>\)</span>但是又不对，例如total noob和noob之间的差别（分配差值为<span class="arithmatex">\(2-1=1\)</span>）等同于noob和amateur之前的差别（差值也为<span class="arithmatex">\(3-2=1\)</span>）吗？显然分类问题不能这样作为回归问题对待.</li>
<li>对于类别间不存在自然顺序的清醒，可以采取<strong>独热编码</strong>（one-hot encoding）i.e.对于<span class="arithmatex">\(<span class="arithmatex">\(C=\{human, plant, computer\}\)</span>\)</span>可以将其编码为：<span class="arithmatex">\(<span class="arithmatex">\(\mathcal{Y}=\{(1,0,0),(0,1,0),(0,0,1)\}\)</span>\)</span>这样模型的每一个输出值都应该是一个<span class="arithmatex">\(n=\lvert C\rvert\)</span>的向量<span class="arithmatex">\(y\in \mathcal{Y}\)</span>.</li>
</ul>
</details>
<details class="abstract">
<summary>softmax运算</summary>
<ul>
<li>比如说我们现在想要给出一个样本的各个类别的概率预测，那么输出的值应该为一个向量，其中的每一个值表示属于该类别的概率，则所有值的和应该为<span class="arithmatex">\(1\)</span>，softmax运算能够将一个向量转化为满足上述条件的向量：<span class="arithmatex">\(<span class="arithmatex">\(\hat{y}=softmax(\mathcal{o}),\quad where\quad \hat{y_j}=\frac{exp(\mathcal{o_j})}{\sum\limits_{k}^{}exp(\mathcal{o_j})}\)</span>\)</span></li>
<li>softmax之所以称“软最大化”</li>
</ul>
</details>
<details class="note">
<summary>PyTorch: softmax计算 <a href="../../Cookbooks/DLCB/#^Softmax">Check here</a></summary>
</details>
<details class="abstract">
<summary>交叉熵损失函数</summary>
<p>考虑一个分类问题，假设待计算损失的集合为：<span class="arithmatex">\(<span class="arithmatex">\(S=\{(x_i,y_i,\hat{y}_i)\}_{1\leq i\leq n}\)</span>\)</span>其中<span class="arithmatex">\(x_i,y_i,\hat{y}_i\)</span>分别为第<span class="arithmatex">\(i\)</span>个样本的特征、标签和预测. 标签<span class="arithmatex">\(y\)</span>是一个独热编码向量，预测<span class="arithmatex">\(\hat{y}\)</span>则是一个概率向量，表示每个类别的概率.</p>
<p>使用极大似然估计法，并结合独立性假设，给定<span class="arithmatex">\(X\)</span>观测到<span class="arithmatex">\(Y\)</span>的似然（函数）为：<span class="arithmatex">\(<span class="arithmatex">\(P(Y \,|\,X)=\prod_{i=1}^nP(y_{i}\,|\,x_{i})\)</span>\)</span></p>
</details>
<p>下面最大化似然函数，相当于最小化负对数似然：<span class="arithmatex">\(<span class="arithmatex">\(-\log{P(T \,|\,X)}=\sum\limits_{i=1}^{n}-\log{P(y_i\,|\,x_{i})}=\sum\limits_{i=1}^{n}l(y_{i},\hat{y}_{i})\)</span>\)</span><br />
其中，对于任何一对标签<span class="arithmatex">\(y_j\)</span>和模型预测<span class="arithmatex">\(\hat{y}_j\)</span>，损失函数<span class="arithmatex">\(<span class="arithmatex">\(l(y_j,\hat{y}_j)=-\log\prod_{k=1}^{q}\left(\hat{y}^{(k)}_j\right)^{y^{(k)}}=-\sum\limits_{k=1}^{q}y^{(k)}_j\log{\hat{y}_j^{(k)}}\)</span>\)</span></p>
<blockquote>
<p>该损失函数通常被称为<strong>交叉熵损失（cross-entropy loss）</strong></p>
</blockquote>
<details class="note">
<summary>PyTorch: 交叉熵损失 <a href="../../Cookbooks/DLCB/#^CrossEntropyLoss">Check here</a></summary>
<div class="highlight"><pre><span></span><code><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="abstract">
<summary>softmax回归解决分类问题</summary>
<p>计算表达式：$$\begin{aligned}</p>
</details>
<p>&amp;O=WX+b\<br />
&amp;\hat{Y}=softmax(O)\<br />
&amp;X\in\mathbb{R}^{n\times d},W\in\mathbb{R}^{d\times q},b\in\mathbb{R}^{1\times q}\end{aligned}$$</p>
<blockquote>
<p>实际就是先使用了线性回归，然后用softmax处理后作为输出值输出. 并可以进行类别预测<span class="arithmatex">\(<span class="arithmatex">\(c=\mathop{argmax}\limits_{j}\hat{y_j}=\mathop{argmax}\limits_{j}\hat{o_j}\)</span>\)</span></p>
</blockquote>
<details class="note">
<summary>softmax回归的上溢和下溢问题</summary>
<p>对于softmax运算：<span class="arithmatex">\(<span class="arithmatex">\(\hat{y}_j=\frac{\exp(o_j)}{\sum\limits_{k}\exp(o_k)}\)</span>\)</span>存在以下两个问题：</p>
<ul>
<li><strong>上溢</strong>（overflow）：如果<span class="arithmatex">\(o_k\)</span>中的一些数值非常大，<span class="arithmatex">\(\exp(o_k)\)</span>可能大于数据类型所容许的最大数字。这将使分母或者分子变为<code>inf</code>，最后得到的<span class="arithmatex">\(\hat{y}_j\)</span>可能为0,<code>inf</code>或者<code>nan</code>（<strong>???</strong>）；<br />
解决该问题的一个技巧是从所有的<span class="arithmatex">\(o_k\)</span>中减去<span class="arithmatex">\(\text{max}o_k\)</span>：$$\begin{aligned}\hat{y}_j</li>
</ul>
</details>
<p>&amp;=\frac{\exp(o_j-\max(o_k))\exp(\max(o_k))}{\sum\limits_{k}\exp(o_k-\max(o_k))\exp(\max(o_k))}\<br />
  &amp;=\frac{\exp(o_j-\max(o_k))}{\sum\limits_{k}\exp(o_k-\max(o_k))}<br />
  \end{aligned}$$</p>
<blockquote>
<ul>
<li><strong>下溢</strong>（underflow）经过上述针对上溢的处理之后，部分<span class="arithmatex">\(o_j-\max(o_k)\)</span>可能具有较大的负值，从而使得一些<span class="arithmatex">\(\exp(o_j-\max(o_k))\)</span>有接近零的值，使得<span class="arithmatex">\(\log(\hat{y}_i)\)</span>的值为<code>-inf</code>（如果计算时分子的值接近零的话），但实际上，在进行交叉熵计算时：<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
\log(\hat{y}_j)&amp;=\log\left(\frac{\exp(o_j-\max(o_k))}{\sum\limits_{k}\exp(o_k-\max(o_k))}\right)\\
&amp;=\log(\exp(o_j-\max(o_k)))-\log\left(\sum\limits_{k}\exp(o_k-\max(o_k))\right)\\
&amp;=o_j-\max(o_k)-\log\left(\sum\limits_{k}\exp(o_k-\max(o_k))\right)
\end{aligned}\)</span>\)</span>这样，如果不直接计算<span class="arithmatex">\(\hat{y}_i\)</span>，就可以避免数值下溢的问题</li>
</ul>
</blockquote>
<details class="note">
<summary>对于交叉熵损失从概率论角度和信息论角度的解释</summary>
<ul>
<li>概率论角度：上面在介绍交叉熵损失时所计算的：<span class="arithmatex">\(<span class="arithmatex">\(P(y^{(i)}\,|\,x^{(i)})=\prod_{j=1}^{q}\hat{y}_j^{y_i}\)</span>\)</span>的意义是模型给出的预测正确的概率，通常为了便于优化和理论计算转换为负对数，即为交叉熵损失；</li>
</ul>
<p>对于交叉熵损失：也可以写作以下连续形式：<span class="arithmatex">\(<span class="arithmatex">\(L=\int p(x)\log \hat{y}(x)dx\)</span>\)</span></p>
<p>交叉熵损失这一概念来自信息论，对于下面介绍的内容的一个总结就是：交叉熵相当于在已知概率分布<span class="arithmatex">\(P\)</span>的情况下，对于概率分布<span class="arithmatex">\(Q\)</span>编码所需要的信息量.</p>
<ul>
<li>信息论角度：<ul>
<li><strong>熵</strong>：信息论的核心思想是<u>量化数据中的信息内容</u>，在信息论中，该数值被称为分布<span class="arithmatex">\(P\)</span>中的熵（entropy）：<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}&amp;H[P]=\sum\limits_{j}-P(j)\log_2 P(j)\\&amp;H[P]=\sum\limits_{j}-P(j)\ln P(j)\end{aligned}\)</span>\)</span>底数为2的对数表示信息的单位是比特（binary digit，二进制位），底数为<span class="arithmatex">\(e\)</span>的对数表示信息的单位是纳特（nat），一个纳特是<span class="arithmatex">\(\frac{1}{\ln 2}\approx1.44\)</span>比特. </li>
<li>熵所反映的直接信息在于：对于概率越大的事件，携带的信息量很小（已经比较确定其会发生）；对于概率越小的事件，其携带的信息量越大（其发生是不确定的）.或者说，熵是一个随机变量的不确定性的度量，越多的信息会导致越小的熵.</li>
<li><strong>信息量</strong>：如果不能够完全预测每一个事件，可能会感到“惊异”，采用<strong>信息量</strong><span class="arithmatex">\(\log\frac{1}{P(j)}=-\log P(j)\)</span>量化这种惊异程度. 在观察到一个事件<span class="arithmatex">\(j\)</span>时，赋予其（主观）概率<span class="arithmatex">\(P(j)\)</span>，当赋予一个事件较低的概率时，该事件的惊异程度更大，信息量也就更大，从这个角度看，上面定义的熵即为信息量的期望.</li>
<li><strong>交叉熵</strong>：如果将熵<span class="arithmatex">\(H(P)\)</span>理解为“知道真实概率的人所经历的惊异程度”，那么对于交叉熵的定义：<span class="arithmatex">\(<span class="arithmatex">\(H[P,Q]=\sum\limits_{j}^{}-Q(j)\log P(j)\)</span>\)</span>可以理解为“主观概率为<span class="arithmatex">\(Q\)</span>的观察者在看到概率<span class="arithmatex">\(P\)</span>生成的数据时的预期惊异”，当<span class="arithmatex">\(P=Q\)</span>时，交叉熵达到最低.</li>
</ul>
</li>
</ul>
</details>
<details class="abstract">
<summary>分布迁移</summary>
<p>在<a href="#^GeneralizationOverfitting">泛化</a>中已经提到预测器最终学习到的应该是潜在的概率分布. 但实际上会存在这种潜在的概率分布随着时间发生变化的情况，称为<strong>分布迁移</strong></p>
</details>
<hr />
<h3 id="_4">多层感知机<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>这里特别提了一下感知机.</p>
<h4 id="_5">感知机<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>写在前面</summary>
<p>感知机可以视为神经网络的前身，相当于非线性神经元，其中采用梯度下降优化、使用激活函数提升拟合性能和多层感知机一致.</p>
<p>但是感知机这种十分简单的模型和现代深度学习的网络结构设计其实又有很大的区别...</p>
<p>本部分将传统感知机称为I型感知机，然后定义了II型感知机.</p>
</details>
<details class="abstract">
<summary>概念：线性假设的不足</summary>
<p>之前在<a href="#^LinearFlaws">线性神经网络部分</a>中已经提及了线性的不足之处，这里总结一下主要的点：</p>
<ul>
<li>
<p>单调假设：认为一个特征的增大（或者减少）只会使得结果发生单调的变化. 但是：<br />
有可能在某个区间段是单调增的，在另一个区间段是单调减的；</p>
</li>
<li>
<p>单个影响：认为特征的影响是独立于彼此的. 但是：<br />
有可能存在特征之间的制约和协同. 例如两个特征同时增大会对结果造成更大的影响.</p>
</li>
</ul>
</details>
<details class="abstract">
<summary>感知机</summary>
<p><strong>感知机</strong>（perceptron）模型如下：</p>
<ul>
<li>I型：<span class="arithmatex">\(<span class="arithmatex">\(f(x)=\text{sign}(\mathbf{w}\cdot \mathbf{x}+b),\mathbf{x}\in \mathcal{X}\subset\mathbb{R}^n,\mathbf{w}\in \mathbb{R}^n,b\in \mathbb{R},y\in \mathcal{Y}=\{0,1\}\subset \mathbb{R}\)</span>\)</span>其中<span class="arithmatex">\(\text{sign}\)</span>为符号函数.</li>
<li>
<p>II型：<span class="arithmatex">\(<span class="arithmatex">\(f(x)=\text{ReLU}(\mathbf{w}\cdot \mathbf{x}+b),\mathbf{x}\in \mathcal{X}\subset\mathbb{R}^n,\mathbf{w}\in \mathbb{R}^n,b\in \mathbb{R}\)</span>\)</span></p>
</li>
<li>
<p>I型感知机实际上是一个<strong>线性分类器</strong>（linear classifier），其要分类的样本所在的<strong>特征空间</strong>（feature space）为<span class="arithmatex">\(\mathcal{X}\)</span>，从特征空间映射到的输出空间为<span class="arithmatex">\(\mathcal{Y}\)</span>，<span class="arithmatex">\(\mathbf{w}\cdot \mathbf{x}+b=0\)</span>为<span class="arithmatex">\(\mathcal{X}\)</span>中的一个<strong>超平面</strong>（hyperplane）.给定一个训练集<span class="arithmatex">\(TD=\{(\mathbf{x}_1,y_1), \cdots, (\mathbf{x}_N,y_N)\},\mathbf{x}_i\sim \mathcal{D},y_i=\text{label}(\mathbf{x}_i)\)</span>，其中<span class="arithmatex">\(\mathcal{D}\)</span>为一个未知分布，<span class="arithmatex">\(\text{label}\)</span>为真实的标签函数，I型感知机的目的即为从中学习到一个对于特征空间的超平面，将特征空间划分为两部分，使得其对于从未知分布<span class="arithmatex">\(\mathcal{D}\)</span>中生成的新样本具备较好的分类效果.</p>
</li>
<li>
<p>II型感知机则将特征空间<span class="arithmatex">\(\mathcal{X}\)</span>映射到一个新的空间<span class="arithmatex">\(\mathcal{Y}\)</span>中.（与I型感知机对特征空间进行划分对比）</p>
<ul>
<li>对于<span class="arithmatex">\(\mathbf{X}\in \mathbb{R}\)</span>的情形，可以简单地理解为曲线拟合，效果非常差.<br />
^Perceptron</li>
</ul>
</li>
</ul>
</details>
<details class="abstract">
<summary>从感知机（单个神经元）到多个神经元（一个层）</summary>
<p>之前已经说过，感知机<br />
^Layer</p>
</details>
<hr />
<h4 id="_6">从感知机到多层感知机<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<p>感知机使用激活函数克服线性模型的限制了一些线性限制，但是仍有不足. 事实上，一个相当重要的能力就是模型的表示能力，为此需要引入分析理论.</p>
<details class="note">
<summary>通用近似理论</summary>
<p>Sources: <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Wikipedia</a><br />
通用近似理论（Universal approximation theorem）</p>
<details class="note">
<summary>定理：任意宽度情形下的通用近似理论</summary>
<p><span class="arithmatex">\(C(X,\mathbb{R}^m)\)</span>表示从<span class="arithmatex">\(X\subset \mathbb{R}^n\)</span>到<span class="arithmatex">\(\mathbb{R}^m\)</span>的所有连续函数组成的集合，取<span class="arithmatex">\(\sigma\in C(\mathbb{R},\mathbb{R})\)</span>，定义按元素计算<span class="arithmatex">\((\sigma\circ x)_i=\sigma(x_i)\)</span>. <span class="arithmatex">\(\sigma\)</span>不是多项式当且仅当对于任意<span class="arithmatex">\(n\in \mathbb{N},m\in \mathbb{N}\)</span>，紧集<span class="arithmatex">\(K\subset \mathbb{R}^n,f\in C(K,\mathbb{R}),\epsilon&gt;0\)</span>，存在<span class="arithmatex">\(k\in \mathbb{N},A\in \mathbb{R}^{k\times n},b\in \mathbb{R}^k, C\in \mathbb{R}^{m\times k}\)</span>使得：<span class="arithmatex">\(<span class="arithmatex">\(\sup_{x\in K}\lVert f(x)-g(x)\rVert\leq \epsilon\)</span>\)</span>其中<span class="arithmatex">\(g(x)=C\cdot (\sigma\circ (A\cdot x+b))\)</span></p>
<ul>
<li>该定理表明：隐层具有任意宽度（或任意单元数量）的多层感知机能够近似任意函数.</li>
<li>任意宽度情形（arbitrary width case, 1989, Kurt Hornik, Maxwell Stinchcombe, Halbert White）证明，多层前馈神经网络（multilayer feed-forward networks）为通用逼近器，并且Hornik之后也指出：并不是激活函数的选择，而是多层前馈结构本身使得神经网络成为通用逼近器. Moshe Leshno等人指出通用近似性质等价于<strong>非多项式激活函数</strong>（nonpolynomial activation function）</li>
</ul>
</details>
</details>
<details class="note">
<summary>感知机的能力</summary>
<p>感知机模型：<span class="arithmatex">\(<span class="arithmatex">\(f(\mathbf{x})=a(\mathbf{w}\cdot \mathbf{x}+b)\)</span>\)</span></p>
</details>
<details class="abstract">
<summary>概念：多层感知机</summary>
<p><strong>多层感知机</strong>（Multilayer perceptron, MLP）的结构如下（以单隐藏层为例）：$$\begin{aligned}</p>
</details>
<p>&amp;H=\sigma(XW<sup _1_="(1)">{(1)}+b</sup>)\<br />
&amp;O=HW<sup _2_="(2)">{(2)}+b</sup><br />
\end{aligned}$$</p>
<blockquote>
<ul>
<li><span class="arithmatex">\(X\in\mathbb{R}^{n\times d}\)</span>表示<span class="arithmatex">\(n\)</span>个样本的小批量，其中每个样本有<span class="arithmatex">\(d\)</span>个输入特征.</li>
<li><span class="arithmatex">\(H\in\mathbb{R}^{n\times h}\)</span>表示隐藏层的输出，也称为<strong>隐藏表示</strong>（hidden representation），其具有<span class="arithmatex">\(h\)</span>个<strong>隐藏单元</strong>.</li>
<li>隐藏层和输出层都是全连接的，因而有隐藏层权重<span class="arithmatex">\(W^{(1)}\in\mathbb{R}^{d\times h}\)</span>和隐藏层偏置<span class="arithmatex">\(b^{(1)}\in\mathbb{R}^{1\times h}\)</span>，以及输出层权重<span class="arithmatex">\(W^{(2)}\in\mathbb{R}^{h\times q}\)</span>和输出层偏置<span class="arithmatex">\(b^{(2)}\in\mathbb{R}^{1\times q}\)</span></li>
<li>在多层感知机的结构中<span class="arithmatex">\(XW^{(1)}+b^{(1)}\)</span>即为线性神经网络中的仿射变换，在仿射变换之后对于每一个<strong>隐藏单元</strong>应用非线性的<strong>激活函数</strong>（activation function）<span class="arithmatex">\(\sigma\)</span>，<strong>激活函数的输出值或被称为</strong>活性值**（activation）</li>
<li>可以将激活函数视为一个神经网络层，其包含<span class="arithmatex">\(n\)</span>个单元，每个单元分别只与前一个神经网络层的神经元连接，接受该神经元的输出并计算输出活性值.</li>
</ul>
</blockquote>
<details class="abstract">
<summary>概念：块</summary>
<p>之前讨论<a href="#^Perceptron">II型感知机</a>时，一个感知机相当于<u>一个神经元</u>，其接收一个或者多个输入（<span class="arithmatex">\(\mathbf{x}\)</span>），对其进行线性变换<span class="arithmatex">\(\mathbf{w}\cdot \mathbf{x}+b\)</span>，然后进行激活（<span class="arithmatex">\(\text{ReLU}\)</span>等）</p>
<p><a href="#^Layer">随后</a>介绍了多个II型感知机组成形成的<u>层</u>的能力：将特征空间映射到另一个特征空间，给定一个带标签的训练集，多个感知机组成的层可以从中学习到映射.</p>
<p><strong>块</strong>（Module, Block）的概念则更为广泛些，块有许多结构，常见的结构有：</p>
<ul>
<li>顺序结构：块<span class="arithmatex">\(M\)</span>由多个层<span class="arithmatex">\(L_1,L_2,\cdots,L_m\)</span>按照顺序组成，对于输入<span class="arithmatex">\(X\)</span>，按照顺序层对其多次作用以给出输出：<span class="arithmatex">\(<span class="arithmatex">\(L_m(\cdots(L_2(L_1(X)))\cdots)\)</span>\)</span>多层感知机就是一种顺序结构；</li>
<li><a href="#^ResidualLayer">残差结构</a>：数据的传递并不是顺序的. 例如，假设块<span class="arithmatex">\(M\)</span>中接受输入<span class="arithmatex">\(X\)</span>，包含<span class="arithmatex">\(L_1,L_2\)</span>两个层，该块的输出为<span class="arithmatex">\(O=X+L_2(L_1(X))\)</span>，其中<span class="arithmatex">\(X\)</span>为输入的数据，其首先被传递给了<span class="arithmatex">\(L_1\)</span>，然后<span class="arithmatex">\(L_1(X)\)</span>传递给了<span class="arithmatex">\(L_2\)</span>，但在输出时<span class="arithmatex">\(X\)</span>又被传递到输出层中.</li>
<li>循环结构：该块会被反复使用.</li>
</ul>
<p>一个块做的事情和层或者单独的神经元类似：<br />
1) 初始化参数<br />
2) 前向传播阶段：接收输入数据，计算后将输出传递到之后的层；<br />
3) 反向传播阶段：接收来自上游的梯度（见<a href="Graph">计算图</a>），根据链式法则计算梯度并更新块中的参数.<br />
此外这个块和应该能够访问并修改参数.</p>
<p>块这种概念在现代的网络结构中非常常见，很多网络结构的中间部分都是由多个相同或者相似块组成的，多个块接连作用，使得网络学习到更复杂的表示.</p>
</details>
<details class="abstract">
<summary>概念：常见激活函数</summary>
<ul>
<li><strong>ReLU</strong>（Rectified linaer unit，修正线性单元，或被称为整流函数）：<span class="arithmatex">(<span class="arithmatex">\(\text{ReLU}(x)=\max(x,0)\)</span>\)</span><ul>
<li>当输入为<span class="arithmatex">\(0\)</span>时，ReLU不可导，但通常情况下定义输入为<span class="arithmatex">\(0\)</span>时导数为<span class="arithmatex">\(0\)</span>；ReLU的优点在于，进行梯度计算时，导数或为<span class="arithmatex">\(0\)</span>或为<span class="arithmatex">\(1\)</span>，减轻了神经网络的梯度爆炸或梯度消失问题.</li>
<li>ReLU函数有许多变体，例如参数化ReLU（Parameterized ReLU, pReLU）：<span class="arithmatex">\(<span class="arithmatex">\(\text{pReLU}(x)=\max(0,x)+\alpha\min(0,x)\)</span>\)</span></li>
</ul>
</li>
<li><strong>Sigmoid函数</strong>：<span class="arithmatex">(<span class="arithmatex">\(\text{Sigmoid}(x)=\frac{1}{1+\exp(-x)}\)</span>\)</span><ul>
<li>Sigmoid函数将输入变换为区间<span class="arithmatex">\((0,1)\)</span>上的输出，因此特性也被称为<strong>挤压函数</strong>（squashing function）</li>
<li>Sigmoid函数的导数为：<span class="arithmatex">\(<span class="arithmatex">\(\frac{d}{dx}\text{Sigmoid}(x)=\frac{\exp(-x)}{(1+\exp(-x))^2}=\text{Sigmoid}(1)(1-\text{Sigmoid}(x))\)</span>\)</span></li>
</ul>
</li>
<li><strong>tanh函数</strong>：<span class="arithmatex">(<span class="arithmatex">\(\tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}=2\text{Sigmoid}(2x)-1\)</span>\)</span><ul>
<li>正如上面公式表达的，tanh函数实际即为将Sigmoid函数放大<span class="arithmatex">\(2\)</span>倍之后向下平移<span class="arithmatex">\(1\)</span>个单位，其将输入压缩到<span class="arithmatex">\((-1,1)\)</span>，并且函数关于原点对称；</li>
<li>tanh函数的导数为：<span class="arithmatex">\(<span class="arithmatex">\(\frac{d}{dx}\text{tanh}(x)=1-\tanh^2(x)\)</span>\)</span></li>
</ul>
</li>
</ul>
</details>
<details class="note">
<summary>PyTorch: ReLU, Sigmoid, Tanh [[1-多层感知机#ReLU函数]]</summary>
<p>下面用到的<code>.backward()</code>中传入了一个Tensor作为参数，可以在这里理解<a href="../../Cookbooks/DLCB/#^Autograd">Check here</a><br />
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">f</span>
<span class="sd">&#39;&#39;&#39;绘制函数及其导数&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">plot_functions</span><span class="p">(</span><span class="nb">range</span><span class="p">,</span> <span class="n">functions</span><span class="p">,</span> <span class="n">titles</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="nb">range</span><span class="p">,</span> <span class="nb">range</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">__</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">functions</span><span class="p">),</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">functions</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">functions</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1"># 画图的时候注意把梯度清零了，因为多次backward()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;C:</span><span class="se">\\</span><span class="s1">Users</span><span class="se">\\</span><span class="s1">24696</span><span class="se">\\</span><span class="s1">Desktop</span><span class="se">\\</span><span class="s1">images.png&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_functions</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Sigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;Tanh&#39;</span><span class="p">,</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">])</span>

<span class="sd">&#39;&#39;&#39;PyTorch中实现激活函数有三种方法，下面以ReLU为例&#39;&#39;&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># torch.sigmoid 只是一个函数</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="c1"># nn.Sigmoid 是一个类，继承了nn.Module</span>
<span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">fSigmoid</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">relu</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fSigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</code></pre></div><br />
还需要完善，<code>f.relu</code>和<code>relu</code>（以及其他函数）有什么区别？<br />
<a href="https://discuss.pytorch.org/t/torch-nn-sigmoid-vs-torch-sigmoid/57691/3">PyTorch Discuss</a></p>
</details>
<details class="note">
<summary>PyTorch: 多层感知机实现</summary>
<p>在PyTorch框架下近乎无脑的简单. 这里使用Fashion-MNIST数据集 <a href="DLCB">[Check here]</a><br />
<div class="highlight"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Flattern</span><span class="p">(),</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
                    <span class="p">)</span>
<span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="tip">
<summary>计算图 &amp; PyTorch自动微分</summary>
<p>（参考一下Mathematica的<code>TreeForm</code>）</p>
<ul>
<li>注1：不管理论分析还是PyTorch的实践环节，实际上不太需要<strong>计算图</strong>（Computational Graph）和<strong>自动微分</strong>（Autograd），但是个人这个部分十分基础，没有完全理解计算图就不会真正理解梯度更新，对PyTorch核心中的自动微分也不可能理解. 因此这部分写在了本篇中，<a href="../../Cookbooks/DLCB/#^Autograd">DLCB</a>中也涉及这方面的内容.</li>
<li>注2：本部分主要结合PyTorch框架说明计算图的概念，为了不与PyTorch的概念产生冲突使用了一些PyTorch中的术语.</li>
</ul>
<p>计算图是一种<strong>图结构</strong>，其主要的元素有：</p>
<ul>
<li>叶节点（leave）&lt;-&gt; 输入数据；</li>
<li>局部节点（local nodes）&lt;-&gt; 中间数据；</li>
<li>运算节点（operations）&lt;-&gt; 运算操作</li>
<li>根节点（root）&lt;-&gt; 输出数据；</li>
</ul>
<p>计算图中包含两种流动：<u>数据流动</u>和<u>梯度流动</u>，分别对应前向传播和反向传播.</p>
<ul>
<li><strong>前向传播：数据流动</strong>：在前向传播阶段，输入数据通过运算操作传递到中间数据，中间数据接着通过运算操作传递到下一个中间数据（如果有的话），直至最终产生输出数据.<ul>
<li>这个过程<u>构建了一个计算图</u>，从叶节点出发通过运算节点生成局部节点，局部节点通过运算节点再生成局部节点，最终生成根节点.</li>
<li>规定计算图生成的方向为正，运算节点的负向的节点称为其<strong>下游</strong>（upstream），运算节点的正向的节点称为其<strong>上游</strong>；</li>
</ul>
</li>
<li><strong>反向传播：梯度流动</strong>：在反向传播阶段，<u>按照与计算图生成相反的方向</u>，计算<u>每个</u>运算节点的上游相对于其下游的梯度，然后从<u>根节点</u>出发，按照链式法则，利用已经计算好的梯度，计算根节点相对于所有节点的梯度. 完成一次梯度的流动.</li>
<li>PyTorch：在前向传播阶段，给定输入<code>Tensor</code>，按照要求的运算逐步计算，并保留每一个运算的<strong>梯度函数</strong>（<code>grad_fn</code>）；在反向传播阶段，首先根据<code>grad_fn</code>计算对应的梯度，然后通过<code>Tensor</code>的<code>.grad</code>属性访问计算得到的梯度</li>
</ul>
<p><div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="n">a</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">isleaf</span> <span class="c1"># False, True, True</span>
<span class="n">c</span><span class="o">.</span><span class="n">grad_fn</span> <span class="c1"># &lt;MulBackward0 object at 0x0000017BE5B7FC40&gt;</span>
<span class="n">c</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">12.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
<span class="c1"># e.backward() you an not access c.grad() because it is not a leaf, however you can set c.retain_grad() to get its grad</span>
<span class="n">c</span> <span class="o">+=</span> <span class="mf">1.</span> <span class="c1"># this is an inplace function, which will change c&#39;s version</span>
<span class="n">c</span><span class="o">.</span><span class="n">_version</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">_version</span> <span class="c1"># 1,0</span>
<span class="c1"># e.backward() </span>
<span class="c1"># calls into the C++ engine to run the backward pass</span>
<span class="c1"># RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</span>
</code></pre></div><br />
下面这个例子讨论的是<code>grad_fn</code>的标号<br />
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span> <span class="c1">#　b,c,d are not leaves</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
</code></pre></div><br />
下面介绍如何在不销毁计算图的情况下将变量从计算图中剥离出来<br />
<div class="highlight"><pre><span></span><code><span class="n">k</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">k</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">k</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">k</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div></p>
<p>考虑一个向量值函数<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{y}=F(\mathbf{x}),\mathbf{x}\in \mathbb{R}^n, \mathbb{R}^m\)</span>\)</span>的Jacobian矩阵：<span class="arithmatex">\(<span class="arithmatex">\(J=\begin{bmatrix}\frac{\partial{\mathbf{y}}}{\partial{x_1}} &amp; \cdots &amp; \frac{\partial{\mathbf{y}}}{\partial{}x_n}\end{bmatrix}=\begin{bmatrix}\frac{\partial{y_1}}{\partial{x_1}}&amp; \cdots &amp; \frac{\partial{y_1}}{\partial{x_n}} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial{y_m}}{\partial{x_1}} &amp; \cdots &amp; \frac{\partial{y_m}}{\partial x_n} \end{bmatrix}\)</span>\)</span></p>
<p>PyTorch中自动微分计算的实际上是<span class="arithmatex">\(\mathbf{y}=F(\mathbf{x})\)</span>的Jacobian矩阵与一个向量<span class="arithmatex">\(v\in \mathbb{R}^m\)</span>的乘积（<code>v.shape==y.shape</code>）之所以这样设置是为了依据链式法则计算最终的标量函数对于每个变量的梯度. </p>
<p>例如，计算<span class="arithmatex">\(l=g(\mathbf{y})\)</span>，依据链式法则：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{l}}{\partial{\mathbf{x}}}=\begin{bmatrix}\frac{\partial{l}}{\partial{x_1}}\\ \vdots \\ \frac{\partial{l}}{\partial{x_n}}\end{bmatrix}=\left(\frac{\partial{l}}{\partial{\mathbf{y}}}\right)^T \frac{\partial{\mathbf{y}}}{\partial{\mathbf{x}}}=\begin{bmatrix}\frac{\partial{l}}{\partial{y_1}} &amp; \cdots &amp; \frac{\partial{l}}{\partial{y_m}}\end{bmatrix}\cdot J\)</span>\)</span><br />
对应于上面提到的，这里<span class="arithmatex">\(v=...\)</span></p>
<p>在PyTorch中的实现<br />
<div class="highlight"><pre><span></span><code><span class="n">n</span><span class="o">=</span><span class="mi">5</span>
<span class="n">m</span><span class="o">=</span><span class="mi">4</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">),</span><span class="n">X</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div><br />
^Graph</p>
</details>
<details class="abstract">
<summary>概念：梯度爆炸，梯度消失</summary>
<p>考虑具有<span class="arithmatex">\(L\)</span>层，输入为<span class="arithmatex">\(x\)</span>，输出为<span class="arithmatex">\(o\)</span>，每一个层<span class="arithmatex">\(l_i(i=1,2,\cdots,L)\)</span>对应一个变换<span class="arithmatex">\(f_i\)</span>，则可以将网络表示为：<span class="arithmatex">\(<span class="arithmatex">\(o=f_L\circ f_{L-1}\circ\cdots\circ f_1(x)\)</span>\)</span><br />
从而对于任何一层的参数<span class="arithmatex">\(\phi_i\)</span>，根据链式法则：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{o}}{\partial{W^{(l)}}}=\frac{\partial{o}}{\partial{h^{L-1}}}\frac{\partial{h^{L-1}}}{\partial{h^{L-2}}}\cdots\frac{\partial{h^{(l)}}}{\partial{W^{(l)}}}\)</span>\)</span><br />
该梯度实际上为向量<span class="arithmatex">\(v\)</span>和<span class="arithmatex">\(L-l\)</span>个矩阵的乘积，很容易产生梯度爆炸或者梯度消失，甚至是数值上下溢.</p>
<p>应对这种情况的一种方法是使用对数表示，但这只能减轻梯度爆炸的影响，对于梯度消失问题则不可取.</p>
<p>此外，激活函数在反向传播时的梯度如果过小也会使得梯度消失：Sigmoid, Tanh在输入值的绝对值很大时反向传播时的梯度很小，且非常快地接近<span class="arithmatex">\(0\)</span>. 这点在一些处理中将会带来很大的问题，例如应用于[GAN]时.</p>
<p>因此，激活函数的选择也十分重要，目前更稳定的ReLU系列函数已经成为默认的选择.</p>
</details>
<details class="note">
<summary>ReLU系列激活函数</summary>
<p>ReLU函数的稳定性体现在相比于Sigmoid和Tanh不会产生输出值较大时梯度消失的情况，并且无需计算梯度（总为<span class="arithmatex">\(1\)</span>或<span class="arithmatex">\(0\)</span>）. 但是因为其负数部分总为<span class="arithmatex">\(0\)</span>，在实践中没有很大理由这样做.</p>
<ul>
<li><strong>LeakReLU</strong>: $$\text{LeakReLU}(x)=\left{\begin{aligned}</li>
</ul>
</details>
<p>&amp;x,\quad x&gt;0\<br />
&amp;x*k,\quad x\leq 0<br />
\end{aligned}\right.<span class="arithmatex">\(<span class="arithmatex">\(其中\)</span>k\)</span>为超参数，或者通过学习得到.</p>
<blockquote>
<ul>
<li><strong>ELU</strong>：<span class="arithmatex">\(<span class="arithmatex">\(\text{ELU}(x)=\left\{\begin{aligned}
&amp;x,\quad x&gt;0\\
&amp;\alpha(e^x-1),\quad x\leq 0
\end{aligned}\right.\)</span>\)</span></li>
</ul>
</blockquote>
<details class="note">
<summary>概念：对称性</summary>
</details>
<details class="note">
<summary>参数初始化 | Xavier初始化</summary>
<p>下面从经过线性变化之后输出的分布的变化这一角度说明为什么参数初始化是必要的.</p>
<p>假设一个全连接层有输入<span class="arithmatex">\(X=[x_{k}],k=1,2,\cdots,n\)</span>以及权重<span class="arithmatex">\(W=[w_{ij}]_{m\times n}\)</span>，输出为<span class="arithmatex">\(O=WX=[o_1,o_2,\cdots,o_m]\)</span>，那么<span class="arithmatex">\(<span class="arithmatex">\(o_i=\sum\limits_{j=1}^{n}w_{ij}x_j\)</span>\)</span><br />
假设权重<span class="arithmatex">\(W\)</span>的各个分量<span class="arithmatex">\(w_{ij}\)</span>独立同分布于<span class="arithmatex">\(\mathcal{D}_1\)</span>，<span class="arithmatex">\(\mathcal{D}_1\)</span>具有零均值和方差<span class="arithmatex">\(\sigma^2\)</span>，假设该层输入<span class="arithmatex">\(X\)</span>的各个分量<span class="arithmatex">\(x_k\)</span>独立同分布于<span class="arithmatex">\(\mathcal{D}_2\)</span>，<span class="arithmatex">\(\mathcal{D}_2\)</span>具有零均值和方差<span class="arithmatex">\(\gamma^2\)</span>，<span class="arithmatex">\(x_k\)</span>和<span class="arithmatex">\(w_{ij}\)</span>独立（<span class="arithmatex">\(k,j=1,\cdots,n;i=1,\cdots,m\)</span>）则有：$$\begin{aligned}E[o_i]</p>
</details>
<p>&amp;=\sum\limits_{j=1}^{n}E[w_{ij}x_j]\<br />
&amp;=\sum\limits_{j=1}^{n}E[w_{ij}]E[x_j]\<br />
&amp;=0,\<br />
Var[o_i]&amp;=E[o_i<sup>2]-(E[o_i])</sup>2\<br />
&amp;=\sum\limits_{j=1}<sup>{n}E[w_{ij}</sup>2x_{j}^2]-0\<br />
&amp;=\sum\limits_{j=1}<sup>{n}E[w_{ij}</sup>2]E[x_j^2]\<br />
&amp;=n\sigma<sup>2\gamma</sup>2<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(在上面的假设下为了保持输出的方差不变，应当设置：\)</span>\)</span>n \sigma^2=1$$如果不这样做，输出的方差过大或者过小，会造成在反向传播过程中梯度爆炸或者梯度消失的问题.</p>
<blockquote>
<p>在反向传播环节：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{l}}{\partial{W}}=\)</span>\)</span>可得保持梯度的方差不变的条件为<span class="arithmatex">\(<span class="arithmatex">\(m\sigma^2=0\)</span>\)</span>综合考虑，取<span class="arithmatex">\(<span class="arithmatex">\(\frac{1}{2}(n+m)\sigma^2=1\)</span>\)</span>从而得到：<span class="arithmatex">\(<span class="arithmatex">\(\sigma=\sqrt{\frac{2}{n+m}}\)</span>\)</span></p>
<p>Xavier从均值为<span class="arithmatex">\(0\)</span>，方差<span class="arithmatex">\(\sigma^2=\frac{2}{n+m}\)</span>的Gaussian分布中初始化权重；此外Xavier也可改为从均值为<span class="arithmatex">\(0\)</span>，方差为<span class="arithmatex">\(\sigma^2=\frac{2}{n+m}\)</span>的均匀分布中初始化权重：<span class="arithmatex">\(<span class="arithmatex">\(U\left(-\sqrt{\frac{6}{n_{in}+n_{out}}},\sqrt{\frac{6}{n_{in}+n_{out}}}\right)\)</span>\)</span>（因为<span class="arithmatex">\(U(-a,a)\)</span>的方差为<span class="arithmatex">\(\frac{a^2}{3}\)</span>）</p>
<ul>
<li>在上面分析中的假设都过分简单，但是实践证明Xavier初始化方法确实是有效的.</li>
<li><strong>???</strong> 本部分需要更多理论分析<br />
^Xavier</li>
</ul>
</blockquote>
<details class="note">
<summary>PyTorch: Xavier <a href="../../Cookbooks/DLCB/#^init">Check here</a></summary>
</details>
<details class="note">
<summary>启发式参数初始化</summary>
</details>
<details class="note">
<summary>参数初始化和激活函数的结合</summary>
</details>
<details class="note">
<summary>Paper: Understanding the difficulty of training deep feedforward neural networks</summary>
</details>
<hr />
<h3 id="_7">卷积神经网络<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<p>对于卷积神经网络（Convolutional Neural Network, CNN）可以在<a href="https://poloclub.github.io/cnn-explainer/">这里</a>有一个直观的认识. (in case that website unvailable, <a href="https://www.youtube.com/watch?v=HnWIHWFbuUQ">check here</a>)</p>
<p><a href="https://poloclub.github.io/">poloclub</a></p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">Convolution Arithmetic</a></p>
<details class="abstract">
<summary>概念：图像的性质</summary>
<p>对于图像辨别问题，我们的模型应当考虑到图像的如下性质：</p>
<ul>
<li><strong>局部性质</strong>：在某些情况下，一张图片中局部的图像与距离其较远的图像没有很大关联，这种情况下很远处的图像不应当有较大的影响</li>
<li><strong>空间不变性</strong>：局部区域内的图像如果出现在其他位置，那么两者应该看起来是一样的，例如对于一个眼睛，其经过平移之后，模型识别时输出的结果应该相差不多.</li>
<li><strong>像素相邻性质</strong>：像素之间的相邻关系（上下左右）不可忽视.</li>
</ul>
</details>
<details class="abstract">
<summary>概念：二维卷积</summary>
<p>考虑之前提到的图像的性质，提出进行进行卷积操作，对于原始输入<span class="arithmatex">\(X\)</span>的第<span class="arithmatex">\((i,j)\)</span>位置，矩阵<span class="arithmatex">\(V\)</span>在<span class="arithmatex">\(X\)</span>上的<strong>卷积</strong>（convolution）操作为：<span class="arithmatex">\(<span class="arithmatex">\(H_{i,j}=\text{bias}+\sum\limits_{a=-\Delta}^{\Delta}\sum\limits_{b=-\Delta}^{\Delta}V_{a,b}X_{i+a,j+b}\)</span>\)</span>称<span class="arithmatex">\(V\)</span>为<strong>卷积核</strong>.</p>
<ul>
<li>卷积：定义<span class="arithmatex">\(f,g:\mathbb{R}^d\rightarrow\mathbb{R}\)</span>的运算<span class="arithmatex">\(f*g\)</span>：<span class="arithmatex">\(<span class="arithmatex">\((f*g)(x)=\int f(\mathbf{z})g(\mathbf{x}-\mathbf{z})d \mathbf{z}\)</span>\)</span>离散情形则为：<span class="arithmatex">\(<span class="arithmatex">\((f*g)(i)=\sum\limits_{a}^{}f(a)g(i-a)\)</span>\)</span>对于二维张量：<span class="arithmatex">\(<span class="arithmatex">\(f(*g)(i,j)=\sum\limits_{a}^{}\sum\limits_{b}^{}f(a,b)g(i-a,j-b)\)</span>\)</span></li>
<li>在实际运算中，上面的运算与<span class="arithmatex">\(<span class="arithmatex">\(H_{i,j}=\text{bias}+\sum\limits_{a=0}^{\Delta}\sum\limits_{b=0}^{\Delta}V_{a,b}X_{i+a,j+b}\)</span>\)</span>等价，通常使用该运算来表达卷积运算，这一运算并不是严格意义上的卷积，而称<strong>互相关运算</strong>（cross-correlation），深度学习习惯上称此为卷积.</li>
<li>对于一个<span class="arithmatex">\(V\in \mathbb{R}^{m\times n}\)</span>的卷积核对于<span class="arithmatex">\(X\in \mathbb{R}^{h\times w}\)</span>的矩阵进行卷积，所返回的输出值的大小为<span class="arithmatex">\(Y\in \mathbb{R}^{(h-m)+1\times (w-n)+1}\)</span>.</li>
</ul>
</details>
<details class="note">
<summary>PyTorch: 卷积运算</summary>
<p>上面提到的卷积运算在PyTorch中可以表示为一个输入通道为<span class="arithmatex">\(1\)</span>，输出通道为<span class="arithmatex">\(1\)</span>，填充为<span class="arithmatex">\(1\)</span>，步幅为<span class="arithmatex">\(1\)</span>的的卷积层（概念见下）<br />
<div class="highlight"><pre><span></span><code><span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">conv2d</span><span class="o">.</span><span class="n">weight</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 对应输入通道为1，输出通道为1</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">Z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">conv2d</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># 每一个参数获得的梯度均为16</span>
</code></pre></div><br />
下面实现的卷积运算将二维数据展开之后，利用转换后的卷积核进行运算.<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>一维卷积和三维卷积</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n">stackexchange</a></p>
</details>
<details class="abstract">
<summary>概念：多通道</summary>
<ul>
<li><strong>颜色通道</strong>（color channel）：图像一般包括三个通道（RGB），应该视作一个三维张量（<span class="arithmatex">\(channels\times height\times weight\)</span>）</li>
<li><strong>通道</strong>（channel）：卷积中所提到的通道推广了颜色通道的概念，比如说对于一个具有3个颜色通道的图像，可以将其输出为10个通道（在这种情况下卷积曾的大小将为<span class="arithmatex">\(10\times height\times weight\)</span>，可以视为<span class="arithmatex">\(10\)</span>个卷积核在单独作用和学习），这些通道中的一些通道在训练之后可能成为边缘检测通道，有些则是颜色通道，这些通道会再传入下一层（也是多个通道）.</li>
<li>到这里重新定义卷积层：<span class="arithmatex">\(<span class="arithmatex">\([H]_{c,i,j}=\sum\limits_{s=-\Delta}^{\Delta}\sum\limits_{t=-\Delta}^{\Delta}[K]_{c,s,t}[X]_{i+s,j+t}\)</span>\)</span>表示隐层<span class="arithmatex">\(H\)</span>的第<span class="arithmatex">\(c\)</span>个通道的<span class="arithmatex">\((i,j)\)</span>位置将由卷积核与<span class="arithmatex">\(X\)</span>的相应位置进行互运算得到.</li>
</ul>
</details>
<details class="note">
<summary>PyTorch: 卷积核学习</summary>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span> <span class="c1"># batch_size, in_channel, height, width</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
<span class="n">Y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Y</span><span class="p">[:,</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span> <span class="c1"># batch_size, output_channel, height, width</span>

<span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># 1,1 分别对应 in_channels, out_channels</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">Y_hat</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">l</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_hat</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">conv2d</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">conv2d</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:]</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">conv2d</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">, loss </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="abstract">
<summary>概念：特征映射和感受野</summary>
<ul>
<li><strong>特征映射</strong>（feature map）：卷积层的另一种说法，一个层到另一个层的映射，通常会称一个卷积核为一个特征映射；</li>
<li><strong>感受野</strong>（receptive field）：在前向传播过程中可能影响<span class="arithmatex">\(x\)</span>的所有元素.<ul>
<li>随着卷积层的增多，之后的神经元的感受野可能会越来越大.</li>
</ul>
</li>
</ul>
</details>
<details class="abstract">
<summary>概念：填充和步幅</summary>
<p>在没有填充和步幅的概念之前，对于（二维情形）形状为<span class="arithmatex">\(n_h\times n_w\)</span>的输入，卷积核形状为<span class="arithmatex">\(k_h\times k_w\)</span>，则输出形状为：<span class="arithmatex">\(<span class="arithmatex">\((n_h-k_h+1)\times(n_w-k_w+1)\)</span>\)</span>这存在一些问题，例如对于一个<span class="arithmatex">\(300\times300\)</span>像素的图像输入，那么经过<span class="arithmatex">\(10\)</span>层<span class="arithmatex">\(5\times5\)</span>的卷积层之后大小将会降低到<span class="arithmatex">\(260\times 260\)</span>，会造成边界许多有用的信息丢失（边界的像素值将会被更少次地用于卷积核运算）. <a href="https://www.quora.com/What-is-the-advantage-of-using-padding-in-CNN">Check here</a></p>
<ul>
<li><strong>填充</strong>（padding）：添加<span class="arithmatex">\(p_h,p_w\)</span>行和列的填充，输出形状为：<span class="arithmatex">(<span class="arithmatex">\((n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)\)</span>\)</span>设置<span class="arithmatex">\(p_h=k_h-1,p_w=k_w-1\)</span>可以使得输入与输出具有相同的高度和宽度.<ul>
<li>注意<span class="arithmatex">\(k_h,k_w\)</span>的奇偶性还会影响具体填充（上下，左右）的方式，一般为了保证上下、左右填充的行或列的数量相同都会选择卷积核的高度，宽度为奇数（对二维）；</li>
</ul>
</li>
</ul>
<p>步幅做的事情和填充相反，其加速减小图像输出的大小，一些可能的原因：1) 减小存储；2) 小的步幅重复程度太高 <a href="https://www.quora.com/Why-would-one-use-larger-strides-in-convolutional-NNs-as-opposed-to-smaller-strides">Check here</a></p>
<ul>
<li><strong>步幅</strong>（stride）：使用垂直和水平步幅分别为<span class="arithmatex">\(s_h,s_w\)</span>的卷积核，连同上面的填充假设，输出形状为：<span class="arithmatex">\(<span class="arithmatex">\([(n_h-k_h+p_h)/s_h+1]\times[(n_w-k_w+p_w)/s_w+1]\)</span>\)</span>如果设置<span class="arithmatex">\(p_h=k_h-1,p_w=k_w-1\)</span>，则输出形状将简化为：<span class="arithmatex">\(<span class="arithmatex">\([(n_h-1)/s_h+1]\times[(n_w-1)/s_w+1]\)</span>\)</span></li>
</ul>
</details>
<details class="note">
<summary>PyTorch: 带填充和步幅的二维卷积层</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</code></pre></div>
</details>
<details class="abstract">
<summary>概念：池化</summary>
<p><strong>池化核</strong>：一个固定形状的窗口（类似于卷积核），根据给定的步幅大小移动并给出输出，池化核不含参数，其所给出的输出为：</p>
<ul>
<li>池化核中所有元素的最大值，称为<strong>最大池化</strong>（maximum pooling）；</li>
<li>池化核中所有元素的平均值，称为<strong>平均池化</strong>（average pooling）；<ul>
<li>注：平均池化可以也可以推广为带参数的卷积核：<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{k}=(1/l)_{h\times s}\)</span>\)</span>，在这种情况下核运算也称为<strong>下采样</strong>（downsample），相当于每<span class="arithmatex">\(10l\)</span>个像素在池化之后只占据<span class="arithmatex">\(10\)</span>个像素.</li>
</ul>
</li>
<li>如果池化核形状为<span class="arithmatex">\(p\times q\)</span>，则称其进行的运算为<span class="arithmatex">\(p\times q\)</span><strong>池化</strong>；</li>
<li>类似卷积核，池化核也可以设置<strong>填充</strong>和<strong>步幅</strong>以获取所需输出形状；</li>
<li>处理多个通道的输入数据时，池化层在每个输入通道上单独运算（❗与卷积层在通道上对于输入然后进行汇总不同）</li>
<li>池化层的目的在于：<ul>
<li>降低卷积层对于位置的敏感性（i.e. 移动一个像素可能会使得卷积层的输出大不相同，但对于汇聚层来说不一定）；</li>
<li>降低对空间降采样表示的敏感性 <strong>???</strong></li>
</ul>
</li>
</ul>
</details>
<details class="note">
<summary>PyTorch：二维汇聚层 <a href="../../Cookbooks/DLCB/#^maxavgpool">Check here</a></summary>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># batch_size, channel number, height, width</span>

<span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 注意，步幅默认与汇聚窗口的大小相同</span>
<span class="n">max_pool2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">max_pool2d_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 设置一个任意大小的矩形汇聚窗口</span>
</code></pre></div>
</details>
<details class="abstract">
<summary>LeNet-5 (1989 Yann LeCun)</summary>
<ul>
<li>注1：LeCun在其论文中用到的是下采样（subsampling）而不是池化（pooling），其将平均池化层作为有参数（并且带了偏置）的卷积层（见概念：池化）进行处理.</li>
</ul>
<details class="quote">
<summary>Subsampling</summary>
<p>The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient, <u>and then added to a trainable bias</u>.</p>
</details>
<ul>
<li>注2：LeCun使用的手写数字数据集中每个图片的大小为<span class="arithmatex">\(28\times28\)</span>，但其数据集中字母最大也只占有<span class="arithmatex">\(20\times20\)</span>个像素，LeCun的考虑是希望随着网络层数的加深，笔画的端点也可以出现在感受野的中心：</li>
</ul>
<details class="quote">
<summary>The reason for padding</summary>
<p>The reason is that it is desirable that <u>potential distinctive features</u> such as stoke endpoints or corner can appear in the center of the receptive field of the highest level feature detectors.</p>
</details>
<ul>
<li>LeNet的主要组成部分：<ul>
<li>卷积编码器：两个卷积层和两个池化层；</li>
<li>全连接层<u>密集块</u>：三个连续的全连接层</li>
</ul>
</li>
</ul>
<p><strong>LeNet的网络结构</strong>：<br />
1) <span class="arithmatex">\(6\)</span>通道卷积层，每一个卷积核的大小为<span class="arithmatex">\(5\times5\)</span>，填充为<span class="arithmatex">\(2\)</span>；</p>
<pre><code> - 注1：这里的填充正好使得图像大小未发生改变（$(-k_h+p_h+1,-k_w+p_w+1)=(0,0)$）
 - 注2：这里参数的参数数量（含偏置）$6\times25+6=156\ll 28^2=784$
</code></pre>
<p>2) <span class="arithmatex">\(6\)</span>通道下采样层（对应前一个卷积层的<span class="arithmatex">\(6\)</span>个通道，<span class="arithmatex">\(6\)</span>个池化核），每一个池化核大小为<span class="arithmatex">\(2\times2\)</span>，有<span class="arithmatex">\(6\)</span>个参数；<br />
3) Sigmoid激活函数（<strong>???</strong> 用ReLU更好，90s还未被应用）；<br />
4) <span class="arithmatex">\(16\)</span>通道卷积层（将<span class="arithmatex">\(6\)</span>个通道扩充为<span class="arithmatex">\(16\)</span>个通道，<span class="arithmatex">\(16\times6=96\)</span>个卷积核）：每个卷积核的大小为<span class="arithmatex">\(5\times5=25\)</span>，参数数量为<span class="arithmatex">\(25\times16+16=1516\)</span>个；</p>
<pre><code>    - 注意：$16$通道中的每一个通道接受$6$个通道的输入.
</code></pre>
<p>5) <span class="arithmatex">\(16\)</span>通道下采样层（对应前一个卷积层的<span class="arithmatex">\(16\)</span>个通道）：有<span class="arithmatex">\(32\)</span>个参数；<br />
6) <span class="arithmatex">\(120\)</span>通道卷积层</p>
<pre><code> - 注：LeCun用的是每一个卷积核大小为$5\times5$的$120$通道卷积层，但是每一个通道中输入的张量大小为$5\times5$，所以这其实相当于$16*5*5\times120$全连接层. 但是LeCun不将其称为全连接层，其理由：
</code></pre>
<details class="quote">
<summary>Why not call it a fully connected layer?</summary>
<p>If LeNet-5 input were made bigger with everything else kept constant, the feature map dimension would be larger than <span class="arithmatex">\(1\times1\)</span>. <strong>???</strong></p>
</details>
<p>7) <span class="arithmatex">\(84\times10\)</span>全连接层输出<span class="arithmatex">\(a_i(i=1,2,\cdots,10)\)</span>，LeCun选择<span class="arithmatex">\(84\)</span>个单元的理由：<br />
8) 整流：<span class="arithmatex">\(x_i=f(a_i)\)</span>其中<span class="arithmatex">\(f(a)=A\tanh(Sa)\)</span>，<span class="arithmatex">\(A\)</span>决定上下确界（或振幅，amplitude），<span class="arithmatex">\(S\)</span>决定原点处的斜率. <br />
9) 最后使用RBF层，每一个输出计算如下：<span class="arithmatex">\(<span class="arithmatex">\(y_i=\lVert \mathbf{x}-\mathbf{w}_i\rVert\)</span>\)</span></p>
<ul>
<li><span class="arithmatex">\(\mathbf{x}\)</span>表示上一个全连接层的<span class="arithmatex">\(84\)</span>个输出；</li>
<li><span class="arithmatex">\(\mathbf{w}_i\)</span>表示第<span class="arithmatex">\(i\)</span>RBF神经元的权重，大小亦为<span class="arithmatex">\(84\)</span>；</li>
<li>RBF层的输出可以视为一个惩罚项</li>
</ul>
<div class="admonition quote">
<p class="admonition-title">Quote</p>
</div>
<p>LeCun选择<span class="arithmatex">\(A=1.7159\)</span>理由见附录<span class="arithmatex">\(A\)</span></p>
</details>
<details class="note">
<summary>PyTorch：LeNet-5</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> 
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
</details>
<details class="note">
<summary>RBF网络 (1988 Broomhead, Lowe)</summary>
<p>RBF（Radial Basis Function）网络可以视为一个三层网络，输入层、隐层和输出层，其核心结构如下：<span class="arithmatex">\(<span class="arithmatex">\(\varphi(x)=\sum\limits_{i=1}^{N}a_i \rho(\lVert \mathbf{x}-\mathbf{c}_i\rVert),\mathbf{x}\in \mathbf{R}^n\)</span>\)</span><br />
<span class="arithmatex">\(\mathbf{x}\in \mathbb{R}^n\)</span>为输入</p>
<ul>
<li><span class="arithmatex">\(N\)</span>为隐层的神经元数量；</li>
<li><span class="arithmatex">\(\mathbf{c}_i\)</span>表示第<span class="arithmatex">\(i\)</span>个神经元的中心向量；</li>
<li><span class="arithmatex">\(a_i\)</span>表示第<span class="arithmatex">\(i\)</span>个神经元的权重；</li>
<li>注意到：函数<span class="arithmatex">\(\rho(\lVert \mathbf{x}-\mathbf{c}_i\rVert)\)</span>关于中心向量<span class="arithmatex">\(\mathbf{c}_i\)</span>是径向对称的（radially symmetric），并且<span class="arithmatex">\(N\)</span>个函数<span class="arithmatex">\(\rho(\lVert \mathbf{x}-\mathbf{c}_i\rVert)\)</span>可以视作生成<span class="arithmatex">\(\varphi(x)\)</span>的基，因此称之为径向基函数（Radial Basis Function）</li>
<li>范数通常取欧氏距离，基函数则一般取高斯函数：<span class="arithmatex">\(<span class="arithmatex">\(\rho(\lVert \mathbf{x}-\mathbf{c}_i\rVert)=\exp(-\beta_i \lVert \mathbf{x}-\mathbf{c}_i\rVert)\)</span>\)</span>并有：<span class="arithmatex">\(<span class="arithmatex">\(\lim_{\lVert \mathbf{x}\rVert\rightarrow\infty} \rho(\lVert \mathbf{x}-\mathbf{c}_i\rVert)=0\)</span>\)</span></li>
</ul>
</details>
<details class="abstract">
<summary>概念：特征学习</summary>
</details>
<details class="note">
<summary>AlexNet</summary>
</details>
<details class="note">
<summary>Transposed Convolution / Fractionally-strided convolution</summary>
<p><a href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">TowardsDataScience</a> and <a href="https://github.com/aqeelanwar/conv_layers_animation">Python GIFs</a><br />
Note that Transposed Convolution <span class="arithmatex">\(\neq\)</span> DeConvolution, it only reverse the dimensions.</p>
<p>Used for upsampling.</p>
<p>假设输入的单个样本的大小（不考虑通道）为：<span class="arithmatex">\(<span class="arithmatex">\((n_h,n_w)\)</span>\)</span>则输出的大小为：$$\begin{aligned}</p>
</details>
<p>&amp;(s_h(n_h-1)-2p_h+\text{dilation}_h(k_h-1)+\text{output_padding}_h+1)\times \<br />
&amp;(s_w(n_w-1)-2p_w+\text{dilation}_w(k_w-1)+\text{output_padding}_w+1)<br />
\end{aligned}$$</p>
<details class="note">
<summary>PyTorch: Transposed Convolution <a href="../../Cookbooks/DLCB/#^ConvTranspose2d">Check here</a></summary>
<div class="highlight"><pre><span></span><code><span class="n">conv2dT</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
</details>
<h3 id="_8">循环神经网络<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h3>
<h4 id="_9">关于序列数据预测的一个简单介绍<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<details class="abstract">
<summary>概念：序列数据，序列数据预测</summary>
<p>序列数据<br />
<strong>时间步</strong>，上下文<br />
<span class="arithmatex">\(x_t\sim P(x_t \,|\,x_{t-1},\cdots,x_1)\)</span><br />
在序列数据中，引入状态变量存储过去的信息，结合当前的输入给出输出是一种常见的做法.</p>
</details>
<details class="abstract">
<summary>概念：序列预测的假设，单步预测，和多步预测</summary>
<ul>
<li>序列模型的一个常见假设为：序列本身的动力学不会发生改变，或称序列的动力学是<strong>静止</strong>（stationary）的，整个序列的估计值可以通过：<span class="arithmatex">\(<span class="arithmatex">\(P(x_1,\cdots,x_T)=\prod_{t=1}^{T}P(x_t \,|\,x_{t-1},\cdots,x_1)\)</span>\)</span>获取.</li>
<li><strong>单步预测</strong>（one-step-ahead prediction）预测器只预测未来一个时间步的结果，称为单步预测；</li>
<li><strong>多步预测</strong>（multi-step-ahead prediction）预测器需要给出未来<span class="arithmatex">\(k\)</span>个时间步的预测，在这种情况下其通常需要基于原始序列数据<span class="arithmatex">\(\{x_{t-1},\cdots,x_{t-\tau}\}\)</span>给出预测<span class="arithmatex">\(\hat{x}_t\)</span>，然后将该预测连同原始序列数据用于对<span class="arithmatex">\(\hat{x}_{t+1}\)</span>的预测中，以此类推，这种多步预测的主要问题是：误差会逐次积累，可能完全偏离实际.</li>
</ul>
</details>
<details class="abstract">
<summary>概念：自回归模型，马尔可夫模型</summary>
<p>为估计<span class="arithmatex">\(P(x_t \,|\,x_{t-1},\cdots,x_1)\)</span>提出：</p>
<ul>
<li><strong>自回归模型</strong>（autoregressive models）：对于序列数据<span class="arithmatex">\(\{x_{t-1},\cdots,x_1\}\)</span>只选择（最近的）时间跨度为<span class="arithmatex">\(\tau\)</span>的序列用于预测：<span class="arithmatex">\(\{x_{t-1},\cdots,x_{t-\tau} \}\)</span>，则模型的参数是固定的.</li>
<li><strong>隐变量自回归模型</strong>（latent autoregressive models）：保留对过去的预测的总结<span class="arithmatex">\(h_{t-1}\)</span>（<span class="arithmatex">\(h\)</span>在这里表示的是hidden），然后据此同时更新预测<span class="arithmatex">\(\hat{x}_t=P(x_t \,|\,h_{t-1})\)</span>和总结<span class="arithmatex">\(h_t=g(h_{t-1},x_{t-1})\)</span>，因为其中的任何总结<span class="arithmatex">\(h_t\)</span>都并不存在，因此称为隐变量.</li>
</ul>
</details>
<details class="abstract">
<summary>概念：马尔可夫模型</summary>
<p>马尔可夫模型是一种常见的自回归模型.<br />
<strong>马尔可夫条件</strong>（Markov condition）：使用<span class="arithmatex">\(x_{t-1},\cdots,x_{t-\tau}\)</span>估计<span class="arithmatex">\(x_t\)</span>的结果和使用<span class="arithmatex">\(x_{t-1},\cdots,x_1\)</span>相比是<u>近似</u>精确的，则称序列满足马尔可夫条件.<br />
<strong>一阶马尔可夫模型</strong>（first-order Markov model）：<span class="arithmatex">\(<span class="arithmatex">\(P(x_1,\cdots,x_T)=\prod_{t=1}^{T}P(x_t \,|\,x_{t-1}),\quad P(x_1 \,|\,x_0)=P(x_1)\)</span>\)</span>并可以得出：$$\begin{aligned}</p>
</details>
<p>P(x_{t+1} \,|\,x_{t-1})&amp;=\frac{\sum\limits_{x_t}^{}P(x_{t+1},x_t,x_{t-1})}{P(x_{t-1})}\<br />
&amp;=\frac{\sum\limits_{x_t}^{}P(x_{t+1}\,|\,x_t,x_{t-1})P(x_t,x_{t-1})}{P(x_{t-1})}\<br />
&amp;=\sum\limits_{x_t}^{}P(x_{t+1}\,|\,x_t)P(x_t \,|\,x_{t-1})<br />
\end{aligned}$$（<strong>???</strong> 这里，条件公式能这样用吗）</p>
<blockquote>
<p>^AutoregressiveModel</p>
</blockquote>
<details class="note">
<summary>PyTorch: 一个简单的自回归模型</summary>
<p>用PyTorch写一个自回归模型，信号是一个正弦函数加噪，之后将其截断为两部分，前半部分用于训练，后半部分作为测试集的标签.</p>
</details>
<h4 id="_10">循环神经网络初步<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<details class="abstract">
<summary>概念：（单层）循环神经网络</summary>
<p>假设在时间步<span class="arithmatex">\(t\)</span>有小批量输入<span class="arithmatex">\(\mathbf{X}_t\in \mathbb{R}^{n\times d}\)</span>，<span class="arithmatex">\(\mathbf{X}_t\)</span>的每一行对应来自相应序列的时间步<span class="arithmatex">\(t\)</span>处的一个样本，时间步<span class="arithmatex">\(t\)</span>处的<strong>隐状态</strong>（hidden state）记为<span class="arithmatex">\(\mathbf{H}_t\in \mathbb{R}^{n\times h}\)</span>，其计算公式为：<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{H}_t=\phi(\mathbf{X}_t \mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}+\mathbf{b}_h)\)</span>\)</span></p>
<ul>
<li><span class="arithmatex">\(\phi\)</span>为激活函数</li>
<li><span class="arithmatex">\(\mathbf{W}_{xh}\in \mathbb{R}^{d\times h},\mathbf{W_{hh}}\in \mathbb{R}^{h\times h},\mathbf{b}_h\in \mathbb{R}^{1\times h}\)</span>，其中<span class="arithmatex">\(\mathbf{b}_h\)</span>通过广播运算作用.</li>
<li>隐变量<span class="arithmatex">\(\mathbf{H}_{t}\)</span>表示直至时间步<span class="arithmatex">\(t\)</span>的所有历史信息（可以理解为神经网络的记忆，更常见的说法是状态），每一个隐状态的计算公式都如此，因此称之为<strong>循环</strong>（recurrent），每一个进行该计算的层称为<strong>循环层</strong>（recurrent layer），这种神经网络称为<strong>循环神经网络</strong>（recurrent neural network, RNN）.</li>
<li>计算隐状态中<span class="arithmatex">\(\mathbf{X}_t \mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}\)</span>相当于<span class="arithmatex">\(\mathbf{X}_t\)</span>和<span class="arithmatex">\(\mathbf{H}_{t-1}\)</span>沿着<span class="arithmatex">\(1\)</span>轴的拼接得到的矩阵与<span class="arithmatex">\(\mathbf{W}_{xh}\)</span>和<span class="arithmatex">\(\mathbf{W}_{h\times h}\)</span>沿着<span class="arithmatex">\(0\)</span>轴的拼接得到的矩阵的乘积.</li>
</ul>
<p>在每一个时间步<span class="arithmatex">\(t\)</span>，根据当前时间步的隐状态<span class="arithmatex">\(\mathbf{H}_t\)</span>计算该时间步的输出：<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{O}_t=\mathbf{H}_t \mathbf{W}_{hq}+\mathbf{b}_q\)</span>\)</span></p>
<ul>
<li><span class="arithmatex">\(\mathbf{W}_{hq}\in \mathbb{R}^{h\times q},\mathbf{b}_q\in \mathbb{R}^{1\times q}\)</span></li>
</ul>
<p>注意：在任何时间步，循环神经网络<u>都使用相同的模型参数</u><span class="arithmatex">\(\mathbf{W}_{xh}, \mathbf{W}_{hh}, \mathbf{b}_{h}, \mathbf{W}_{hq},\mathbf{b}_{q}\)</span>计算输出，循环神经网络的参数数量不随输入序列的大小发生改变.</p>
<ul>
<li>在隐状态这一说法下，多层感知机（这里以两层的举个例子）可以被视为“无隐状态”神经网络：$$\begin{aligned}</li>
</ul>
</details>
<p>&amp;\mathbf{H}=\phi(\mathbf{X}\mathbf{X}<em hq="hq">{xh}+\mathbf{b}_h)\<br />
&amp;\mathbf{O}=\mathbf{HW}</em>_q}+\mathbf{b<br />
\end{aligned}$$</p>
<details class="note">
<summary>PyTorch: 循环神经网络实现 <a href="../../Cookbooks/DLCB/#^RNN">Check here</a></summary>
<ul>
<li>注1：本代码中用较低层的代码实现的单层RNN与用PyTorch直接实现的RNN共享参数之后输出的结果基本一致（部分值存在e-07到e-08的误差），该数值误差问题在其他代码中也出现，这个问题现在还未得到解决；</li>
<li>注2：本代码给定输入之后返回的结果中<code>grad_fn=&lt;SliceBackward0&gt;</code>，而PyTorch中的RNN返回的结果中<code>grad_fn=&lt;SelectBackward0&gt;</code>，该问题还未弄清楚原因.<br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="sd">&#39;&#39;&#39;首先从在PyTorch框架下搭建一个&#39;&#39;&#39;</span>

<span class="k">class</span> <span class="nc">RNNModels</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 定义隐层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hiddens</span> <span class="o">=</span> <span class="n">num_hiddens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">)</span> <span class="c1"># 初始状态化状态</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">))</span> <span class="c1"># 这里也可以用nn.Linear(num_inputs, num_hiddens)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_hh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_f</span> <span class="o">=</span> <span class="n">activation_function</span> <span class="c1"># 激活函数需要另外提供</span>

        <span class="c1"># 定义输出层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_qh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">)</span>

        <span class="c1"># 附上梯度</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W_hx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_hh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_qh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_q</span><span class="p">]:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calculate_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">time_step</span><span class="p">):</span>
        <span class="c1"># 循环神经网络需要迭代地处理数据，因此这里先写一个用于迭代的函数，计算每一个时间步处的隐藏状态</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_f</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="n">time_step</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_hx</span><span class="p">)</span>

            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="n">time_step</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_hh</span><span class="p">)</span> 
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># 首先计算隐状态，假设输入形状为 (Batch_size, time_steps, num_inputs)，此处num_inputs指的是一个时间步处一个数据的维度</span>
        <span class="n">h_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">init_state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_hiddens</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_state</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># 这里我的隐状态包含了h_0这个初始状态，所以第t个时间对应第t+1个状态</span>
            <span class="n">h_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span>

        <span class="c1"># 我们并不在这前向传播环节计算输出状态，因为一些原因并不是所有的输出状态都需要进行计算.</span>

        <span class="k">return</span> <span class="n">h_states</span> <span class="c1"># 这里返回最近的一次隐藏状态，以及已经计算的每个时间步的隐藏状态</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># 一个批量大小为2，长度为5，维度为4的数据</span>
<span class="n">aci_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">tanh</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNNModels</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">aci_func</span><span class="p">)</span>
<span class="n">states</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;直接使用PyTorch中定义好的函数，并共享上面的参数&#39;&#39;&#39;</span>

<span class="n">rnn_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">rnn_</span><span class="o">.</span><span class="n">weight_ih_l0</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">W_hx</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="n">rnn_</span><span class="o">.</span><span class="n">weight_hh_l0</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">W_hh</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

<span class="n">states_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span><span class="n">rnn_</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># 验证结果是否相同</span>
<span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">states_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 这里本人返回的结果中有误差e-07到e-08数量级</span>
</code></pre></div><br />
^PyTorchSimpleRNN</li>
</ul>
</details>
<details class="abstract">
<summary>概念：损失函数：困惑度</summary>
<p>对于一个序列（i.e.语句）：<span class="arithmatex">\(<span class="arithmatex">\(S=x_1,x_2,\cdots,x_n\)</span>\)</span>在给定模型下出现的概率为：<span class="arithmatex">\(<span class="arithmatex">\(P(S)=P(x_1,x_2,\cdots,x_n)=P(x_1)P(x_2 \,|\,x_1),\cdots P(x_n \,|\,x_1,x_2,\cdots,x_{n-1})\)</span>\)</span></p>
<p>困惑度的想法是当将测试集中的序列输入到模型中时，如果其</p>
<p>一个语言模型的预测应该是准确的，可以用交叉熵损失来度量这点：<span class="arithmatex">\(<span class="arithmatex">\(\frac{1}{n}\sum\limits_{t=1}^{n}-\log P(x_t \,|\,x_{t-1},\cdots,x_1)\)</span>\)</span></p>
<ul>
<li>其中<span class="arithmatex">\(x_t,x_{t-1},\cdots,x_1\)</span>均为实际序列在时间步<span class="arithmatex">\(t\)</span>的值，<span class="arithmatex">\(P\)</span>由语言模型给出.</li>
<li>自然语言处理中用<strong>困惑度</strong>（perplexity）对此进行衡量：<span class="arithmatex">(<span class="arithmatex">\(\exp \left(-\frac{1}{n}\sum\limits_{i=1}^{n}\log P(x_t \,|\,x_{t-1},\cdots,x_1)\right)\)</span>\)</span><ul>
<li>当模型预测结果中真实标签的概率总为<span class="arithmatex">\(1\)</span>时，模型的困惑度为<span class="arithmatex">\(1\)</span>；</li>
<li>当模型预测结果中真实标签的概率总为<span class="arithmatex">\(0\)</span>时，模型的困惑度无穷大；</li>
<li>如果模型的预测<span class="arithmatex">\(P\)</span>是一个均匀分布，则困惑度为<span class="arithmatex">\(n\)</span>（词表的唯一词元的数量），应该作为衡量模型质量的基线，有用的模型的困惑度必须要低于这个上限.</li>
</ul>
</li>
</ul>
</details>
<details class="note">
<summary>PyTorch: 计算困惑度</summary>
</details>
<details class="abstract">
<summary>RNN的损失函数</summary>
<p>在RNN中，损失函数计算的是一个序列的损失，即对于长度为<span class="arithmatex">\(T\)</span>的序列<span class="arithmatex">\(\{x_1,x_2,\cdots,x_T\}\)</span>，对应的标签为<span class="arithmatex">\(\{y_1,y_2,\cdots,y_T\}\)</span>，RNN的预测序列为<span class="arithmatex">\(\{\hat{y}_1,\hat{y}_2,\cdots,\hat{y}_T\}\)</span>，在定义损失函数（例如交叉熵）之后计算每一个时间步<span class="arithmatex">\(t\)</span>处的损失<span class="arithmatex">\(L(\hat{y}_t,y_t)\)</span>，然后求和作为该序列的损失：<span class="arithmatex">\(<span class="arithmatex">\(L(\hat{y},\hat{y})=\sum\limits_{t=1}^{T}L(\hat{y}_t,y)\)</span>\)</span></p>
</details>
<details class="abstract">
<summary>概念：通过时间反向传播</summary>
<p><strong>通过时间反向传播</strong>（backpropagation through time, BPTT）的的思想是：将循环神经网络的计算图一次展开一个时间步，以获取模型变量和参数之间的依赖关系，然后基于链式法则，应用反向传播计算和存储梯度.</p>
<p>在（单层）循环神经网络中提到，RNN的模型参数是共享的，并且由网路本身的循环性质会造成梯度的“循环”性质，下面是数学推导，首先回顾：$$\begin{aligned}</p>
</details>
<p>&amp;\mathbf{H}<em xh="xh">t=\phi(\mathbf{X}_t \mathbf{W}</em>}+\mathbf{H<em hh="hh">{t-1}\mathbf{W}</em>}+\mathbf{b<em hq="hq">h)\<br />
&amp;\mathbf{O}_t=\mathbf{H}_t \mathbf{W}</em>_q}+\mathbf{b<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(我们直接使用简化的写法：用\)</span>h_t\)</span>表示隐藏层张量<span class="arithmatex">\(\mathbf{H}_t\)</span>，用<span class="arithmatex">\(w_h\)</span>表示隐层的权重张量<span class="arithmatex">\(\mathbf{W}_{xh}, \mathbf{W}_{hh}\)</span>（因为可以视为拼接情形进行计算），用<span class="arithmatex">\(x_t\)</span>表示输入张量<span class="arithmatex">\(\mathbf{X}_t\)</span>，用<span class="arithmatex">\(o_t\)</span>表示输出张量<span class="arithmatex">\(\mathbf{O}_t\)</span>，用表示输出层的权重张量<span class="arithmatex">\(\mathbf{W}_{hq}\)</span>，偏置<span class="arithmatex">\(\mathbf{b}_h,\mathbf{b}_q\)</span>可以作为权重张量的一部分，因此可以将<span class="arithmatex">\(t\)</span>时间步的隐状态和输出写为：<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
&amp;h_t=f(x_t,h_{t-1},w_h)\\
&amp;o_t=g(h_t,w_o)
\end{aligned}\)</span>\)</span>设时间步<span class="arithmatex">\(t\)</span>对应的标签为<span class="arithmatex">\(y_t\)</span>，设在所有<span class="arithmatex">\(T\)</span>个时间步的损失函数为：<span class="arithmatex">\(<span class="arithmatex">\(L(x_1,\cdots,x_T,y_1,\cdots,y_T,w_h,w_o)=\frac{1}{T}\sum\limits_{t=1}^{T}l(y_t,o_t)\)</span>\)</span>按照链式法则：<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
\frac{\partial{L}}{\partial{w_h}}&amp;=\frac{1}{T}\sum\limits_{t=1}^{T}\frac{\partial{l(y_t,o_t)}}{\partial{w_h}}\\
&amp;=\frac{1}{T}\sum\limits_{t=1}^{T}\frac{\partial{l(y_t,o_t)}}{\partial{o_t}}\frac{\partial{g(h_t,w_0)}}{\partial{h_t}}\frac{\partial{h_t}}{\partial{w_h}}
\end{aligned}\)</span>\)</span>前面提到，<span class="arithmatex">\(w_h\)</span>是各个隐状态共享的参数，于是该梯度计算中的问题出现在：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{h_t}}{\partial{w_h}}=\frac{\partial{f(x_t,h_{t-1},w_h)}}{\partial{w_h}}+\frac{\partial{f(x_t,h_{t-1},w_h)}}{\partial{h_{t-1}}}\frac{\partial{h_{t-1}}}{\partial{w_h}}\)</span>\)</span>这就构成了一个梯度计算的循环，最终可以得到：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{h_t}}{\partial{w_h}}=\frac{\partial{f(x_t,h_{t-1},w_h)}}{\partial{w_h}}+\sum\limits_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}\frac{\partial{f(x_j,h_{j-1},w_h)}}{\partial{h_{j-1}}}\right)\frac{\partial{f(x_i,h_{i-1},w_h)}}{\partial{w_h}}\)</span>\)</span>当<span class="arithmatex">\(t\)</span>很大时，该链会变得很长，如果完全计算，梯度爆炸和梯度消失的可能性非常大，不可行；</p>
<blockquote>
<ul>
<li><strong>截断时间步</strong>：一种直观的方法是在反向<span class="arithmatex">\(\tau\)</span>步之后不再计算，即只计算：<span class="arithmatex">\(<span class="arithmatex">\(\cdots+\sum\limits_{t-\tau}^{t-1}\left(\prod_{j=i+1}^{t}\cdots\right)\cdots\)</span>\)</span>但是这样又会使得模型主要侧重于短期影响. </li>
<li><strong>???</strong> <strong>随机截断</strong>：可以使用一个随机变量替换某一个</li>
</ul>
</blockquote>
<details class="note">
<summary>RNN梯度计算的具体细节</summary>
<p>建议参考附录部分的矩阵微分理解.</p>
</details>
<details class="abstract">
<summary>概念：梯度裁剪</summary>
</details>
<h4 id="_11">自然语言处理<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h4>
<details class="abstract">
<summary>概念：词元，词表，语料，语言模型</summary>
<ul>
<li><strong>词元</strong>（token）：文本的基本单位，例如单词；根据文本获取词元的过程称为<strong>词元化</strong>（tokenization），主要存在以下问题：<ul>
<li>标点符号（punctuation）：除了对于显见的单词、字母进行编码，我们同样需要对标点符号进行编码. （例如，一个语句中的问号、引号包含的信息可能同样很重要）</li>
</ul>
</li>
<li><strong>词表</strong>（vocabulary）：一个字典，将字符串类型的次元与数字索引一一对应；</li>
<li><strong>语料</strong>（corpus）：一个文本中的所有不重复的词元构成语料，通常会将很少出现的词元移除以便于处理.</li>
<li><strong>语言模型</strong>（language model）：假设长度为<span class="arithmatex">\(L\)</span>的文本序列中的词元依次为<span class="arithmatex">\(x_1,\cdots,x_L\)</span>，则<span class="arithmatex">\(x_t(1\leq t\leq L)\)</span>可以被认为是文本序列在时间步<span class="arithmatex">\(t\)</span>处的观测（标签），语言模型的目标为估计序列的联合概率：<span class="arithmatex">(<span class="arithmatex">\(P(x_1,\cdots,x_2,\cdots,x_L)\)</span>\)</span>从而对于一个给定的文本序列<span class="arithmatex">\(\{x_{t-1},\cdots,x_1\}\)</span>，可以通过逐次抽取次元<span class="arithmatex">\(x_t\sim P(x_t \,|\,x_{t-1},\cdots,x_1)\)</span>生成自然文本.<ul>
<li>这里定义的语言模型完全按照概率运行，其并不理解语法，但是这仍然是有用的，例如在语音识别中“to recognize speech”和“to wreck a nice beach”读音尽管相似，但是前者的概率更大，后者不太可能是结果，于是可以通过语言模型来提高识别的正确性.</li>
</ul>
</li>
</ul>
</details>
<details class="note">
<summary>Python: 文本预处理的常用方法</summary>
<p><a href="PyDoc.md#^RegularExpression">Check here</a><br />
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;按行阅读.txt文件&#39;&#39;&#39;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">txt_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlins</span><span class="p">()</span> <span class="c1"># 返回一个二维列表，每一行对应文本中的一个行</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="n">line1</span> <span class="o">=</span> <span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="sd">&#39;&#39;&#39;常用操作&#39;&#39;&#39;</span>
<span class="n">line1</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="c1"># 去前后字符</span>
<span class="n">line1</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="c1"># 全转小写</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span> <span class="c1"># 统一处理</span>

<span class="sd">&#39;&#39;&#39;正则化字符&#39;&#39;&#39;</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[^A-Za-z]+&#39;</span><span class="p">,</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">line1</span><span class="p">)</span>
<span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[A-Za-z]+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">line1</span><span class="p">)</span> <span class="c1"># 当然，也可以把英文字符转换为空格</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>拉丁语系次元化</summary>
<details class="note">
<summary>Word-based</summary>
</details>
<details class="note">
<summary>Character-based</summary>
</details>
<details class="note">
<summary>Subword tokenization</summary>
<p>不将常用词拆分为更小的子词，而将稀有词分解为有意义的子词<br />
Let's do tokenization!<br />
Let's\</w> | do\</w> | token | ization\</w> | !\</w></p>
</details>
</details>
<details class="note">
<summary>各种词元化</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">re</span>
<span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[\w&#39;]+|[.,!?;]&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello, I&#39;m a string!&quot;</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="note">
<summary>PyTorch: 将文本词元化，并生成词表</summary>
</details>
<details class="abstract">
<summary>子词处理</summary>
</details>
<details class="abstract">
<summary>概念：词嵌入</summary>
<p><strong>词嵌入</strong>（Embeddings，或称词向量）技术是将词汇映射到高维空间的方法，与其人工设计词汇间的对应关系，词嵌入技术让模型去学习对应关系，直观表现是相似的词语（walk, walked/dog, cat/hello, goodbye）在高维空间中的距离更近；</p>
<p>首先假设我们有一个大小为<span class="arithmatex">\(\lvert V\rvert\)</span>的词表，对于一列输入<span class="arithmatex">\(\mathbf{X}\)</span>，可以将每一个词元依据词表转换为一个独热向量进行处理.<br />
^Embeddings</p>
</details>
<details class="note">
<summary>PyTorch: Embedding <a href="../../Cookbooks/DLCB/#^Embedding">Check here</a></summary>
<p><div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="nb">input</span><span class="p">]</span>

<span class="n">emb</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="n">emb</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

<span class="n">emb_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">emb_</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">Embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
<span class="n">emb_</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
</code></pre></div><br />
^Embedding</p>
</details>
<details class="abstract">
<summary>概念：语言模型的学习</summary>
<p>按照语言模型的定义，序列<span class="arithmatex">\(\{x_1,x_2,\cdots,x_t\}\)</span>的概率为：<span class="arithmatex">\(<span class="arithmatex">\(\prod_{t=1}^{t}P(x_t \,|\,x_{t-1},\cdots,x_1),P(x_1 \,|\,x_0)=x_1\)</span>\)</span><br />
为训练一个语言模型需要计算单词的概率以及给定单词后出现某个单词的条件概率.</p>
<ul>
<li>一种训练方法是根据已有文本计算相对词频：<span class="arithmatex">\(<span class="arithmatex">\(\hat{P}(\mathrm{on} \,|\,\mathrm{move})=\frac{n(\mathrm{move},\mathrm{on})}{n(\mathrm{move})}\)</span>\)</span>但是获取正确的估计并不简单，例如上面<span class="arithmatex">\(\text{on}\)</span>的频率显然远远大于<span class="arithmatex">\(\text{move on}\)</span>.</li>
<li><strong>拉普拉斯平滑</strong>（Laplace smoothing）通过引入超参数试图解决上面提到的问题：$$\begin{aligned}</li>
</ul>
</details>
<p>&amp;\hat{P}(x)=\frac{n(x)+\epsilon_1/m}{n+\epsilon_1}\<br />
&amp;\hat{P}(x' \,|\,x)=\frac{n(x,x')+\epsilon_2 \hat{P}(x')}{n(x)+\epsilon_2}\<br />
&amp;\hat{P}(x''\,|\,x,x')=\frac{n(x,x',x'')+\epsilon_3\hat{P}(x'')}{n(x,x')+\epsilon_3}\<br />
&amp;\cdots\cdots<br />
\end{aligned}\end{center}$$显然这又是有问题的，如何选择超参？</p>
<blockquote>
<ul>
<li>奇普夫定律（Zipf's law）指出：第<span class="arithmatex">\(i\)</span>个最常见的单词的频率<span class="arithmatex">\(n_i\)</span>有如下近似关系：<span class="arithmatex">\(n_i\propto \frac{1}{i^{\alpha}}\)</span></li>
<li>此外语言学习还需要考虑意思相近的词；</li>
</ul>
</blockquote>
<details class="note">
<summary>PyTorch: 判定消极评论和积极评论</summary>
<p>Sources: <br />
通过阅读</p>
</details>
<details class="abstract">
<summary>概念：稀疏特征，紧密特征</summary>
<p><a href="https://induraj2020.medium.com/what-are-sparse-features-and-dense-features-8d1746a77035">here</a></p>
</details>
<details class="note">
<summary>AdaGrad</summary>
<p><strong>一个问题</strong>：Adagrad是否适合在Fine-Tune阶段进行？</p>
<p>多数情况下，我们希望降低学习率以提高优化效果. 然而在一些情况下，对于一些出现次数少的特征（<strong>稀疏特征</strong>，sparse features），比如说在训练语言模型中对于一些不常见的词语或者是词语搭配，我们希望<u>提高学习率</u>来学会这个特征.（<strong>???</strong> 太笼统了）同时，对于一些经常出现的特征，希望<u>减少学习率</u>，也就是一个同时进行的过程.</p>
<p>一种想法是：可以用观测到稀疏特征的次数<span class="arithmatex">\(S(t,i)\)</span>作为第<span class="arithmatex">\(t\)</span>次更新特征<span class="arithmatex">\(i\)</span>时学习率调整的依据，从而为每一个特征定制学习率：<span class="arithmatex">\(<span class="arithmatex">\(\eta_i=\frac{\eta_0}{\sqrt{S(t,i)}+\epsilon}\)</span>\)</span></p>
<p>Adagrad以<u>计算先前的梯度之和</u>作为一个粗略的计数器<span class="arithmatex">\(S(t,i)\)</span></p>
<p>算法如下：$$\begin{aligned}</p>
</details>
<p>&amp; g_t=\partial_{\mathbf{w}}l(y_t,f(x_t;\mathbf{w}))\<br />
&amp; s_t=s_{t-1}+g_t^2\<br />
&amp; \mathbf{w}<em t-1="t-1">t=\mathbf{w}</em>\cdot g_t\}-\frac{\eta_0}{\sqrt{s_t}+\epsilon<br />
&amp; s_0=0<br />
\end{aligned}$$</p>
<blockquote>
<p>但是AdaGrad也有一个缺陷，学习率下降地太快（<span class="arithmatex">\(s_t\)</span>基本以线性速率增长，学习率的变化为<span class="arithmatex">\(\mathcal{O}(t^{-\frac{1}{2}})\)</span>）.我们会希望学习率降低地更慢一些.</p>
<p>^AdaGrad</p>
</blockquote>
<h3 id="_12">残差网络<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<details class="abstract">
<summary>网络的序列结构及其缺陷</summary>
<p>MLP和CNN都是序列结构的网络，可以被视为复合映射.</p>
<p>事实上，序列结构的网络越深其表示能力应当越强，实践中随着网络层数加深到一定程度之后模型的表现能力反而下降，这说明问题出在对于网络的训练上. 关于网络无法被理想地训练，一种猜想是模型参数初始化的问题.</p>
<p>??? However, the derivative assumes an infinitesimal change in the parameter, whereas optimization algorithms use a finite step size. （指的应该是Learning Rate限制了梯度的连续变化，梯度只能够进行离散的变化，相当于是完全盲目的.，所以深层网络的损失函数太过于复杂，以至于这种离散优化的问题变得严重？）</p>
<p>经验观察：浅层网络的输出的梯度随着输入的变化改变地更<strong>慢</strong>，然而，对于深层神经网络来说，输入结果的一点变化就会引发一个完全不同的梯度</p>
<ul>
<li><strong>破碎梯度</strong>（shattered gradients）：深层神经网络的梯度的自相关性随着<span class="arithmatex">\(\Delta x\)</span>的增大会迅速减小在<span class="arithmatex">\(0\)</span>附近，对于这种现象的一种猜想是浅层的神经网络会对更深层的神经网络造成更复杂的影响，i.e. 对于一个包含<span class="arithmatex">\(3\)</span>个隐层的序列结构网络：$$\begin{aligned}</li>
</ul>
</details>
<p>&amp;h_1 = f_1(x,\omega_1)\<br />
&amp;h_2 = f_2(h_1, \omega_2)\<br />
&amp;h_3 = f_3(h_2, \omega_3)\<br />
&amp;y = f_4(h_3, \omega_4)<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(计算网络输出相对于\)</span>h_1\)</span>的梯度：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{y}}{\partial{f_1}}=\frac{\partial{f_4}}{\partial{f_3}}\frac{\partial{f_3}}{f_2}\frac{\partial{f_2}}{\partial{f_1}}\)</span>\)</span>如果改变<span class="arithmatex">\(f_1\)</span>的参数，序列中的其他的函数的梯度也会发生变化（因为在序列结构网络中深层的参数来自浅层的），从而使得最终计算得到的梯度本身发生更复杂的变化（就是梯度爆炸）</p>
<details class="abstract">
<summary>概念：残差连接和残差层</summary>
<p>对于上面的<span class="arithmatex">\(3\)</span>个隐层的序列神经网络，残差神经网络定义为：$$\begin{aligned}</p>
</details>
<p>&amp;h_1 = x + f_1(x, \omega_1)\<br />
&amp;h_2 = h_1 + f_2(h_1, \omega_2)\<br />
&amp;h_3 = h_2 + f_3(h_2, \omega_3)\<br />
&amp;y = h_3 + f_4(h_3, \omega_4)<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(称\)</span>x,h_1,h_2,h_3\)</span>为<strong>残差连接</strong>（residual connection）. 输入<span class="arithmatex">\(x\)</span>（或者<span class="arithmatex">\(h_i\)</span>）和<span class="arithmatex">\(f_i(h_{i-1}, \omega_{i}),h_0=x\)</span>的加法组合称为<strong>残差层</strong>（residual layer）</p>
<blockquote>
<p>之所以称为残差，是因为从另一个角度看网络学习到的实际上是<span class="arithmatex">\(H(x)=f(x)-h(x)\)</span>，称为残差其中<span class="arithmatex">\(h\)</span>为之前的层，不需要在<span class="arithmatex">\(H\)</span>层学习.<br />
^ResidualLayer</p>
</blockquote>
<details class="note">
<summary>PyTorch: 残差层</summary>
<p>残差层非常容易实现，下面是一个简单的例子<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ResModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">f1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">h2</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">f2</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">h2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">f3</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h3</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">f4</span><span class="p">(</span><span class="n">h3</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="abstract">
<summary>概念：残差层中的运算顺序</summary>
<p>在顺序结构网络中，输入往往是经过线性变换（全连接层、卷积层）之后应用激活函数（例如ReLU），但是在残差层中如果使用这种运算顺序，会使得输出值只能是输入值的增加（为理解这点，将输出值展开即可）</p>
<p>因此残差层中，输入通常先进行一次线性计算，在使用激活函数（例如ReLU）之后再次应用线性计算. 通常来说残差层不会首先使用ReLU()进行作用，一个直观的考虑是如果输入中的负值太多，ReLU()会直接输出<span class="arithmatex">\(0\)</span>，影响之后的层的表现，可能直接降低网络的表示能力.</p>
</details>
<details class="abstract">
<summary>概念：ReLU中的梯度爆炸</summary>
</details>
<details class="abstract">
<summary>标准化</summary>
<p>在进行对于BN，LN的讨论之前，先来谈谈为什么要对样本进行标准化.</p>
</details>
<p>Batch Normlaization的原理仍然没有得到很好的解答<a href="https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338#b93c">Towards Data Science</a></p>
<details class="abstract">
<summary>概念：批量正则化</summary>
<p><strong>批量正则化</strong>（Batach normalization, BatchNorm）的观点是：首先将样本进行标准化（均值为<span class="arithmatex">\(0\)</span>，标准差为<span class="arithmatex">\(1\)</span>），然后在网络中引入参数<span class="arithmatex">\(\delta,\gamma\)</span>，让网络自己学习对样本的均值和标准差进行偏移和放缩.</p>
<p>首先计算每一个批量中的激活值（用<span class="arithmatex">\(h\)</span>表示）的经验均值和标准差：$$\begin{aligned}</p>
</details>
<p>&amp;\mu_h=\frac{1}{\lvert B\rvert}\sum\limits_{i\in B}^{}h_i\<br />
&amp;\sigma_h=\sqrt{\frac{1}{\lvert B\rvert}\sum\limits_{i\in B}<sup>{}(h_i-\mu_h)</sup>2}<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(然后根据经验均值和标准差*标准化*该批量中的每个激活值：\)</span>\)</span>h_i\leftarrow \frac{h_i-\mu_h}{\sigma_h+\epsilon},\forall i\in B<span class="arithmatex">\(<span class="arithmatex">\(其中\)</span>\epsilon\)</span>是一个小值数字，用于避免分母为<span class="arithmatex">\(0\)</span>（<strong>???</strong> 这标准化之后会让值非常的大吗？）. 在进行标准化之后，使用<span class="arithmatex">\(\gamma\)</span>和<span class="arithmatex">\(\delta\)</span>再次进行处理<strong>正则化</strong>：<span class="arithmatex">\(<span class="arithmatex">\(h_i\leftarrow \gamma h_i + \delta,\forall i\in B\)</span>\)</span>在处理之后，批量样本的均值和标准差被改变为<span class="arithmatex">\(\delta\)</span>和<span class="arithmatex">\(\gamma\)</span>，其中<span class="arithmatex">\(\delta\)</span>和<span class="arithmatex">\(\gamma\)</span>则是<u>在网络中经过学习得到的参数</u>.</p>
<blockquote>
<ul>
<li>
<p>批量正则化在一个隐藏单元上独立地进行处理，对于一个具有<span class="arithmatex">\(K\)</span>个层全连接的网络，每一层中有<span class="arithmatex">\(D_k\)</span>个隐藏单元，则将会设置<span class="arithmatex">\(\sum\limits_{k=1}^{K}D_k\)</span>个独立的<span class="arithmatex">\(\delta\)</span>和<span class="arithmatex">\(\gamma\)</span></p>
</li>
<li>
<p>注1：对于具有<span class="arithmatex">\(K\)</span>个卷积层的卷积神经网络，一个层中包含<span class="arithmatex">\(C_k\)</span>个通道，则设置<span class="arithmatex">\(\sum\limits_{k=1}^{K}C_k\)</span>个<span class="arithmatex">\(\delta\)</span>和<span class="arithmatex">\(\gamma\)</span>.<br />
 对于卷积神经网络，使用BatchNorm时会对一个卷积层中的每一个通道分别进行正则化：<br />
 假设输入的形状为<span class="arithmatex">\((N,C,H,W)\)</span>，批量大小<span class="arithmatex">\(m\)</span>，通道个数<span class="arithmatex">\(c\)</span>，高度<span class="arithmatex">\(h\)</span>，宽度<span class="arithmatex">\(w\)</span><br />
 首先对于一个通道上的卷积核在所有位置处理之后的输出值<span class="arithmatex">\(\mathbf{A}=[a_{ij}]\)</span>，进行统一的标准化. 假设输出的大小为<span class="arithmatex">\((p,q)\)</span>（亦或称特征映射的大小），那么<em>有效</em>的批量大小为<span class="arithmatex">\(m\times p \times q\)</span>（而不是<span class="arithmatex">\(m\)</span>），使用有效批量中的所有数据计算均值和标准差<span class="arithmatex">\(\mu,\sigma\)</span>，然后对输出值<span class="arithmatex">\(\mathbf{A}\)</span>进行标准化之后添加学习参数<span class="arithmatex">\(\delta,\gamma\)</span>，再传入到激活函数中. <mark>注意</mark>：以上这种操作，即使批量大小为<span class="arithmatex">\(m=1\)</span>，也可以进行BatchNorm操作（有效批量大小为<span class="arithmatex">\(p\times q\)</span>）</p>
</li>
<li>
<p>引入批量正则化方法训练后的网络用于测试时，一个问题是如何对于测试集进行<u>标准化</u>，测试集并不会被分成批量，因此常用的方法是根据对整个测试训练集进行标准化（计算整个测试集的均值与标准差）.</p>
</li>
</ul>
</blockquote>
<details class="note">
<summary>批量正则化变体：GhostNorm, Batch Renormalization, LayerNorm</summary>
<ul>
<li><strong>GhostNorm</strong>（Ghost batch normalization）相比于BathcNorm并不使用一个批量中的所有样本进行标准化，而是从中选取一部分进行正则化，加入了更多的噪声，<strong>???</strong> increases the amount of regularization when the batch size is very large.</li>
<li>
<p><strong>Batch Renormalization</strong>：当批量大小很小，或者对于NLP中的一个批量（样本之间的变化往往非常大 <strong>???</strong>），BatchNorm一般不可靠，</p>
</li>
<li>
<p><strong>LayerNorm</strong>（Layer normalization）不使用批量的数据标准化样本，而使用每一个层的计算结果进行标准化. <br />
下面对一个具有<span class="arithmatex">\(H\)</span>的隐藏单元的层<span class="arithmatex">\(l\)</span>，首先计算样本输入之后在<u>未激活</u>之前的值<span class="arithmatex">\(a^l_i(i=1,2,\cdots,H)\)</span>的经验均值<span class="arithmatex">\(\mu^l\)</span>和经验标准差<span class="arithmatex">\(\sigma^l\)</span>，然后对<span class="arithmatex">\(a=[a_1,a_2,\cdots,a_H]\)</span>进行标准化，如下：<br />
$$\begin{aligned}</p>
</li>
</ul>
</details>
<p>&amp;\mu<sup H="H">l=\frac{1}{H}\sum\limits_{i=1}</sup>a_i<sup>l,\sigma</sup>l=\sqrt{\frac{1}{H}\sum\limits_{i=1}<sup>{H}(a_i</sup>l-\mu<sup>l)</sup>2}\<br />
&amp;a^l\leftarrow \frac{a<sup>l-\mu</sup>l}{\sigma^l}<br />
\end{aligned}$$</p>
<blockquote>
<p>然后进行偏移：<span class="arithmatex">\(<span class="arithmatex">\(\bar{a}^l\leftarrow \delta^l + \gamma a^l\)</span>\)</span>最后将<span class="arithmatex">\(\bar{a}^l\)</span>进行激活之后输出.<br />
 将以上连起来即可得到网络<span class="arithmatex">\(l\)</span>使用LayerNorm处理之后的输出值：<br />
<span class="arithmatex">\(<span class="arithmatex">\(h^l = f(\gamma \frac{a_i-\mu}{\sigma}+\delta)\)</span>\)</span><br />
 - 其中<span class="arithmatex">\(f\)</span>为激活函数.<br />
 - <mark>注意</mark>，这里提到的<span class="arithmatex">\(a^l\)</span>为网络在未激活之前的值，针对RNN、CNN和全连接层有不同的情形.<br />
 - 对于RNN，在第<span class="arithmatex">\(t\)</span>个时间步，未激活的隐层的输出值为：<span class="arithmatex">\(\mathbf{a}^t=W_{hh}h^{t-1}+W_{xh}\mathbf{x}^t\)</span></p>
<p>PyTorch Code Example: <a href="https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1">Check here</a><br />
  - LayerNorm最初的提出是针对于RNN的，因为BatchNorm中所提到的“批量”在RNN中并不适用，一个想法：RNN中的输入形状为<span class="arithmatex">\((N,T,D)\)</span>，批量大小，时间步和每一个时间步处的维度，RNN所处理的输入的典型特征即为<span class="arithmatex">\(T\)</span>不相同（序列的长度不同），因此如果按照批量计算均值，那么<em>时间步就对不上</em>，也就没有实际意义<br />
  - LayerNorm的优势：<span class="arithmatex">\(\text{LayerNorm}(\mathbf{x})=\text{LayerNorm}(\alpha \mathbf{x})\)</span><br />
^BatchNormVariants</p>
</blockquote>
<details class="note">
<summary>PyTorch: BatchNorm, LayerNorm，以及从数据张量角度理解BatchNorm和LayerNorm <a href="../../Cookbooks/DLCB/#Regularization">Check here</a></summary>
<p>代码实现查看上面的链接.</p>
<p>注意从数据张量的角度，对于<u>一个输入</u>，<u>一个数据样本</u>习惯上<u>只涉及到</u>最后的几个维度上.</p>
<p>例如图片是<span class="arithmatex">\((B,C,H,W)\)</span>，其中<span class="arithmatex">\((C,H,W)\)</span>对应的是一个样本的数据张量（通道数<span class="arithmatex">\(\times\)</span>图片高度<span class="arithmatex">\(\times\)</span>图片宽度）；<br />
或者序列数据<span class="arithmatex">\((B,N,T,D)\)</span>其中<span class="arithmatex">\((T,D)\)</span>对应的是一个样本的数据矩阵（时间步<span class="arithmatex">\(\times\)</span>单个时间步的数据的维度）；</p>
<p>现在我们统一地将一个样本的数据矩阵记为<span class="arithmatex">\(D\)</span>，注意对于图片来说，其实际上的数据矩阵有所不同，具体见<a href="#^BatchNorm">批量正则化</a>注1</p>
<p>那么在上面这种习惯规定下，BatchNorm即为沿着第一个维度<span class="arithmatex">\(B\)</span>对于数据矩阵<span class="arithmatex">\(D_1,D_2,\cdots,D_m\)</span>（假设<span class="arithmatex">\(\lvert B\rvert=m\)</span>）进行正则化. 而LayerNorm则是对于数据矩阵本身进行正则化.</p>
</details>
<details class="note">
<summary>ResNet | Paper: Deep Residual Learning for Image Recognition</summary>
<details class="note">
<summary>衰减问题（Degradation）</summary>
<p>随着网络深度增加，训练误差会很快达到上限（饱和），并且相比于浅层网络训练误差和测试误差都很大.</p>
<p>这个问题并不是由过拟合造成的（深层网络的拟合能力比浅层网络要强 <strong>???</strong>，理论支撑），相反，深层网络完全可以采用这样一种构造：在浅层网络的基础上增加恒等映射层，这样处理效果至少不会比浅层网络差. 但是事实恰好相反.</p>
</details>
<p>考虑到上面的衰减问题，显然不能再采取序列结构网络（一层一层地堆叠块），那么是否可以让网络学习到恒等映射呢（或者比恒等映射效果更好的表示）？</p>
<p>考虑输入<span class="arithmatex">\(\mathbf{x}\)</span>的一个映射<span class="arithmatex">\(\mathcal{H}\)</span>，让网络学习<span class="arithmatex">\(\mathcal{F}(\mathbf{x})=\mathcal{H}(\mathbf{x})-\mathbf{x}\)</span>，从而得到<span class="arithmatex">\(\mathcal{H}\)</span>. 则在最坏的情况下，<span class="arithmatex">\(\mathcal{H}\)</span>也应该学习到一个恒等映射.</p>
<p>残差块可以写作：<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{y}=\mathcal{F}(\mathbf{x};W)+\mathbf{x}\)</span>\)</span>其中<span class="arithmatex">\(\mathbf{x}\)</span>和<span class="arithmatex">\(\mathbf{y}\)</span>分别是残差块接收的输入和输出.</p>
<p>如果<span class="arithmatex">\(\mathcal{F}(\mathbf{x})\)</span>和<span class="arithmatex">\(\mathbf{x}\)</span>的形状不同，可以对<span class="arithmatex">\(\mathbf{x}\)</span>进行线性变换：<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{y}=\mathcal{F}(\mathbf{x};W)+W'\mathbf{x}\)</span>\)</span></p>
<p>^ResNet</p>
</details>
<details class="note">
<summary>DenseNet</summary>
</details>
<details class="note">
<summary>U-Nets</summary>
</details>
<h3 id="_13">注意力机制<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h3>
<details class="abstract">
<summary>概念：非自主性提示，自主性提示</summary>
<ul>
<li>在没有自身注意力的引导下，人们会不自主地受到外界的影响发生注意力转移，而当注意力会受到自身提示的引导时也会发生转移，并且这种自主的影响一般大于外界的影响.</li>
<li><strong>非自主性提示</strong>：环境中的物体的突出性和易见性引导注意力变化，i.e. 黑白照片中的彩色部分；<ul>
<li>神经网络表示非自主性提示：含参数的全连接层，或者不含参数的最大池化层、平均池化层；</li>
</ul>
</li>
<li><strong>自主性提示</strong>：主观意愿引导注意力变化，i.e. 从干扰中专注于当前的阅读.</li>
</ul>
</details>
<details class="abstract">
<summary>概念：查询，键，值</summary>
<p>神经网络中通过给值加权重（注意力）的方式实现注意力汇聚，权重是依据查询（自主性提示）和键（外界环境）之间的符合程度给出的.</p>
<p>i.e. 当我走进一间教室选择位置落座时，教室里面有黑板、第一排、第二排···</p>
<ul>
<li>在没有任何注意力的情况下（即我没有任何想法时），每一个键<span class="arithmatex">\(k_i\)</span>对应一个<u>值</u><span class="arithmatex">\(v_i\)</span>：在这里设置为我前往的意愿值.</li>
<li>现在，我的注意力（<u>查询</u>）<span class="arithmatex">\(q\)</span>为第五排.</li>
<li>与我的查询不符合的键对应的值都会被分配很低的<u>注意力权重</u><span class="arithmatex">\(a(q,v_i)\)</span>，而对于和我的查询符合的键键将会被给予很高的注意力权重.</li>
<li>我开始检查教室里的<u>键</u>和我的查询的符合程度分配权重并计算值：讲台<span class="arithmatex">\(a(q,v_0)=0.01,a(q,v_0)v_0\)</span>、第一排<span class="arithmatex">\(a(q,v_1)=0.02,a(q,v_1)v_1\)</span>····第四排<span class="arithmatex">\(a(q,v_2)=0.2,a(q,v_4)v_4\)</span>···</li>
<li>
<p>第五排这个键和我的查询符合，因此我会基于其对应的值很高的注意力权重<span class="arithmatex">\(a(q,v_5)=0.7\)</span>，加权之后第五排的值大于其他排的值，于是我给出输出：前往第五排.</p>
</li>
<li>
<p><strong>查询</strong>（query）：在注意力机制中自主性提示被称为查询，给定查询之后，通过<strong>注意力凝聚</strong>（attention pooling）将选择引导至<strong>感官输入</strong>（sensory inputs，例如中间特征表示）</p>
</li>
</ul>
</details>
<details class="abstract">
<summary>概念：Nadaraya-Watson核回归（1964），注意力汇聚</summary>
<p>对于一个数据集<span class="arithmatex">\(\{(x_1,y_1),\cdots,(x_n,y_n)\}\)</span>，学习一个<span class="arithmatex">\(f\)</span>预测<span class="arithmatex">\(\hat{y}=f(x)\)</span>；<br />
首先考虑平凡的平均汇聚：<span class="arithmatex">\(<span class="arithmatex">\(f(x)=\frac{1}{n}\sum\limits_{i=1}^{N}\)</span>\)</span><br />
显然，效果是非常差的，因为没有考虑输入<span class="arithmatex">\(x\)</span>的位置会造成影响，<strong>Nadaraya-Watson核回归</strong>为：<span class="arithmatex">\(<span class="arithmatex">\(f(x)=\sum\limits_{i=1}^{N}\frac{K(x-x_i)}{\sum\limits_{j=1}^{n}K(x-x_j)}y_i\)</span>\)</span>其中<span class="arithmatex">\(K\)</span>称为核.</p>
<p>在注意力机制框架下，类似于上面的思想，有<strong>注意力汇聚</strong>（attention pooling）公式：<span class="arithmatex">\(<span class="arithmatex">\(f(x)=\sum\limits_{i=1}^{n}\alpha(x,x_i)y_i\)</span>\)</span></p>
<ul>
<li><span class="arithmatex">\(x\)</span>为查询，<span class="arithmatex">\((x_i,y_i)\)</span>为键值对，<span class="arithmatex">\(\alpha(x,x_i)\)</span>为<strong>注意力权重</strong>（attention weight）</li>
<li>约定注意力权重是一个概率分布：<span class="arithmatex">\(\alpha(x,x_i)\geq0, \sum\limits_{i=1}^{n}\alpha(x,x_i)=1\)</span>；</li>
</ul>
<p>在这个说法下，Nadaraya-Watson是不含参数的注意力汇聚，称为<strong>非参数注意力汇聚</strong>（nonparametric attention pooling）</p>
</details>
<details class="abstract">
<summary>概念：注意力评分函数</summary>
<p>假设有一个查询<span class="arithmatex">\(\mathbf{q}\in\mathbb{R}^q\)</span>，<span class="arithmatex">\(m\)</span>个键值对<span class="arithmatex">\((\mathbf{k}_1,\mathbf{v}_1),\cdots,(\mathbf{k}_m,\mathbf{v}_m)\)</span><br />
注意力权重的计算：先确定<strong>注意力评分函数</strong>（attention scoring function）<span class="arithmatex">\(s(\mathbf{q},\mathbf{k}_i)\)</span>，然后转换为注意力权重（例如通过Sigmoid函数）：<span class="arithmatex">\(<span class="arithmatex">\(\alpha(\mathbf{q},\mathbf{k}_i)=\mathrm{(s(\mathbf{q},\mathbf{k}_i))}=\frac{\exp(s(\mathbf{q},\mathbf{k}_i))}{\sum\limits_{j=1}^{m}\exp(s(\mathbf{q},\mathbf{k}_j))}\)</span>\)</span></p>
</details>
<details class="example">
<summary>高斯核注意力汇聚</summary>
<p>以高斯核（Gaussian kernel）作为注意力评分函数：<span class="arithmatex">\(<span class="arithmatex">\(K(u)=\frac{1}{\sqrt{2\pi}}\exp \left(-\frac{u^2}{2}\right)\)</span>\)</span></p>
<ul>
<li>非参数注意力汇聚：定义查询<span class="arithmatex">\(x\)</span>与键<span class="arithmatex">\(x_i\)</span>之间的注意力权重：$$\begin{aligned}</li>
</ul>
</details>
<p>\alpha(x,x_i)&amp;=\frac{K(x-x_i)}{\sum\limits_{j=1}^{n}K(x-x_j)}\<br />
&amp;=\sum\limits_{i=1}<sup>{n}\mathrm{softmax}\left(-\frac{1}{2}(x-x_i)</sup>2\right)y_i<br />
\end{aligned}<span class="arithmatex">\(<span class="arithmatex">\(这里定义的注意力权重：键\)</span>x_i\)</span>越是接近查询<span class="arithmatex">\(x\)</span>，分配给对应的值<span class="arithmatex">\(y_i\)</span>的注意力权重越大.</p>
<details class="note">
<summary>遮蔽softmax操作/遮蔽注意力</summary>
<p>在某些情况下，并非所有的注意力都需要被计算. 也就是说给定一个序列数据<span class="arithmatex">\([\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_T]\)</span>，在指定<span class="arithmatex">\(\mathbf{x}_m\)</span>为查询时，我们可能不需要汇聚所有的注意力（<span class="arithmatex">\([a[\mathbf{x_1},a[\mathbf{x}_m],a[\mathbf{x_2},\mathbf{x}_m],\cdots,a[\mathbf{x_n},\mathbf{x}_m]]\)</span>）来计算注意力权重，例如在之后介绍的<a href="#^GPT3">GPT3</a>中，对于查询<span class="arithmatex">\(\mathbf{x}_m\)</span>，注意力汇聚时只需要考虑键<span class="arithmatex">\(\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_m\)</span>（其他键则因为一些原因被被遮蔽了），最终在计算<span class="arithmatex">\(\mathbf{x}_m\)</span>对各个键对应的值的注意力权重时：$$\begin{aligned}</p>
</details>
<p>&amp;w[\mathbf{x}<em j="1">i,\mathbf{x}_m]=\frac{\exp(a[\mathbf{x}_i,\mathbf{x}_m])}{\sum\limits</em>\quad 1\leq i\leq m\}^{m}\exp(a[\mathbf{x}_j,\mathbf{x}_m])<br />
&amp;w[\mathbf{x}_i,\mathbf{x}_m]=0\quad m&lt;i\leq n<br />
\end{aligned}$$</p>
<blockquote>
<ul>
<li>注：这里略去了相关的参数.</li>
<li>在实现时，会将要被遮蔽的注意力设置为一个非常大的负值. <strong>???</strong> 最后直接设置为<span class="arithmatex">\(0\)</span>不可以吗？</li>
</ul>
</blockquote>
<details class="note">
<summary>PyTorch: 遮蔽注意力 <a href="../../Cookbooks/DLCB/#^MultiheadAttention">Check here</a></summary>
<p>遮蔽注意力，函数的话下面是一个直接的例子<br />
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">masked_attention</span><span class="p">(</span><span class="n">Seq</span><span class="p">,</span> <span class="n">masked_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">masked_positions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Seq</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">masked_positions</span><span class="p">)):</span>
            <span class="n">Seq</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">masked_positions</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e6</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Seq</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span><span class="p">[:,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 输出未经过遮蔽的注意力权重</span>
<span class="n">masked_attention</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span> <span class="c1"># 分别遮蔽X[0]的第2个和X[1]的第2,3个注意力以计算注意力权重</span>
</code></pre></div><br />
当然也可以用<span class="arithmatex">\(0-1\)</span>矩阵来做Masking，<a href="https://discuss.pytorch.org/t/attn-mask-in-nn-multiheadattention/173603">inspired by here</a><br />
^MaskedAttention</p>
</details>
<p>下面介绍一些注意力评分函数.</p>
<details class="abstract">
<summary>点积注意力</summary>
</details>
<details class="abstract">
<summary>可加性注意力</summary>
<p>当查询<span class="arithmatex">\(\mathbf{q}\in \mathbb{R}^q\)</span>和键<span class="arithmatex">\(\mathbf{k}\in\mathbb{R}^k\)</span>是不同长度的向量时，定义<strong>可加性注意力</strong>（additive attention）的评分函数为：<span class="arithmatex">\(<span class="arithmatex">\(s(\mathbf{q},\mathbf{k})=\mathbf{w}_v^T\tanh(\mathbf{W}_q \mathbf{q}+\mathbf{W}_k \mathbf{k})\in \mathbb{R}\)</span>\)</span></p>
<ul>
<li>可学习参数为<span class="arithmatex">\(\mathbf{W}_q\in \mathbb{R}^{h\times q},\mathbf{W}_k\in \mathbb{R}^{h\times k},\mathbf{w}_v\in \mathbb{R}^h\)</span></li>
<li>该评分函数即将<span class="arithmatex">\(\mathbf{q},\mathbf{k}\)</span>连接起来之后输入到一个多层感知机中.</li>
</ul>
</details>
<details class="abstract">
<summary>缩放点积注意力</summary>
<p>直接将查询<span class="arithmatex">\(\mathbf{q}\)</span>和键<span class="arithmatex">\(\mathbf{k}\)</span>进行点积操作可以衡量两者之间的相似程度，点积操作要求<span class="arithmatex">\(\lvert \mathbf{q}\rvert=\lvert \mathbf{k}\rvert\)</span>. <strong>缩放点积注意力</strong>（scaled dot-product attention）评分函数定义为：<span class="arithmatex">\(<span class="arithmatex">\(s(\mathbf{q},\mathbf{k})=\mathbf{q}^T \mathbf{k}/\sqrt{d}\)</span>\)</span></p>
<ul>
<li>上式的意义是，假设<span class="arithmatex">\(\mathbf{q},\mathbf{k}\)</span>中的所有元素都是独立的随机变量，并且满足均值为<span class="arithmatex">\(0\)</span>，方差为<span class="arithmatex">\(1\)</span>，则两个向量的均值和方差仍然保持不变.</li>
<li>实际操作中使用小批量提高效率，例如对于<span class="arithmatex">\(n\)</span>个查询和<span class="arithmatex">\(m\)</span>个键值对，其中查询和键的长度均为<span class="arithmatex">\(d\)</span>，值的长度为<span class="arithmatex">\(v\)</span>，查询<span class="arithmatex">\(\mathbf{Q}\in \mathbb{R}^{n\times d}\)</span>，键<span class="arithmatex">\(\mathbf{K}\in \mathbb{R}^{m\times d}\)</span>，值<span class="arithmatex">\(\mathbf{V}\in \mathbb{R}^{m\times v}\)</span>的缩放点积注意力为：<span class="arithmatex">\(<span class="arithmatex">\(\text{Softmax}\left(\frac{\mathbf{QK}^T}{\sqrt{d}}\right)\cdot\mathbf{V}\in \mathbb{R}^{n\times v}\)</span>\)</span></li>
<li>使用点积的意义：点积的计算效率相比于其他评分函数中的运算要高.</li>
<li><strong>???</strong> 但缩放点积计算得到的注意力权重和不缩放的一样啊. 意义何在?</li>
</ul>
</details>
<details class="abstract">
<summary>概念：自注意力</summary>
<p>给定一个输入<span class="arithmatex">\(X\)</span></p>
</details>
<details class="note">
<summary>PyTorch: 缩放点积自注意力</summary>
<p>在自注意力运算中需要对于每个批量中的矩阵分别进行计算，PyTorch的<code>torch.bmm</code>提供了这种方法，<a href="../../Cookbooks/DLCB/#^bmm">Check here</a>，并且而需要在每个批量中转置查询Q来完成注意力计算，PyTorch的<code>torch.transpose</code>提供了交换两个维度的方法，<a href="DLCB.md#^transpose">Check here</a> </p>
<p>PyTorch中也提供了实现缩放点积注意力方法，<a href="../../Cookbooks/DLCB/#^MultiheadAttention">Check here</a><br />
<div class="highlight"><pre><span></span><code><span class="n">Embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 批量大小为3，长度为10的序列，词向量大小为5</span>
<span class="k">class</span> <span class="nc">SDP_Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Seq</span><span class="p">):</span>
        <span class="n">Sa</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">Seq</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">Seq</span><span class="p">),</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">))</span>
        <span class="n">Sa_weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Sa</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">Sa_weight</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">(</span><span class="n">Seq</span><span class="p">))</span>
</code></pre></div><br />
下面直接使用PyTorch的框架. 注：这里本来想用PyTorch框架的结果验证我写的代码，但是PyTorch的写法是在多头注意力框架下的，单头注意力为多头注意力的特殊情形，最后还加了一层全连接层（与多头保持一致）具体<a href="../../Cookbooks/DLCB/#^MultiheadAttention">Check here</a><br />
<div class="highlight"><pre><span></span><code><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">Singlehead</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="abstract">
<summary>概念：多头注意力机制</summary>
<p>多头注意力的理念是：在同一个注意力机制下（例如相同的注意力评分函数）应用多个注意力模型，分别关注不同的行为，然后将其组合起来.</p>
<p>一个包含<span class="arithmatex">\(H\)</span>个自注意力头的模型，其第<span class="arithmatex">\(h\)</span>个自注意力头有独立的权重计算值、查询和键：$$\begin{aligned}</p>
</details>
<p>&amp;Q_h=\beta_{qh}+XW_{qh}\in \mathbb{R}^{N\times D_q}\<br />
&amp;K_h=\beta_{kh}+XW_{kh}\in \mathbb{R}^{N\times D_k}\<br />
&amp;V_h=\beta_{vh}+XW_{vh}\in \mathbb{R}^{N\times D_v}\<br />
&amp;D_q=D_k<br />
\end{aligned}$$</p>
<blockquote>
<ul>
<li>这里假设输入大小为<span class="arithmatex">\(N\times D\)</span>.<br />
使用缩放点击注意力，该自注意头的输出可以被写作：<span class="arithmatex">\(<span class="arithmatex">\(\text{Sa}_h[X]=\text{Softmax}\left[\frac{Q_hK_h}
{\sqrt{D_q}}\right]\cdot V_h\in \mathbb{R}^{N\times D_v}\)</span>\)</span><br />
再次强调：每个注意力头有独立的参数<span class="arithmatex">\(\{\beta_{vh},W_{vh}\},\{\beta_{qh},W_{qh}\},\{\beta_{kh},W_{kh}\}\)</span><br />
考虑一种特殊的表示方法：假设输入<span class="arithmatex">\(\mathbf{X}\)</span>的维度为<span class="arithmatex">\(N\times D\)</span>，注意力头的数量为<span class="arithmatex">\(H\)</span>，并设置键、查询和值的权重矩阵大小均为<span class="arithmatex">\(N\times D/H\)</span>，可以将所有注意力头的输出<span class="arithmatex">\(<span class="arithmatex">\(Sa_1[\mathbf{X}],Sa_2[\mathbf{X}],\cdots,Sa_H[\mathbf{X}]\in \mathbb{R}^{D/H}\)</span>\)</span>沿着维度<span class="arithmatex">\(1\)</span>进行连接，组成<span class="arithmatex">\(N\times D\)</span>矩阵之后再次进行线性变换：<span class="arithmatex">\(<span class="arithmatex">\(W_C \begin{bmatrix}\text{Sa}_1[\mathbf{X}] &amp; \text{Sa}_2[\mathbf{X}] &amp; \cdots &amp;\text{Sa}_n[\mathbf{X}]\end{bmatrix}\)</span>\)</span>直观上，这可以理解为多个注意力头平行的进行计算（就像一个卷积层中的多个卷积核一样）然后将结果进行线性变换（卷积核则一般是相加）</li>
<li>多头注意力并不会改变参数矩阵的数量</li>
</ul>
</blockquote>
<details class="note">
<summary>PyTorch：多头注意力</summary>
<p>下面构建的多头用缩放点积作为注意力函数，下面的写法模仿了PyTorch中Multihead的做法，因为我发现如果将Multiheads的权重矩阵写为<code>(num_heads, 3, embed_dim, head_dim)</code>在计算的时候十分难以操作，不如直接写为：<code>(embed_dim * 3, embed_dim=num_heads * head_dim)</code>，然后在计算的时候再做分割.<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span>
        <span class="n">heads_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">heads_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="c1"># 此处，我假设query, key, value的形状均为$(B,N,D)$（批量大小，长度，嵌入维度）</span>
        <span class="n">proj_query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">heads_weights</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">query</span><span class="p">)</span>
</code></pre></div><br />
下面的实现使用了之前的<a href="#^PyTorchScaledDotAttention">缩放点积注意力</a>，但做了一些改编.具体见Jupyter Notebook<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Multi_Head_Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_values</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_heads</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
            <span class="n">heads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">Scaled_Dot_Attention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">heads_weight_Q_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads_weight_K_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="o">.</span><span class="n">K</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads_weight_V_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="nb">input</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div><br />
^PyTorchMultiHeadAttention</p>
</details>
<h4 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h4>
<p><a href="https://www.youtube.com/watch?v=dichIcUZfOw">Youtube</a></p>
<details class="abstract">
<summary>位置编码</summary>
<p>Sources: <a href="https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model">stackexchange</a><br />
自注意力相较于RNN的顺序计算，其并行计算会忽略序列的位置信息，为此提出使用<strong>位置编码</strong>（Position encoding）向序列中注入位置信息，假设长度为<span class="arithmatex">\(N\)</span>，嵌入维度大小为<span class="arithmatex">\(d\)</span>的序列<span class="arithmatex">\(X\)</span>（已经进行过嵌入处理），其位置编码表示为：<span class="arithmatex">\(<span class="arithmatex">\(P=(P_{i,j})_{N\times d}\)</span>\)</span>则加入位置编码之后得到的嵌入为:<span class="arithmatex">\(<span class="arithmatex">\(X+P\)</span>\)</span>（<strong>???</strong> 采用乘法如何？）</p>
<p>位置编码可以通过学习和预设得到，这里介绍的位置编码为预设的，其形式为：$$\begin{aligned}</p>
</details>
<p>&amp;P_{i,2j}=\sin\left(\frac{i}{10000^{2j/d}}\right)\<br />
&amp;P_{i,2j+1}=\cos\left(\frac{i}{10000^{2j/d}}\right)<br />
\end{aligned}$$</p>
<details class="note">
<summary>PyTorch: Position Encoding</summary>
<p><div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">PositionEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;这里使用的是绝对？相对？位置编码&#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_pos</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_pos</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">divided</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">10000.</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># 这里我看见别人写的代码：torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.) / d_model))</span>
        <span class="c1"># 然后 pe[:, 0::2] = torch.sin(pos * divided)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_pos</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="n">divided</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="n">divided</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)])</span> <span class="c1"># 这里默认是d_model是偶数，如果奇数，则需要计算的divided应该增加一个长度</span>
        <span class="k">if</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div><br />
^PyTorchPositionEncoding</p>
</details>
<details class="abstract">
<summary>逐位前馈神经网络</summary>
<p>逐位前馈神经网络（positionwise feed-forward network, PFFN）使用相同的MLP对于序列中的所有位置进行处理.</p>
<p>逐位前馈神经网络和感知机的概念相同，使用“逐位”所表达的意思是：对于一个输入形状为<span class="arithmatex">\((B,T,D)\)</span>的数据（批量大小、时间步、词嵌入维度），传入到PFFN中，每一个时间步的数据将被单独进行处理，并输出，因而称之为“逐位”.</p>
</details>
<details class="note">
<summary>PyTorch：逐位前馈神经网络</summary>
<p>和一般的感知机完全相同<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">PFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">),</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">)</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="abstract">
<summary>Transformer</summary>
<p>Transformer中所涉及到的主要结构为多自注意力头和<a href="#^BatchNormVariants">LayerNorm</a>方法，两者组成一个块（或者子层，sublayer）：$$\begin{aligned}</p>
</details>
<p>&amp;\mathbf{X}\leftarrow \mathbf{X} + \text{MhSa}[\mathbf{X}]\<br />
&amp;\mathbf{X}\leftarrow \text{LayerNorm}[\mathbf{X}]\<br />
&amp;\mathbf{X}_n\leftarrow \mathbf{X}_n+\text{mlp}[\mathbf{X}_n]\<br />
&amp;\mathbf{X}\leftarrow \text{LayerNorm}[\mathbf{X}]<br />
\end{aligned}$$</p>
<blockquote>
<p>Transformer模型有三种类型，这里针对<a href="#自然语言处理">#自然语言处理</a>：<br />
- <strong>编码器</strong>（Encoder）：将文本转化为<a href="#^Embeddings">词嵌入</a>之后转换为能够处理多种任务的表示；<br />
- <strong>解码器</strong>（Decoder）：根据输入的文本预测下一个词元；<br />
- <strong>自解码器</strong>（Encoder-decoder）：通常用于<strong>序列到序列</strong>（sequence-to-sequence）任务中，将文本首先进行编码，然后解码，例如机器翻译.<br />
^Transformer</p>
</blockquote>
<details class="note">
<summary>PyTorch: Transformer <a href="../../Cookbooks/DLCB/#^Transformer">Check here</a></summary>
<p>输入首先转换为<a href="#^Embedding">Embedding</a>，然后加上<a href="#^PyTorchPositionEncoding">位置编码</a>，</p>
<p>下面搭建Transformeer的Encoder的一个子块，其包含两个子层：</p>
<p>之前已经提到过使用<a href="#^PyTorchScaledDotSelfAttention">Scaled Dot Self-Attention</a> 多头注意力即为Scaled Dot Self-Attention的简单推广，见<a href="#^PyTorchMultiheadAttention">Multihead Attention</a></p>
<ul>
<li>
<p>子层<span class="arithmatex">\(1\)</span>：<br />
-&gt; <span class="arithmatex">\(X_0\)</span>（实际向多头注意力层传入<span class="arithmatex">\((X_0,X_0,X_0)\)</span>） <br />
-&gt; 多头注意力层 <span class="arithmatex">\(Y_0\)</span> <br />
-&gt; Dropout <span class="arithmatex">\(Y_0'\)</span><br />
-&gt; 残差连接 <span class="arithmatex">\(X_0+Y_0'\)</span><br />
-&gt; 层正则化 <span class="arithmatex">\(X_1\)</span></p>
</li>
<li>
<p>子层<span class="arithmatex">\(2\)</span>：<br />
-&gt; <span class="arithmatex">\(X_1\)</span><br />
-&gt; MLP <span class="arithmatex">\(Y_1\)</span><br />
-&gt; Dropout <span class="arithmatex">\(Y_1'\)</span><br />
-&gt; 残差连接 <span class="arithmatex">\(X_1+Y_1'\)</span><br />
-&gt; 层正则化 <span class="arithmatex">\(X_2\)</span></p>
</li>
</ul>
</details>
<details class="abstract">
<summary>概念：迁移学习</summary>
<p><strong>迁移学习</strong>（transfer learning）的思想是：为了实现某一任务（称之为<u>主任务</u>），首先在一个<u>与其相关</u>（i.e. 都是NLP）的任务（称之为<u>第二任务</u>）上通过<strong>自监督学习</strong>（self-supervision）<strong>预训练</strong>（pre-train）一个模型，然后将其应用于主任务上进行训练.</p>
<ul>
<li>迁移学习的一种使用场景是主任务的数据量较少，但是第二任务的数据量多，足以训练一个良好的适用于第二任务的模型；</li>
<li>迁移模型在具体实现方面有以下形式：<ul>
<li>冻结预训练好的模型，其层的参数完全固定，在此基础上添加层，或者移除掉网络最后的一些层后冻结，在此基础上添加层进行训练；</li>
<li>人工监督学习，<strong>微调</strong>（fine-tune）</li>
</ul>
</li>
<li>预训练的一个例子：以下面介绍的BERT为例，其在预训练阶段的主要任务是“完形填空”：根据一个给出的不完整的句子，在空处填入词元. 在训练过程中BERT使用了包含<span class="arithmatex">\(3.3b\)</span>的语料库，最大输入词元数为<span class="arithmatex">\(512\)</span>，批量大小为<span class="arithmatex">\(256\)</span></li>
</ul>
</details>
<details class="abstract">
<summary>编码器举例: BERT</summary>
<p>BERT属于一种编码器，其所使用的词表包含<span class="arithmatex">\(30000\)</span>个词元，输入文本在词元化之后转化为大小为<span class="arithmatex">\(30000\times T\)</span>的独热向量组成的矩阵，词嵌入处理之后转化为大小为<span class="arithmatex">\(1024\times T\)</span>的词嵌入，将词嵌入传入到<span class="arithmatex">\(24\)</span>个Transfomer中（Transfomer的结构如上），每一个Transformer包含<span class="arithmatex">\(16\)</span>大小的多头自注意力，每个自注意力头的查询、键和值的大小均为<span class="arithmatex">\(64\times T\)</span>（即<span class="arithmatex">\(\Omega_{vh},\Omega_{qh},\Omega_{kh}\)</span>的大小分别为<span class="arithmatex">\(64\times1024\)</span>），最终输出的大小仍然为<span class="arithmatex">\(1024\times T\)</span>，Transformer中的多层感知机的全连接层的隐层大小为<span class="arithmatex">\(4096\)</span>. Bert的参数数量约为<span class="arithmatex">\(340m\)</span>.</p>
<details class="note">
<summary>进一步了解BERT，参数数量计算</summary>
<ul>
<li><strong>嵌入矩阵</strong>（Embedding matrices）<ul>
<li>词嵌入矩阵（Word embedding eatrix）：<span class="arithmatex">\(\text{Vocabulary size}\times \text{Embedding dimension}=30522\times 768\)</span></li>
<li>位置编码矩阵（Position embedding matrix）：<span class="arithmatex">\(\text{Maximum sequence length}\times \text{Embedding dimension}=512\times 768\)</span></li>
<li>词元类型嵌入矩阵（Token type embedding matrix）：<span class="arithmatex">\(2\times 768\)</span></li>
<li>层正则化：<span class="arithmatex">\(768 + 768\)</span>（权重和偏置）</li>
<li>总数量：</li>
</ul>
</li>
<li><strong>注意力头</strong>:<ul>
<li><span class="arithmatex">\(12\)</span>个注意力头的参数总量：每个注意力头的查询、键、值权重矩阵（大小全部相同，均为<span class="arithmatex">\(768\times 64\)</span>）：<span class="arithmatex">\(768\times 64\times 3\)</span>，偏置：<span class="arithmatex">\(768\times 3\)</span>，每一个<span class="arithmatex">\(12\)</span>头注意力的参数总量：<span class="arithmatex">\(12 * (784\times 64+368)\times 3*\)</span></li>
<li>多注意力头之后的全连接层（Dense weight for projection after concatenation of heads）：<span class="arithmatex">\(768\times 768\)</span>，偏置：<span class="arithmatex">\(768\)</span></li>
<li>层正则化：<span class="arithmatex">\(768+768\)</span></li>
<li>逐位前馈神经网络（Positionwise feedforward network）：<span class="arithmatex">\(3072\times768+3072 + 768\times3072+368\)</span></li>
<li>层正则化：<span class="arithmatex">\(768+768\)</span></li>
</ul>
</li>
<li><strong>输出层</strong>：<ul>
<li>Dense Weight Matrix and Bias <span class="arithmatex">\(768\times 768 + 768\)</span></li>
</ul>
</li>
</ul>
</details>
<p>BERT使用了迁移学习，在预训练阶段，模型从大型语料库中进行<strong>自监督学习</strong>（self-supervision），模型在这一阶段学到一些大概的语言特性（例如：_指甲，模型更可能填入“剪”，而不是“撕”，语言特性也可理解为一种弱常识），在微调阶段，使用相对预训练阶段要小的数据进行监督学习，以学会某一样具体的任务 <strong>???</strong></p>
<p>BERT在经过预训练阶段之后进入到微调阶段，在原有网络的基础上，针对不同的任务添加了一些层（例如MLP）</p>
<ul>
<li>文本分类（Text classificatiion）：在BERT的预训练阶段，每一段文本的都添加了一个特殊的词元\&lt;cls&gt;（放在文本的最开始）. 例如，对于情感分析（积极、消极）</li>
</ul>
</details>
<details class="abstract">
<summary>解码器举例: GPT3</summary>
<p>NLP中解码器的目的：生成下一个词元，从而可以生成一段文本.<br />
之前已经介绍了<a href="#^AutoregressiveModel">自回归模型</a>，在解码器的训练过程中，输入数据为一段文本（转换为词嵌入），解码器需要给出这段文本的联合概率. i.e. 输入一段文本转换为词嵌入<span class="arithmatex">\(\{\mathbf{x}_1,\mathbf{x}_2,\cdots, \mathbf{x}_T\}\)</span>那么模型给出的联合概率：$$\begin{aligned}</p>
</details>
<p>&amp;Pr(\mathbf{x}<em T-1="T-1">1,\mathbf{x}_2,\cdots,\mathbf{x}_T)=\<br />
&amp;Pr (\mathbf{x}_1)\times Pr(\mathbf{x}_2 \,|\,\mathbf{x}_1)\cdots\cdots Pr(\mathbf{x}_T \,|\,\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}</em>)<br />
\end{aligned}$$</p>
<blockquote>
<p>在这里产生问题：如果直接将整段文本直接用于训练，那么模型其实是可以通过注意力机制注意到<span class="arithmatex">\(\mathbf{x}_m\)</span>之后的数据<span class="arithmatex">\(\mathbf{x}_{m+1},\cdots\)</span>，而在实际测试时，模型并不能注意到这些不存在的文本（模型需要生成它们），所以这样做是有问题的，直观上相当于开卷. 为此需要遮蔽<span class="arithmatex">\(\mathbf{x}_m\)</span>之后的注意力以计算权重. 需要用到<a href="#^MaskedAttention">注意力遮蔽</a>. <strong>一个直观的图</strong>：<br />
- 此外，应用注意力遮蔽之后，序列中每一个词向量的概率都只取决于先前的词，计算结果可以循环使用. <strong>???</strong> 需要更详细的算法<br />
- GPT3的这种学习策略，一种出乎意料的效果是其可以在不进行微调的情况下进行其他类型的任务：纠正语法错误、写代码、翻译... <strong>???</strong> 需要再补充（Few-shot learning）<br />
^GPT3</p>
</blockquote>
<details class="note">
<summary>Paper: Attention is all you need</summary>
<details class="note">
<summary>Optimizer</summary>
<p><a href="#^Adam">Adam</a> optimizer: <span class="arithmatex">\(\beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9}\)</span></p>
<p>Adjust the learning rate over the course of training: <span class="arithmatex">\(<span class="arithmatex">\(lr=d_{model}^{-0.5}\cdot \min(step\_num^{-0.5}, step\_num\cdot warmup\_steps^{-1.5})\)</span>\)</span> </p>
<p><span class="arithmatex">\(warmup\_steps=4000\)</span></p>
</details>
</details>
<details class="note">
<summary>维数处理 &amp; 数据矩阵角度看待深度学习中的常见方法</summary>
<p>本文和<a href="#^Graph">计算图 &amp; 自动微分</a>一样结合PyTorch讨论深度学习中的问题.</p>
</details>
<h3 id="_14">图神经网络<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h3>
<details class="abstract">
<summary>图</summary>
<p>一个带有边的点集</p>
<ul>
<li>图：<span class="arithmatex">\((V,E,U)\)</span></li>
<li>简单有向图：<span class="arithmatex">\(G=(V,E,\varphi)\)</span></li>
</ul>
</details>
<details class="abstract">
<summary>图神经网络的下游任务</summary>
<ul>
<li>图层面</li>
<li>边层面</li>
<li>节点层面</li>
</ul>
</details>
<details class="note">
<summary>谱域卷积方法</summary>
</details>
<h3 id="_15">生成对抗神经网络<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h3>
<details class="note">
<summary>PyTorch: 一个自编码器的例子</summary>
<p>看这篇用Tensorflow写的文章不错，有空试着用PyTorch实现<a href="https://www.tensorflow.org/tutorials/generative/autoencoder">Autocoder</a>, 计算生物前沿课程中有一个简单的概览[[自编码器（Auto-encoder）]]</p>
</details>
<details class="abstract">
<summary>概念</summary>
<p><strong>生成对抗网络</strong>（General Adversial Network, GAN）的目的是学习真实数据的概率分布<span class="arithmatex">\(\mathcal{D}\)</span>. 其特点在于：<br />
1) 最终获得的网络不借助Markov链或者其他概率方法（不是概率模型），<strong>生成器</strong>（Generator）完全借助<strong>隐变量</strong>（Latent variable）生成数据；<br />
2) 生成器并不直接接触真实数据，由<strong>判别器</strong>（Discriminator）计算损失；这样做的主要原因为了同时让生成器和判别器提升.</p>
</details>
<details class="note">
<summary>初代GAN算法：minimax损失函数 + 小批量随机梯度下降</summary>
<p>Ian Goodfellow et.(2014)最早提出的GAN算法.</p>
<p>定义<u>生成器</u><span class="arithmatex">\(G(z;\theta_G)\)</span>：<span class="arithmatex">\(<span class="arithmatex">\(G：z\in \mathcal{Z}\rightarrow x^*\in \mathcal{X}\)</span>\)</span>，其中<span class="arithmatex">\(z\sim P_z\)</span>，于是生成器隐式地对应于一个分布<span class="arithmatex">\(P_G\)</span>；</p>
<p><u>真实数据</u><span class="arithmatex">\(x\in \mathcal{X}\)</span>，<span class="arithmatex">\(x\sim P_{data}\)</span>.</p>
<p><u>定义判别器</u><span class="arithmatex">\(D(x;\theta_D)\)</span>：<span class="arithmatex">\(<span class="arithmatex">\(D: x\in\mathcal{X} \rightarrow y\in[0,1]\)</span>\)</span></p>
<p>设置超参数<span class="arithmatex">\(k\)</span>，<span class="arithmatex">\(m\)</span></p>
<p>1) 从<span class="arithmatex">\(1\)</span>迭代到<span class="arithmatex">\(k\)</span>，从<span class="arithmatex">\(P_{data}\)</span>中采样<span class="arithmatex">\(m\)</span>个样本<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{x}=\{x_1,x_2,\cdots,x_m\}\)</span>\)</span>从<span class="arithmatex">\(P_{z}\)</span>中采样<span class="arithmatex">\(m\)</span>个样本，由<span class="arithmatex">\(G\)</span>映射到<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{x}^*=\{x_1^*,x_2^*,\cdots,x_n^*\}\)</span>\)</span>其中<span class="arithmatex">\(x_i^*=G(z_i),i=1,2,\cdots,n\)</span>.<br />
 定义<u>判别器的损失函数</u>：<span class="arithmatex">\(<span class="arithmatex">\(L(D,\mathbf{x},\mathbf{x}^*)=-\frac{1}{m}\sum\limits_{j=1}^{m}\log(D(x_j))-\frac{1}{m}\sum\limits_{i=1}^{m}\log(1-D(x_i^*))\)</span>\)</span><br />
 用梯度更新算法更新判别器<span class="arithmatex">\(D\)</span>：<span class="arithmatex">\(<span class="arithmatex">\(\theta_D\leftarrow\theta_D-\nabla_{\theta_G}L(D,G)\)</span>\)</span><br />
2) 从<span class="arithmatex">\(P_{G}\)</span>中采样<span class="arithmatex">\(m\)</span>个样本<span class="arithmatex">\(\mathbf{x}'=\{x_1^*,x_2^*,\cdots,x_m^*\},x_i^*=G(z_i),i=1,2,\cdots,m\)</span>，<u>计算判别器的损失（固定判别器）</u>：<span class="arithmatex">\(<span class="arithmatex">\(C(G,\mathbf{x}')=+\frac{1}{m}\sum\limits_{i=1}^{m}\log (1-D(G(z_i)))\)</span>\)</span>用梯度更新算法更新生成器<span class="arithmatex">\(D\)</span>（注意上面是加号）：<span class="arithmatex">\(<span class="arithmatex">\(\theta_G\leftarrow \theta_G-\nabla_{\theta_G}C(G,\mathbf{x}')\)</span>\)</span></p>
</details>
<details class="note">
<summary>初代GAN理论</summary>
<p>通过误差的反向传递对于<span class="arithmatex">\(f\)</span>进行求解：<br />
<span class="arithmatex">\(<span class="arithmatex">\(\lim_{\sigma\rightarrow0}\nabla_x E_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}f(x+\epsilon)=\nabla_xf(x)\)</span>\)</span></p>
<p>1) 在交叉熵损失函数度量下，判别器最优化得到：<span class="arithmatex">\(<span class="arithmatex">\(D^*=\frac{P_{data}}{P_G+P_(data)}\)</span>\)</span>hint：以上的GAN算法相当于最大化：<span class="arithmatex">\(<span class="arithmatex">\(V(G,D)=\int_{x}P_{data}(x)\log(D(x))dx+\int_{x^*}P_g(x^*)\log(1-D(x)))dx^*\)</span>\)</span><br />
2) 定义<span class="arithmatex">\(C(G)=\max_{D}V(G,D)\)</span>，<span class="arithmatex">\(C(G)\)</span>取得最小值当且仅当<span class="arithmatex">\(P_g=P_{data}\)</span>，最小值为<span class="arithmatex">\(-\log 4\)</span>.<br />
 $$\begin{aligned}</p>
</details>
<p>C(G)&amp;=\max_{D}V(G,D)\<br />
&amp;=E_{x\sim P_{data}}[\log D^<em>(x)]+E_{x\sim P_{G}}[\log(1-D^</em>(x))]\<br />
&amp;=E_{x\sim P_{data}}\left[\log \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}\right]+E_{x^<em>\sim P_G}\left[\log \frac{P_G(x<sup>*)}{P_{data}(x)+P_G(x</sup></em>)}\right]<br />
\end{aligned}$$</p>
<blockquote>
<p><strong>3)</strong> 如果每一次<span class="arithmatex">\(G,D\)</span>都能达到最优化状态，则<span class="arithmatex">\(P_G\rightarrow P_{data}\)</span></p>
</blockquote>
<details class="note">
<summary>PyTorch: 一个玩具GAN</summary>
<p>这里实现一个GAN，其所要学习的表示很简单：<span class="arithmatex">\(<span class="arithmatex">\(\begin{bmatrix}x_1 &amp;0 &amp;0&amp;0 \\ x_2 &amp; x_3 &amp; 0 &amp;0 \\ x_4 &amp; x_5 &amp; x_6 &amp; 0  \\ x_7 &amp; x_8 &amp; x_9 &amp; x_{10}\end{bmatrix}\quad x_i\in \mathcal{U}(1,5)\)</span>\)</span><br />
定义一个隐变量：<span class="arithmatex">\(z\in \mathcal{N}(0,1)\)</span><br />
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;真实分布&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">data</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">from_</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">to</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">from_</span><span class="p">,</span> <span class="n">to</span><span class="p">))</span> <span class="c1"># 返回下三角形矩阵</span>

<span class="sd">&#39;&#39;&#39;生成器&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_shape</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">data_shape</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_shape</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="n">data_shape</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_shape</span> <span class="o">=</span> <span class="n">latent_shape</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num</span><span class="p">,)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latent_shape</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">train</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="sd">&#39;&#39;&#39;判别器&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_shape</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">data_shape</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">train</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">D</span><span class="p">,</span> <span class="n">G</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(),</span> <span class="n">Discriminator</span><span class="p">()</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># 只生成值看看</span>
<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">D</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="c1"># 只生成值看看</span>
</code></pre></div><br />
下面制定一个训练方案，训练周期设为<span class="arithmatex">\(N\)</span>，一次首先取<span class="arithmatex">\(m\)</span>个真实数据样本和<span class="arithmatex">\(m\)</span>个生成样本，训练<span class="arithmatex">\(k\)</span>次判别器；再取<span class="arithmatex">\(m\)</span>个生成样本，以判别器返回的损失训练生成器.<br />
<div class="highlight"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="n">optimizer1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">optimizer2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer1</span><span class="p">,</span> <span class="n">optimizer2</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">wait_me</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># 首先训练判别器</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">wait_me</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
        <span class="c1"># 每一次先训练k次判别器</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">rd</span> <span class="o">=</span> <span class="n">data</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>
                <span class="n">fd</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="c1"># 默认：train=False</span>
                <span class="n">y_rd</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">rd</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">y_fd</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                <span class="n">loss1</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_rd</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y_rd</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">+</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_fd</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_fd</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
                <span class="n">loss1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">optimizer1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">optimizer1</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 训练1次生成器</span>
        <span class="n">fd_</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">y_fd_</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">fd_</span><span class="p">)</span>
        <span class="n">loss2</span> <span class="o">=</span> <span class="o">-</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_fd_</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_fd_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">loss2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># print(y_fd_)</span>
        <span class="c1"># print(loss2)</span>
        <span class="n">optimizer2</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer2</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>DCGAN</summary>
</details>
<details class="note">
<summary>对GAN的理论分析 Paper: Towards Principled Methods for Training Generative Adversarial Networks</summary>
<p><strong>!!!</strong> 本部分对于流形部分的理论只进行罗列，没有证明.</p>
<p>GAN是一个<a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash均衡</a>问题. 包含两个损失函数</p>
<ul>
<li>
<p>原始论文中使用的Sigmoid函数本身的缺陷：如果判别器训练地过好，使得生成数据与真实数据之间能完全分开时，Sigmoid函数返回的梯度会很小，也就使得梯度接近于<span class="arithmatex">\(0\)</span>，这是生成器无法训练起来的一个原因：生成器的损失太低了.（这里引用UDL中的一张图片）</p>
</li>
<li>
<p>此外在实践中，根据理论分析生成器的损失应该至多为：<span class="arithmatex">\(<span class="arithmatex">\(-2\log 2+2D_{JS}(P_{data}\ \Vert\ P_G)\)</span>\)</span>（当判别器收敛时），但是实际上损失达到了<span class="arithmatex">\(0\)</span>，这种结果出现的原因只有两种可能：</p>
<ul>
<li>分布是不连续的；</li>
<li>两个分布具有不交的<strong>支撑集</strong>（support，低维空间中的流形）这个有证据支撑：经验和理论说明<span class="arithmatex">\(P_{data}\)</span>实际上依赖于低维流形（extremely concentrated on a low dimensional manifold），而对于<span class="arithmatex">\(P_{G}\)</span>，其是根据隐变量<span class="arithmatex">\(z\sim P(z)\)</span>生成的；如果<span class="arithmatex">\(z\in \mathcal{Z}\)</span>，那么<span class="arithmatex">\(P_G\)</span>的支撑将包含在低维流形的一个并下，因此在<span class="arithmatex">\(\mathcal{X}\)</span>中的测度为<span class="arithmatex">\(0\)</span>. <mark>所以问题的主要原因在于</mark>：所选择的隐变量空间的维度与实际上要生成的图片（或者是其他）的==支撑集的维度不同==！</li>
</ul>
</li>
</ul>
<p>针对上面所谈到的问题，有以下定理可以说明：</p>
<details class="note">
<summary>引理：常规神经网络函数作用在隐变量上形成的概率分布包含在低于该隐变量维度的低维流形的并集上</summary>
<p>设<span class="arithmatex">\(g:\mathcal{Z}\rightarrow \mathcal{X}\)</span>为由仿射变换和逐点非线性函数（i.e. rectifiers, leaky rectifiers, smooth strictly incrreasing functions (sigmoid, tanh, softplus, etc.)）复合而成的函数，则<span class="arithmatex">\(g(\mathcal{Z})\)</span>将包含在维数小于<span class="arithmatex">\(\mathcal{Z}\)</span>的低维流形的可数并集中. 从而，如果<span class="arithmatex">\(\mathcal{X}\)</span>的维度低于<span class="arithmatex">\(\mathcal{X}\)</span>，那么<span class="arithmatex">\(g(\mathcal{Z})\)</span>在<span class="arithmatex">\(\mathcal{X}\)</span>中的测度为<span class="arithmatex">\(0\)</span>.</p>
</details>
<p>总之：如果<span class="arithmatex">\(P_{data}\)</span>和<span class="arithmatex">\(P_G\)</span>的支撑集是不交的或者是低维流形，那么总（极大可能）存在判别器能够将两者很好地分开. 这解释了为什么一个最优的判别器会直接干爆生成器.</p>
<p>下面这个定理则说明了在生成分布和真实分布的支撑不交的情况下，生成器无法优化.</p>
<details class="note">
<summary>定理：对于支撑不交的两个分布，存在一个光滑的判别器完全分离之，并且梯度为<span class="arithmatex">\(0\)</span></summary>
<p>设<span class="arithmatex">\(P_{data},P_G\)</span>具有分别包含在两个不交紧致集合<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>中的支撑；则存在<span class="arithmatex">\(D^*:\mathcal{X}\rightarrow[0,1]\)</span>，其满足<span class="arithmatex">\(P_{data}[D(x)=1]=1,P_{G}[D(x)=0]=1\)</span>，并且<span class="arithmatex">\(<span class="arithmatex">\(\nabla_xD^*(x)=0,\forall x\in \mathcal{M}\cup \mathcal{P}\)</span>\)</span></p>
</details>
<details class="note">
<summary>定义：横截性</summary>
<p>设<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>是流形<span class="arithmatex">\(\mathcal{F}=\mathbb{R}^d\)</span>的两个无界正规子流形（boundary free regular submanifolds），设<span class="arithmatex">\(x\in \mathcal{M}\cap \mathcal{P}\)</span>，称<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>在<span class="arithmatex">\(x\)</span>点<strong>横截</strong>（trasversally）相交，如果<span class="arithmatex">\(T_x\mathcal{M}+T_x\mathcal{P}=T_x\mathcal{F}\)</span>，其中<span class="arithmatex">\(T_x\mathcal{M}\)</span>表示<span class="arithmatex">\(x\)</span>点附近的切空间</p>
</details>
<details class="note">
<summary>定义：完全平行</summary>
<p>称两个无界流形<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span><strong>完全平行</strong>（perfectly align）如果存在<span class="arithmatex">\(x\in \mathcal{M}\cap \mathcal{P}\)</span>使得<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>在<span class="arithmatex">\(x\)</span>不横截相交</p>
</details>
<p>下面这个定理说明任何两个低维流形不能完全平行</p>
<details class="note">
<summary>引理：两个不满维的正规子流形的扰动不完全平行的概率为<span class="arithmatex">\(1\)</span></summary>
<p>设<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>为<span class="arithmatex">\(\mathbb{R}^d\)</span>的两个正规子流形并且不满维，设<span class="arithmatex">\(\eta,\eta'\)</span>为两个任意的<u>独立连续随机变量</u>. 定义扰动流形：<span class="arithmatex">\(\widetilde{\mathcal{M}}=\mathcal{M}+\eta,\widetilde{\mathcal{P}}=\mathcal{P}+\eta'\)</span>则有：<span class="arithmatex">\(<span class="arithmatex">\(P_{\eta,\eta'}(\widetilde{\mathcal{M}}\text{ does not perfectly align with }\widetilde{\mathcal{P}})=1\)</span>\)</span></p>
</details>
<details class="note">
<summary>引理：不完全平行且不满维的正规子流形<span class="arithmatex">\(\mathcal{M}\)</span>，<span class="arithmatex">\(\mathcal{P}\)</span>的交<span class="arithmatex">\(\mathcal{L}\)</span>在<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>中的测度均为<span class="arithmatex">\(0\)</span></summary>
<p>设<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>为两个<span class="arithmatex">\(\mathbb{R}^d\)</span>的正规子流形，不完全平行并且不满维度. 如果<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>无界则令<span class="arithmatex">\(\mathcal{L}=\mathcal{M}\cap\mathcal{P}\)</span>，则<span class="arithmatex">\(\mathcal{L}\)</span>也是一个流形，并且维度严格小于<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>；如果<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>有界，则<span class="arithmatex">\(\mathcal{L}\)</span> is a union of at most 4 stricly lower dimensional manifolds. 在两种情况下<span class="arithmatex">\(\mathcal{L}\)</span>在<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>中的测度均为<span class="arithmatex">\(0\)</span>.</p>
</details>
<details class="note">
<summary>定理：</summary>
<p>设<span class="arithmatex">\(P_{data}\)</span>和<span class="arithmatex">\(P_G\)</span>为两个概率分布，其支撑集包含在两个<u>不完全平行</u>且<u>不满维</u>的闭流形<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>中，假设两个概率分布<span class="arithmatex">\(P_{data}\)</span>和<span class="arithmatex">\(P_G\)</span>在其各自的流形中是连续的（即，如果有一个在<span class="arithmatex">\(\mathcal{M}\)</span>中测度为<span class="arithmatex">\(0\)</span>的集合<span class="arithmatex">\(A\)</span>，则<span class="arithmatex">\(P_r(A)=0\)</span>，对<span class="arithmatex">\(P_G\)</span>也是如此）</p>
<p>从而，存在一个最优的判别器<span class="arithmatex">\(D^*:\mathcal{X}\rightarrow[0,1]\)</span>满足：<span class="arithmatex">\(<span class="arithmatex">\(P_{data}(D(x)=1)=1,P_{G}(D(x)=0)=1\)</span>\)</span>并且对于几乎任何<span class="arithmatex">\(x\in \mathcal{M}\)</span>或者<span class="arithmatex">\(x\in \mathcal{P}\)</span>，<span class="arithmatex">\(D^*\)</span>在<span class="arithmatex">\(x\)</span>的附近是平滑的，且<span class="arithmatex">\(<span class="arithmatex">\(\nabla_xD^*(x)=0\)</span>\)</span></p>
</details>
<p>总之，以上两个定理说明了对于两个不完全平行的低维流形<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>，总存在一个光滑的判别器将其完全分开，并且满足<span class="arithmatex">\(<span class="arithmatex">\(P_{data}(D(x)=1)=1,P_{G}(D(x)=0)=0\)</span>\)</span>从而无法进行梯度更新.</p>
<p>下面这个定理则说明了用KL散度和JS散度本身存在的问题.</p>
<details class="note">
<summary>定理：KL散度和JS散度的性质</summary>
<p>设<span class="arithmatex">\(P_{data}\)</span>和<span class="arithmatex">\(P_{G}\)</span>的支撑集分别包含在两个子流形<span class="arithmatex">\(\mathcal{M}\)</span>和<span class="arithmatex">\(\mathcal{P}\)</span>中，<span class="arithmatex">\(\mathcal{M},\mathcal{P}\)</span>均不满维并且不完全相交，假设<span class="arithmatex">\(P_{data}\)</span>和<span class="arithmatex">\(P_G\)</span>在其相对应的流形中是连续的，则有：$$\begin{aligned}</p>
</details>
</details>
<p>&amp;D_{JS}(P_{data} \Vert P_G)=\log 2\<br />
&amp;D_{KL}(P_{data} \Vert P_G)=+\infty\<br />
&amp;D_{KL}(P_G \Vert P_{data})=+\infty<br />
\end{aligned}$$</p>
<blockquote></blockquote>
<details class="note">
<summary>WGAN</summary>
<p>Wasserstein距离相比于<span class="arithmatex">\(KL\)</span>散度和<span class="arithmatex">\(JS\)</span>散度的优越性在于：对于两个没有重叠的分布，Wasserstein距离仍然能够进行度量.</p>
<p><strong>Wasserstein距离</strong>（或称Earth-Mover距离，EM）：<span class="arithmatex">\(<span class="arithmatex">\(W(P_{data},P_G)=\int_{\gamma\in \prod (P_{data}, P_G)}E_{(x,y)\sim \gamma}\left[\left\lVert x-y\right\rVert\right]\)</span>\)</span>其中<span class="arithmatex">\(\prod(P_{data},P_G)\)</span>为<span class="arithmatex">\(P_{data}\)</span>和<span class="arithmatex">\(P_G\)</span>的联合分布的集合：对于<span class="arithmatex">\(\gamma\in\prod(P_{data},P_{G})\)</span>，其边缘分布分别为<span class="arithmatex">\(P_{data},P_G\)</span>，也可以从规划的角度理解这件事：<span class="arithmatex">\(\gamma\)</span>是一个路径规划，Wasserstein距离计算的即为移动分布<span class="arithmatex">\(P_{data}\)</span>到<span class="arithmatex">\(P_G\)</span>所消耗的质量（反之也是）<mark>（用Wiki的图）</mark></p>
<p>作者举了一个例子来说明Wasserstein距离的优越性：</p>
<p>但计算Wasserstein距离是一件十分困难的事情，但其具有对偶形式：<span class="arithmatex">\(<span class="arithmatex">\(W(P_{data},P_G)=\frac{1}{K}\{\sup_{\left\lVert f\right\rVert_L\leq K}E_{x\sim P_{data}}[f(x)]-E_{x\sim P_G}[f(x)]\}\)</span>\)</span>其中<span class="arithmatex">\(\left\lVert f\right\rVert_L\leq K\)</span>指的是<span class="arithmatex">\(f\)</span>满足Lipschitz连续条件：<span class="arithmatex">\(<span class="arithmatex">\(\left\lvert f(x_1)-f(x_2)\right\rvert\leq K\left\lvert x_1-x_2\right\rvert,\forall x_1,x_2\in D\)</span>\)</span>其中<span class="arithmatex">\(D\)</span>为<span class="arithmatex">\(f\)</span>的定义域，<mark>该条件等价于</mark><span class="arithmatex">\(\left\lvert \nabla f\right\rvert\leq K\)</span>，则只需要如下求解Wesserstein距离：<span class="arithmatex">\(<span class="arithmatex">\(W(P_{data},P_G)\approx\frac{1}{K}\max_{w:\left\lVert f_w\right\rVert_L\leq K} E_{x\sim P_{data}}[f_w(x)]-E_{s\sim P_{G}}[f_w(x)]\)</span>\)</span></p>
<p>为了满足Lipshitz条件，一种非常简单的做法是==Clip==：<span class="arithmatex">\(<span class="arithmatex">\(w_i\in[-0.01,0.01]\)</span>\)</span>，另外在实际计算中也不必关心<span class="arithmatex">\(K\)</span>，学习率调参即可.</p>
</details>
<details class="note">
<summary>Papers: Wasserstein GAN</summary>
</details>
<details class="note">
<summary>WGAN算法</summary>
<p>为了求得Wasserstein距离，不再使用Sigmoid函数限制判别器的输出（其输出也不直接表示为损失，需要进一步计算得到Wasserstein距离）</p>
<p>因此相对初代GAN有以下改动：<br />
1) 判别器的最后一层不为挤压函数（i.e. Sigmoid）；<br />
2) 判别器的损失更改，相应的生成器的损失也更改；<br />
3) 作者本人通过实验指出，不应当使用动量进行优化.</p>
<p>设置学习率<span class="arithmatex">\(\alpha\)</span>，截断参数<span class="arithmatex">\(c\)</span>，批量大小<span class="arithmatex">\(m\)</span>，每迭代一次生成器对应判别器的迭代次数<span class="arithmatex">\(n\)</span>，设生成器<span class="arithmatex">\(<span class="arithmatex">\(G:z\in \mathcal{Z}\rightarrow x\in \mathcal{X}\)</span>\)</span>参数为<span class="arithmatex">\(\theta_G\)</span>，设判别器<span class="arithmatex">\(<span class="arithmatex">\(D:x\in \mathcal{X}\rightarrow y\in \mathbb{R}\)</span>\)</span>参数为<span class="arithmatex">\(\theta_D\)</span>.</p>
<p>当<span class="arithmatex">\(\thete_G\)</span>未收敛时：</p>
<p>从<span class="arithmatex">\(1\)</span>迭代到<span class="arithmatex">\(n\)</span>，每次迭代从<span class="arithmatex">\(P_{data}\)</span>中采样<span class="arithmatex">\(m\)</span>个样本<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{x}=\{x_1,x_2,\cdots,x_m\}\)</span>\)</span>从<span class="arithmatex">\(P_{z}\)</span>中采样<span class="arithmatex">\(m\)</span>个样本，并由<span class="arithmatex">\(G\)</span>映射到：<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{x}^*=\{x_1^*,x_2^*,\cdots,x_m^*\}\)</span>\)</span>其中<span class="arithmatex">\(x_i^*=G(z_i),i=1,2,\cdots,m\)</span>，计算梯度：<span class="arithmatex">\(<span class="arithmatex">\(g_{\theta_D}=\nabla_{\theta_D}\left[\frac{1}{m}\sum\limits_{i=1}^{m}f_{\theta_D}(x_{i})-\frac{1}{m}\sum\limits_{i=1}^{m}f_{\theta_D}(x^*_{i})\right]\)</span>\)</span>对<span class="arithmatex">\(\theta_D\)</span>进行梯度更新：$$\begin{aligned}</p>
</details>
<p>&amp;\theta_D\leftarrow \theta_D+\alpha\cdot \text{RMSProp}(\theta_D,g_{\theta_D})\<br />
&amp;\theta_D\leftarrow \text{clip}(\theta_D,-c,c)<br />
\end{aligned}$$结束迭代.</p>
<blockquote>
<p>从<span class="arithmatex">\(P_z\)</span>中采样<span class="arithmatex">\(m\)</span>个样本，计算梯度并更新参数<span class="arithmatex">\(\theta_G\)</span>（注意是尽可能增大Wasserstein距离，所以加负号）：<span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
&amp;\theta_G\leftarrow -\nabla_{\theta_G}\frac{1}{m}\sum\limits_{i=1}^{m}D(G(z_i))\\
&amp;\theta_G\leftarrow \theta-\alpha\cdot \text{RMSProp}(\theta_G,g_{\theta_G})
\end{aligned}\)</span>\)</span></p>
</blockquote>
<details class="note">
<summary>RMSProp</summary>
<p>在<a href="#adagrad">#^AdaGrad</a>中提到学习率下降太快了，所以一种简单的做法是<br />
^RMSProp</p>
</details>
<details class="note">
<summary>WGAN-GP</summary>
<p><span class="arithmatex">\(<span class="arithmatex">\(L(D)=-E_{x\sim P_{data}}[D(x)]+E_{x\sim P_G}[D(x)]+\lambda E_{x\sim \mathcal{X}}[\left\lVert \nabla_xD(x)\right\rVert_p-K]^2\)</span>\)</span><br />
其中对于惩罚项的计算：<span class="arithmatex">\(<span class="arithmatex">\(x\sim P_{data},x^*\sim P_G,\epsilon\sim \text{Uniform}(0,1)\)</span>\)</span>接着进行随机插值抽样：<span class="arithmatex">\(<span class="arithmatex">\(\hat{x}=\epsilon x+(1-\epsilon)x^*\)</span>\)</span>这样就得到了一个分布<span class="arithmatex">\(\mathcal{P}_{x}\)</span>然后计算：<span class="arithmatex">\(<span class="arithmatex">\(\lambda E_{x\sim \mathcal{P}_{x}}[\left\lVert \nabla_xD(x)\right\rVert_p-K]^2\)</span>\)</span><br />
上面这种采样的思想是：重点关注生成样本集中区域、真实样本集中区域和两者之间的区域.</p>
</details>
<h2 id="part-ii-advanced-topics">Part II: Advanced Topics<a class="headerlink" href="#part-ii-advanced-topics" title="Permanent link">&para;</a></h2>
<details class="info">
<summary>Info</summary>
<p>This part focus on more theoretical/practical topics.</p>
</details>
<h3 id="theory">Theory<a class="headerlink" href="#theory" title="Permanent link">&para;</a></h3>
<h3 id="application">Application<a class="headerlink" href="#application" title="Permanent link">&para;</a></h3>
<h4 id="how-to-select-hyperparameters">How to select hyperparameters?<a class="headerlink" href="#how-to-select-hyperparameters" title="Permanent link">&para;</a></h4>
<p>Sources: <a href="https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning">stackexchange</a></p>
<details class="questions">
<summary>In what order shoule we tune hyperparameters?</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/37467647/in-what-order-should-we-tune-hyperparameters-in-neural-networks">stackoverflow</a></p>
</details>
<h2 id="_16">附录<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h2>
<h3 id="_17">数学<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h3>
<h4 id="_18">微积分<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h4>
<details class="note">
<summary><span class="arithmatex">\(n\)</span>元局部Taylor公式</summary>
<p>设<span class="arithmatex">\(D\subset \mathbb{R}^n\)</span>为开区域，<span class="arithmatex">\(\mathbf{x}^0=(x_1^0,x_2^0,\cdots,x_n^0)\in D\)</span>，<span class="arithmatex">\(\Delta \mathbf{x}=(\Delta x_1,\Delta x_2,\cdots, \Delta x_n)\)</span>若函数<span class="arithmatex">\(f:\mathbb{R}^n\rightarrow \mathbb{R}^1\)</span>在<span class="arithmatex">\(D\)</span>上<span class="arithmatex">\(m\)</span>次可微，则有：<span class="arithmatex">\(<span class="arithmatex">\(f(\mathbf{x}_0+\Delta \mathbf{x})=\mathbf{f}(\mathbf{x}_0)+\sum\limits_{k=1}^{m}\frac{1}{k!}\sum\limits_{i_1,i_2,\cdots,i_k=1}^{n}\frac{\partial^k{f}}{\partial{x_{i_1}}\partial{x_{i_2}}\cdots \partial{x_{i_k}}}(\mathbf{x}^0)\Delta x_{i_1}\Delta x_{i_2}\cdots \Delta x_{i_k}+R_m(\Delta \mathbf{x}),\lvert \Delta \mathbf{x}\rvert\rightarrow0\)</span>\)</span><br />
^Taylor</p>
</details>
<details class="note">
<summary>映射微分链式法则</summary>
<p>设<span class="arithmatex">\(D\subset \mathbb{R}^n\)</span>为开区域，<span class="arithmatex">\(F:D\rightarrow \mathbb{R}^m\)</span>，<span class="arithmatex">\(\Omega\subset \mathbb{R}^l\)</span>为开区域，$G:\Omega\rightarrow \mathbb{R}^n</p>
</details>
<details class="note">
<summary>矩阵微分</summary>
<p>Sources: <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_matrices">Wikipedia</a></p>
<ul>
<li><strong>标量函数被矩阵变量微分</strong>：设<span class="arithmatex">\(y:\mathbb{R}^{p\times q}\rightarrow \mathbb{R}\)</span>，<span class="arithmatex">\(\mathbf{X}\in \mathbb{R}^{p\times q}\)</span>，则有：<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial{y}}{\partial{\mathbf{X}}}=\begin{bmatrix}\frac{\partial{y}}{\partial{x_{11}}}&amp;\frac{\partial{y}}{\partial{x_{21}}}&amp;\cdots \frac{\partial{y}}{\partial{x_{p1}}}\end{bmatrix}\)</span>\)</span></li>
</ul>
</details>
<h4 id="_19">概率论<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>概率分布</summary>
<details class="note">
<summary>Dirichlet分布</summary>
</details>
</details>
<details class="note">
<summary>支撑</summary>
<p>设函数<span class="arithmatex">\(f:X\rightarrow \mathbb{R}\)</span>，定义<span class="arithmatex">\(f\)</span>的<strong>支撑</strong>（support）为：<span class="arithmatex">\(<span class="arithmatex">\(\text{supp}(f)=\{x\in X:f(x)\neq0\}\)</span>\)</span></p>
<p>一个概率分布的支撑指的是该分布中非零概率的值所组成的集合.</p>
</details>
<details class="note">
<summary>KL散度</summary>
<p>Sources: <a href="">Wikipedia</a><br />
测量概率分布<span class="arithmatex">\(p(x)\)</span>和<span class="arithmatex">\(q(x)\)</span>之间的差异：<span class="arithmatex">\(<span class="arithmatex">\(D_{KL}[p(x)\,
\Vert\,q(x)]=\int p(x)\log \left[\frac{p(x)}{q(x)}\right]dx\)</span>\)</span><br />
离散情形：<span class="arithmatex">\(<span class="arithmatex">\(D_{KL}\left[p(x)\,\Vert\,q(x)\right]=\sum\limits_{x\in \mathcal{X}}^{}p(x)\log \left[\frac{p(x)}{q(x)}\right]\)</span>\)</span><br />
KL散度总是大于<span class="arithmatex">\(0\)</span>的：$$\begin{aligned}</p>
</details>
<p>D_{KL}[p(x)\,\Vert\,]&amp;=\int p(x)\log \left[\frac{p(x)}{q(x)}\right]dx\<br />
&amp;=-\int p(x)\log \left[\frac{q(x)}{p(x)}\right]dx\<br />
&amp;\geq \int p(x)\left(1-\frac{q(x)}{p(x)}\right)dx=0\<br />
\end{aligned}$$</p>
<blockquote>
<p>对于KL散度的一个浅显的解释是：要计算两个概率分布<span class="arithmatex">\(p(x),q(x)\)</span>之间的差异，需要计算在给定观测<span class="arithmatex">\(x\)</span>之下<span class="arithmatex">\(p(x)\)</span>和<span class="arithmatex">\(q(x)\)</span>之间的差异的期望，我们假设<span class="arithmatex">\(x\sim p(x)\)</span>，或者说<span class="arithmatex">\(p(x)\)</span>是真实的概率分布，<span class="arithmatex">\(q(x)\)</span>是模型给出的概率分布，那么如果<span class="arithmatex">\(q(x)=p(x),\forall x\)</span>，那么按照KL散度，两者之间的差距为<span class="arithmatex">\(0\)</span>，否则如果两者差异较大，无论是<span class="arithmatex">\(\frac{p(x)}{q(x)}\)</span>过小，还是<span class="arithmatex">\(\frac{p(x)}{q(x)}\)</span>过大，都会使得KL散度变大.</p>
</blockquote>
<h4 id="_20">信息论<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>交叉熵</summary>
<p>Sources: <a href="https://en.wikipedia.org/wiki/Cross-entropy">Wikipedia</a><br />
连续形式：<br />
离散形式：<br />
估计真实交叉熵，Monte Carlo测量（经验交叉熵）：</p>
</details>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="最后更新">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2024年4月6日</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="创建日期">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3h-2Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2024年3月31日</span>
  </span>

    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../SL/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 统计学习">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                统计学习
              </div>
            </div>
          </a>
        
        
          
          <a href="../SageMath/" class="md-footer__link md-footer__link--next" aria-label="下一页: SageMath">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                SageMath
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy 2024 zoeminus
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:2210377@mail.nankai.edu.cn" target="_blank" rel="noopener" title="Email" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.7l167.6-182.9c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/zoeplus" target="_blank" rel="noopener" title="Github Profile" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["revision.date", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.action.edit", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.bd41221c.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="../../javascripts/tabSync.js"></script>
      
    
  </body>
</html>