
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../Coding/Python/">
      
      
        <link rel="next" href="../PyDCB/">
      
      
      <link rel="icon" href="../../imgs/zxr.ico">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.16">
    
    
      
        <title>PyTorch - zoeminus</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.bcfcd587.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      

  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 7v2h4l-4 6v2h6v-2h-4l4-6V7H9m3-5a10 10 0 0 1 10 10 10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2Z"/></svg>');}</style>


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#part-i-fundation" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="zoeminus" class="md-header__button md-logo" aria-label="zoeminus" data-md-component="logo">
      
  <img src="../../imgs/zx.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            zoeminus
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PyTorch
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/zoeplus/zoeminus" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    zoeminus
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="zoeminus" class="md-nav__button md-logo" aria-label="zoeminus" data-md-component="logo">
      
  <img src="../../imgs/zx.png" alt="logo">

    </a>
    zoeminus
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/zoeplus/zoeminus" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    zoeminus
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    一
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            一
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/Set/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    集合论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/R/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    实数理论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/MA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数学分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/LAlg/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    高等代数
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/GTopo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一般拓扑学
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/RF/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    实变函数
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/MAlg/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    矩阵代数
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    九
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            九
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/Prob/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    概率论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/Stat/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数理统计
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/Opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    最优化理论
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/DSA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据结构与算法
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/CLT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    计算学习理论
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    二十二
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            二十二
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/DM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据挖掘
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/SL/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    统计学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/DL/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    深度学习
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    二十
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            二十
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Math/SageMath/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SageMath
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Coding/C.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Coding/CPP/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C++
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Coding/Python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    十九
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            十九
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    PyTorch
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    PyTorch
  </span>
  

      </a>
      
        

  

<nav class="md-nav md-nav--secondary" aria-label="本文组织：">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      本文组织：
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#part-i-fundation" class="md-nav__link">
    <span class="md-ellipsis">
      Part I: Fundation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Part I: Fundation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchtensor-other-functions" class="md-nav__link">
    <span class="md-ellipsis">
      torch.tensor &amp; other functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.tensor & other functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    <span class="md-ellipsis">
      Random Variables
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsorted" class="md-nav__link">
    <span class="md-ellipsis">
      Unsorted
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-functions-for-tensors" class="md-nav__link">
    <span class="md-ellipsis">
      Common Functions for Tensors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more" class="md-nav__link">
    <span class="md-ellipsis">
      More
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchlinalg" class="md-nav__link">
    <span class="md-ellipsis">
      torch.linalg
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchutilsdata" class="md-nav__link">
    <span class="md-ellipsis">
      torch.utils.data
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.utils.data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datadataset" class="md-nav__link">
    <span class="md-ellipsis">
      data.Dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datadataloader" class="md-nav__link">
    <span class="md-ellipsis">
      data.DataLoader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datasampler" class="md-nav__link">
    <span class="md-ellipsis">
      data.Sampler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnn" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.nn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cnn" class="md-nav__link">
    <span class="md-ellipsis">
      CNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    <span class="md-ellipsis">
      RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnfunctional" class="md-nav__link">
    <span class="md-ellipsis">
      nn.functional
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnparameter" class="md-nav__link">
    <span class="md-ellipsis">
      nn.parameter
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptim" class="md-nav__link">
    <span class="md-ellipsis">
      torch.optim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more_1" class="md-nav__link">
    <span class="md-ellipsis">
      More
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchvision" class="md-nav__link">
    <span class="md-ellipsis">
      torchvision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torchvision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transforms" class="md-nav__link">
    <span class="md-ellipsis">
      transforms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtext" class="md-nav__link">
    <span class="md-ellipsis">
      torchtext
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torchtext">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchtextdatautils" class="md-nav__link">
    <span class="md-ellipsis">
      torchtext.data.utils
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtextvocab" class="md-nav__link">
    <span class="md-ellipsis">
      torchtext.vocab
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsorted-notes" class="md-nav__link">
    <span class="md-ellipsis">
      Unsorted Notes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hugging-face" class="md-nav__link">
    <span class="md-ellipsis">
      Hugging Face
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../PyDCB/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python DM & ML
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    零零五五
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            零零五五
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Divisadero/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    遥望
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Relate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    溶
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Different.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    异
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Falling.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    落
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/LucidDreaming.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    清醒的梦
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/WithLLM.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对话
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_7" >
        
          
          <label class="md-nav__link" for="__nav_7_7" id="__nav_7_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    存在主义
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_7">
            <span class="md-nav__icon md-icon"></span>
            存在主义
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/Existential/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    存在 I
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Parrot/B%26N/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对存在的追求
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  

<nav class="md-nav md-nav--secondary" aria-label="本文组织：">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      本文组织：
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#part-i-fundation" class="md-nav__link">
    <span class="md-ellipsis">
      Part I: Fundation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Part I: Fundation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchtensor-other-functions" class="md-nav__link">
    <span class="md-ellipsis">
      torch.tensor &amp; other functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.tensor & other functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    <span class="md-ellipsis">
      Random Variables
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsorted" class="md-nav__link">
    <span class="md-ellipsis">
      Unsorted
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-functions-for-tensors" class="md-nav__link">
    <span class="md-ellipsis">
      Common Functions for Tensors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more" class="md-nav__link">
    <span class="md-ellipsis">
      More
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchlinalg" class="md-nav__link">
    <span class="md-ellipsis">
      torch.linalg
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchutilsdata" class="md-nav__link">
    <span class="md-ellipsis">
      torch.utils.data
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.utils.data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datadataset" class="md-nav__link">
    <span class="md-ellipsis">
      data.Dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datadataloader" class="md-nav__link">
    <span class="md-ellipsis">
      data.DataLoader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datasampler" class="md-nav__link">
    <span class="md-ellipsis">
      data.Sampler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnn" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torch.nn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cnn" class="md-nav__link">
    <span class="md-ellipsis">
      CNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rnn" class="md-nav__link">
    <span class="md-ellipsis">
      RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnfunctional" class="md-nav__link">
    <span class="md-ellipsis">
      nn.functional
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nnparameter" class="md-nav__link">
    <span class="md-ellipsis">
      nn.parameter
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchoptim" class="md-nav__link">
    <span class="md-ellipsis">
      torch.optim
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more_1" class="md-nav__link">
    <span class="md-ellipsis">
      More
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchvision" class="md-nav__link">
    <span class="md-ellipsis">
      torchvision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torchvision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transforms" class="md-nav__link">
    <span class="md-ellipsis">
      transforms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtext" class="md-nav__link">
    <span class="md-ellipsis">
      torchtext
    </span>
  </a>
  
    <nav class="md-nav" aria-label="torchtext">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchtextdatautils" class="md-nav__link">
    <span class="md-ellipsis">
      torchtext.data.utils
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtextvocab" class="md-nav__link">
    <span class="md-ellipsis">
      torchtext.vocab
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsorted-notes" class="md-nav__link">
    <span class="md-ellipsis">
      Unsorted Notes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hugging-face" class="md-nav__link">
    <span class="md-ellipsis">
      Hugging Face
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/zoeplus/zoeminus/commits/main/docs/Cookbooks/DLCB.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


  <h1>PyTorch</h1>

<h2 id="part-i-fundation">Part I: Fundation<a class="headerlink" href="#part-i-fundation" title="Permanent link">&para;</a></h2>
<details class="note">
<summary>Look for help</summary>
<p>In Python:<br />
<div class="highlight"><pre><span></span><code><span class="c1"># i.e. check torch.nn&#39;s all functions</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">dir</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="p">))</span>

<span class="c1"># i.e. get help on a function, class or something</span>
<span class="n">help</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">)</span>
</code></pre></div><br />
PyTorch Forum<br />
PyTorch Tutorials<br />
PyTorch Documentation</p>
</details>
<h3 id="torchtensor-other-functions">torch.tensor &amp; other functions<a class="headerlink" href="#torchtensor-other-functions" title="Permanent link">&para;</a></h3>
<details class="warning">
<summary>When creating Tensor using ndarray</summary>
<p>The two will share storage:<br />
<div class="highlight"><pre><span></span><code><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">arr</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">A</span> <span class="c1"># tensor([0., 0., 0.], dtype=torch.float64)</span>

<span class="c1">## not for a scalar, PyTorch will copy a new one</span>
<span class="n">arr_scalar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr_scalar</span><span class="p">)</span>
<span class="n">arr_scaler</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">B</span> <span class="c1"># tensor(3., dtype=torch.float64)</span>

<span class="c1"># tolist</span>
<span class="n">X</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>Basic Operations for Tensors</summary>
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;create tensors&#39;&#39;&#39;</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_repeat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># repeat 3 times along dim=0, 2 times along dim=1</span>

<span class="n">X</span><span class="o">.</span><span class="n">data</span> <span class="c1"># acquire the tensor without gradient</span>
<span class="n">X_clone</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">clone</span> <span class="c1"># this will create the same tensor as X, but don&#39;t share the same storage. </span>
<span class="c1"># ATTENTION: this will be recoded by autograd (when set requires_grad=True), you can use X_clone = X.detach().clone() to disable this</span>

<span class="sd">&#39;&#39;&#39;tensor&#39;s shape and number of dimensions&#39;&#39;&#39;</span>

<span class="n">arrayX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">arrayX</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># return (2,5,10)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># return torch.Size([2,5,10]) with respects to </span>
<span class="c1"># dim 0, dim 1, dim 2. here the dim is same to the axis in NumPy</span>
<span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="c1"># difference?</span>

<span class="sd">&#39;&#39;&#39;tensor&#39;s dtype&#39;&#39;&#39;</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="c1"># return torch.int64</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="c1"># return torch.float32</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;common arithmetic functions&#39;&#39;&#39;</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">Y</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">Y</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_sum</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">X_sum_axis0</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># decrease dimension along axis=0</span>
<span class="n">X_mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="c1"># count num of elements</span>

<span class="sd">&#39;&#39;&#39;matrix operation&#39;&#39;&#39;</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span> <span class="c1"># like matrix multiplication, you can try A = torch.ones(4), which will be different with torch.ones(1,4)</span>
<span class="n">A</span><span class="nd">@B</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="n">X_trans</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span> <span class="c1"># return torch.Size([3,2,6]) batch matrix multiplication</span>

<span class="sd">&#39;&#39;&#39;squeeze and unsqueeze&#39;&#39;&#39;</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Remove length-1 dimension only, otherwise returning the same tensor</span>
<span class="n">X</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Expand a dimension</span>

<span class="sd">&#39;&#39;&#39;concatenate multiple tensors&#39;&#39;&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">cat_XYZ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">Z</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cat_XYZ</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># return torch.Size([1,10,3])</span>

<span class="sd">&#39;&#39;&#39;stack multiple tensors&#39;&#39;&#39;</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># difference between torch.cat and torch.stack</span>

<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># return torch.Size([2,3,4]), concatenate along a new dimension</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># return torch.Size([6,4]), concatenate in the given dimension</span>

<span class="sd">&#39;&#39;&#39;Anti-Stack/Cat&#39;&#39;&#39;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="question">
<summary>Difference between <code>reshape</code> and <code>view</code>?</summary>
<p><a href="https://stackoverflow.com/questions/42479902/what-does-view-do-in-pytorch">Sources</a><br />
<code>reshape</code>: return a new tensor<br />
<code>view</code>: share storage</p>
</details>
<details class="note">
<summary>Device</summary>
<p><a href="https://stackoverflow.com/questions/50954479/using-cuda-with-pytorch">Use cuda</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="c1"># check if your gpu works</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span> <span class="c1"># set X computed by GPU</span>
<span class="n">X</span><span class="o">.</span><span class="n">device</span>

<span class="c1"># here is an example</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_availabe</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>Autograd</summary>
<p><div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">grad_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
            <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]],</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f_X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">f_X</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span>
<span class="n">X</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1"># clear the grad</span>

<span class="sd">&#39;&#39;&#39;quicker ways to require grad when you have many tensors to do so&#39;&#39;&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">]:</span>
    <span class="n">t</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div><br />
^RoughAutograd</p>
</details>
<details class="note">
<summary>More About Autograd</summary>
<p>Sources: <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">PyTorch Tutorials</a>, <a href="https://stackoverflow.com/questions/57248777/backward-function-in-pytorch">stackoverflow</a>, <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html">Documentation</a>, <a href="https://pytorch.org/docs/stable/notes/autograd.html">PyTorch Autograd</a><br />
<div class="highlight"><pre><span></span><code><span class="n">Tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inpus</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>create_graph</code>: if <code>True</code>, the graph of the <strong>derivative</strong> will be constructed, allowing to compute higher oreder derivative products.<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">pow</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">d1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div><br />
By defaults (with no arguments): <code>backward()</code> is called on a scaler<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">6.</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A</span><span class="o">.</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div><br />
Note that PyTorch <strong>does not</strong> support non-scaler function derivatives. Any non-scaler tensors <span class="arithmatex">\(\mathbf{M}\)</span> are regarded as intermediates (or local nodes in computational graphs), and PyTorch always expected that there exists some loss <span class="arithmatex">\(L\)</span> (scaler), and it can calculate <span class="arithmatex">\(\frac{\partial{L}}{\partial{\mathbf{M}}}\)</span> <mark>according to the chain rules</mark>.</li>
</ul>
<p>i.e. the following <code>Y</code> is a vector, when you call backward on it, PyTorch expected you give it the "upstream" gradients it need to calculate <span class="arithmatex">\(\frac{\partial{L}}{\partial{\mathbf{Y}}}\)</span> (it images there exists <span class="arithmatex">\(L\)</span>). Below I gives <code>torch.ones_like(X)</code> as the "upstream" gradient. It can look like <span class="arithmatex">\(\mathbf{Y}\)</span> is calculated by some function as (actually not): L=y1+y2+y3+C,C is some constants,∂L∂Y=[1,1,1] then the gradients of <code>X</code> is calculated as: ∂L∂X=∂L∂Y∂YX=[1,1,1]∘[4,6,8]=[4,6,8]<br />
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;You can &#39;add&#39; grad to a tensor&#39;&#39;&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Y</span> <span class="c1"># a vector</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
</code></pre></div><br />
There still some confusing things according to the Chain rule, see the below examples<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">A</span><span class="p">)</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">]))</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div><br />
<strong>???</strong> need more study<br />
^Autograd</p>
</details>
<details class="question">
<summary>Difference between <code>detach</code> and <code>with torch.no_grad</code></summary>
<p>Sources: <a href="https://stackoverflow.com/questions/56816241/difference-between-detach-and-with-torch-nograd-in-pytorch">stackoverflow</a></p>
<ul>
<li><code>tensor.detach()</code> <strong>creates</strong> a tensor that <u>does not requires grad</u>, which shares the same storage with the original tensor. And it detaches the tensor from the computational graph.<br />
<div class="highlight"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span> <span class="c1"># tensor([1., 1., 1., 1.], requires_grad=True)</span>
<span class="n">X_de</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">X_de</span> <span class="c1"># tensor([1., 1., 1., 1.])</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="c1"># tensor([-0.1955], grad_fn=\&lt;AddBackward0&gt;)</span>
<span class="n">Y_de</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">Y_de</span> <span class="c1"># tensor([-0.1955])</span>
</code></pre></div></li>
</ul>
</details>
<details class="question">
<summary>What is PyTorch Graph?</summary>
<p>Run the below code you will get<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">)</span>
<span class="n">X</span> <span class="c1"># tensor([1.], requires_grad=True)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span>
<span class="n">Y</span> <span class="c1"># tensor([2.], grad_fn=\&lt;MulBackward0&gt;)</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># tensor([2.])</span>
<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 
<span class="c1"># ...... Calls into the C++ engine to run the backward pass </span>
<span class="c1"># RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed)</span>
<span class="c1"># Saved intermediate values of the graph are freed when you call .backward() or autograd.grad()</span>
<span class="c1"># Specify retain_graph = True if you need to backward through the graph a second or if you need to access saved tensors after calling backward()</span>
<span class="n">X</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>

<span class="n">Y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">Y</span> <span class="c1"># </span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">Z</span> <span class="c1"># </span>
</code></pre></div><br />
^PyTorchGraph</p>
</details>
<details class="note">
<summary>masked_fill_</summary>
<p><code>Tensor.masked_fill\_(mask, value)</code></p>
<ul>
<li><code>mask</code> (BoolTensor)<br />
<div class="highlight"><pre><span></span><code><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.6</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">20.</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mf">19.</span><span class="p">)</span>
</code></pre></div></li>
</ul>
</details>
<details class="note">
<summary>PyTorch BroadCasting</summary>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">10.</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># let X.shape be (10,1)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">X</span> <span class="o">/</span> <span class="n">Y</span>
</code></pre></div>
</details>
<h4 id="random-variables">Random Variables<a class="headerlink" href="#random-variables" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>torch.rand</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.rand.html">Documentation</a><br />
Get a uniform distribution in <span class="arithmatex">\([r_1,r_2]\)</span>：<br />
<div class="highlight"><pre><span></span><code><span class="n">r1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">r2</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="p">(</span><span class="n">r_2</span><span class="o">-</span><span class="n">r_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">r_2</span>
</code></pre></div><br />
Make masks:<br />
<div class="highlight"><pre><span></span><code><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.6</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>torch.randn</summary>
<p>Standard normal distribution.<br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>out</code>: the output tensor</li>
<li><code>layout</code>: the storage layout of the tensor</li>
<li><code>device</code>: <code>torch.device('cuda' if cuda.is_available() else 'cpu')</code></li>
</ul>
</details>
<details class="note">
<summary>Tensors' in-place random sampling</summary>
<p><a href="https://pytorch.org/docs/stable/torch.html#inplace-random-sampling">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">4.</span><span class="p">])</span>

<span class="sd">&#39;&#39;&#39;Uniform distribution&#39;&#39;&#39;</span>

<span class="n">X</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">from</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">to</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;Bernoulli Distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Cauchy distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Exponential distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Geometric distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Log-normal distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Normal distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Discrete uniform distribution&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Continuous uniform distribution&#39;&#39;&#39;</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>torch.Generator</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</code></pre></div><br />
Creates and returns a <strong>generator</strong> object, used as a <u>keyword argument</u> in many <strong>in-place</strong> random sampling.</p>
<ul>
<li><code>get_state()</code>: Returns the Generator <u>state</u> as a <code>torch.ByteTensor</code>, which contains all the necessary <u>bits</u> to <u>restore a Generator</u>.</li>
<li><code>set_state()</code></li>
<li><code>manual_seed(seed)</code>: the seed can be any 32-bit integer, returning a <code>torch.Generator</code> object<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">19</span><span class="p">)</span>
<span class="n">state_0</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span> <span class="c1"># get generator&#39;s current state</span>
<span class="n">X</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="c1"># this will change X&#39;s value in place</span>
<span class="n">state_1</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="c1"># this will return a different tensor</span>
<span class="c1"># note that when you pass the keyword argument generator to the in-palce sampling (i.e. normal_)</span>
<span class="c1"># you just tell PyTorch which generator to use, it will not reset the states for you</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state_0</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state_1</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
</code></pre></div><br />
^Generator</li>
</ul>
</details>
<h4 id="unsorted">Unsorted<a class="headerlink" href="#unsorted" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>torch.repeat_interleave</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">ouput_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<p>Repeat elements of a tensor.</p>
<ul>
<li>repeats(Tensor or int): number of repetitions for each elemts</li>
<li>dim(int, opt): the dimension along which to repeate values. By default, will return a flat output array with repeated values.</li>
</ul>
</details>
<details class="question">
<summary><code>reapeat_interleave</code> and <code>repeat</code> in making attention masks (the former is True)</summary>
<p><a href="https://stackoverflow.com/questions/68205894/how-to-prepare-data-for-tpytorchs-3d-attn-mask-argument-in-multiheadattention/75447422#comment120670107_68205894">Sources</a></p>
</details>
<details class="note">
<summary>torch.nan_to_num</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</code></pre></div>
</details>
<h4 id="common-functions-for-tensors">Common Functions for Tensors<a class="headerlink" href="#common-functions-for-tensors" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>Elementary Functions</summary>
<p>Sources: [Wikipedia](https://en.wikipedia.org/wiki/Elementary_function#:~:text=In%20mathematics%2C%20an%20elementary%20function,inverse%20functions%20(e.g.%2C%20arcsin%2C)<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>Activation functions</summary>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="note">
<summary>torch.argmax and torch.max</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
</details>
<h4 id="more">More<a class="headerlink" href="#more" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>Triangular matrices</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.triu.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></p>
</details>
<h3 id="torchlinalg">torch.linalg<a class="headerlink" href="#torchlinalg" title="Permanent link">&para;</a></h3>
<details class="note">
<summary>Norm</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li>for a complex value <code>x</code>, return <code>x.abs()</code></li>
<li><code>dim=None</code>, flatten and compute norm. <code>dim</code> is an <code>int</code> or <code>tuple</code>, compute along the dimensions.</li>
<li><code>ord</code>: <code>inf</code> for max(abx(x)), <code>-inf</code> for min(abs(x)), <code>0</code> for sum(x!=0). Other int or float: <span class="arithmatex">\(<span class="arithmatex">\(\left(\sum\limits_{\lvert x_i\rvert}^{\text{ord}}\right)^{1/\text{ord}}\)</span>\)</span></li>
</ul>
</details>
<h3 id="torchutilsdata">torch.utils.data<a class="headerlink" href="#torchutilsdata" title="Permanent link">&para;</a></h3>
<h4 id="datadataset">data.Dataset<a class="headerlink" href="#datadataset" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>data.Dataset</summary>
<p><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">Documentation</a><br />
Base class for all PyTorch map-styple datasets. All subclasses should overwrite <code>__getitem__()</code>. Because different datasets have different map.</p>
<p>Supporting fetching a data sample for a given key (for hashable object)</p>
</details>
<details class="example">
<summary>Custom Your Dataset</summary>
<p>Overwrite <code>__getitem__</code>, <code>__len__</code><br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="o">...</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>       
</code></pre></div></p>
<p>Here is an advanced example<br />
Sources: <a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">PyTorch Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">io</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
<span class="kn">from</span> <span class="nn">torchvison</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="k">class</span> <span class="nc">FaceLandmarksDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">,</span> <span class="n">root_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>

        <span class="n">img_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span>
        <span class="n">landmarks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">landmark_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="n">landmarks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">landmarks</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;image&#39;</span><span class="p">:</span><span class="n">image</span><span class="p">,</span> <span class="s1">&#39;landmarks&#39;</span><span class="p">:</span><span class="n">landmarks</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tranfrom</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sample</span>
</code></pre></div></p>
</details>
<details class="example">
<summary>Map-style Dataset: Get Image and its target</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</code></pre></div>
</details>
<details class="warning">
<summary>About Dataset Class</summary>
</details>
<details class="note">
<summary>data.TensorDataset</summary>
<p>Wrapping tensors as a dataset, you can then get each sample by indexing tensors along the first dimension.<br />
<div class="highlight"><pre><span></span><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)</span> <span class="c1"># the tensors need to have the same size of the first dimension</span>
</code></pre></div><br />
^TensorDataset</p>
</details>
<details class="example">
<summary>Create an iterable-style dataset with arrays</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
<span class="k">def</span> <span class="nf">load_array</span><span class="p">(</span><span class="n">data_arrays</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="o">*</span><span class="n">data_arrays</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">is_train</span><span class="p">)</span>
</code></pre></div>
</details>
<h4 id="datadataloader">data.DataLoader<a class="headerlink" href="#datadataloader" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>What is DataLoader</summary>
<div class="highlight"><pre><span></span><code><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
        <span class="n">batch_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">worker_init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">persistent_workers</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p>At the heart of PyTorch data loading utility is the <code>torch.utils.data.DataLoader</code> class, which represents a Python iterable over a dataset (combines a dataset and a sampler)</p>
<ul>
<li>dataset(Dataset)</li>
<li>batch_size(int,opt,<code>1</code>):</li>
<li>shuffle(bool,opt,<code>False</code>): set to <code>True</code> to reshuffle the data every epoch</li>
<li>sampler(Sampler or Iterable,opt,<code>None</code>): define the method to get samples from the dataset. ❗If specified, <code>shuffle</code> must <strong>not</strong> be specified</li>
</ul>
</details>
<details class="example">
<summary>Load Fashion-MNIST Dataset</summary>
<p>import torch<br />
import torchvision<br />
from torch.utils import data<br />
from torchvision import transforms<br />
mnist_train = torchvision.datasets<br />
^FashionMNIST</p>
</details>
<h4 id="datasampler">data.Sampler<a class="headerlink" href="#datasampler" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>data.Sampler(data_source=None)</summary>
<p>Base class for all Samples</p>
<p>Every Sampler subclass has to provide an <code>__iter__()</code> method, which is a way to ierate over indices or <u>list of indices</u> (for batches) of dataset elements, and a <code>__len__()</code> method that returns the length of the returned iterators<br />
^sampler</p>
</details>
<details class="note">
<summary>data.random_split</summary>
<p><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span>
            <span class="n">generator</span><span class="o">=&lt;</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">Generator</span> <span class="nb">object</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div><br />
Split a dataset into non-overlapping new datasets. Note that <code>random_split</code> will return <code>Dataset</code> object, so you can then use <code>DataLoader</code> to process <code>Dataset</code></p>
<ul>
<li><code>generator</code>: <a href="#^Generator">Check here</a></li>
<li><code>lengths</code>: a sequence (lengths or fractions of splits to be produced)</li>
</ul>
<p><div class="highlight"><pre><span></span><code><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.7</span><span class="p">],</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">train_iter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
</code></pre></div><br />
^RandomSplit</p>
</details>
<details class="example">
<summary>Split a dataset</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets">stackoverflow</a><br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">test_size</span><span class="p">])</span>
</code></pre></div></p>
</details>
<h3 id="torchnn">torch.nn<a class="headerlink" href="#torchnn" title="Permanent link">&para;</a></h3>
<details class="note">
<summary>nn.Module</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">Documentation</a><br />
base class for all neural network modules</p>
<ul>
<li>
<p><code>apply(fn)</code>: Applies function <strong>recursively</strong> to <u>every submodule</u> as well as self.<br />
<div class="highlight"><pre><span></span><code><span class="c1"># initialize the parameters</span>
<span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequentail</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span> <span class="c1"># this will initialize nn.Linear(4,4) and nn.Linear(2,2) parameters</span>
</code></pre></div></p>
</li>
<li>
<p><code>zero_grad(set_to_none=False)</code>: Reset gradients of all model parameters.</p>
</li>
<li><code>eval()</code>: Set the module in evaluation mode, which is equivalent to `self.train(False)</li>
</ul>
<details class="note">
<summary>Cases using <code>eval()</code></summary>
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;Batch normalization&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Dropout&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;Forbid gradient calculation&#39;&#39;&#39;</span>
</code></pre></div>
</details>
<ul>
<li><code>children()</code>: Returns an iterator over <strong>immediate</strong> children modules. Here's an example:<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ParentNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParentNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bastard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">],[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]])</span> <span class="c1"># this will not be registered as the module&#39;s child</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bastard</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="nb">input</span><span class="p">))))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ParentNetwork</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;reset_parameters&#39;</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
</code></pre></div><br />
About <code>module.train()</code> and <code>module.eval()</code></li>
</ul>
<details class="question">
<summary>Why do we need <code>module.train()</code>/<code>module.eval()</code>?</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch">stackoverflow</a></p>
</details>
<p>You can define names for submodules:<br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">NN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">diy_add_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submodules</span><span class="p">,</span> <span class="n">names</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">submodules</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">submodule</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">who_gonna_do</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">who_gonna_do</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">](</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">My_nn</span> <span class="o">=</span> <span class="n">NN</span><span class="p">()</span>
<span class="n">My_nn</span><span class="o">.</span><span class="n">diy_add_modules</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span><span class="s1">&#39;B&#39;</span><span class="p">,</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="s1">&#39;D&#39;</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">My_nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">])</span>
<span class="n">My_nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="s1">&#39;A&#39;</span><span class="p">,</span><span class="s1">&#39;D&#39;</span><span class="p">])</span>

<span class="c1"># PyTorch also provides you with add_modules() method</span>
<span class="n">My_nn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="s1">&#39;A_&#39;</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>add_module(name, module)</code></li>
<li><code>modules()</code>: Returns an iterator over all modules.</li>
</ul>
<details class="question">
<summary><code>nn.ModuleDict</code>, <code>OrderedDict</code> and <code>dict</code></summary>
<p>Sources: <a href="https://discuss.pytorch.org/t/nn-moduledict-vs-ordereddict-vs-dict/145418">PyTorch Forum</a><br />
Note that submodules are <u>registered</u> using <code>_modules</code>.</p>
</details>
<details class="question">
<summary>Difference between <code>.module()</code> and <code>.children()</code> &amp; Remove NN layers</summary>
<p><a href="https://discuss.pytorch.org/t/module-children-vs-module-modules/4551">Forum</a><br />
<div class="highlight"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()))</span>
<span class="sd">&#39;&#39;&#39;.modules() will recursively go into all modules in the network&#39;&#39;&#39;</span>
<span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">())</span>
<span class="sd">&#39;&#39;&#39;[Sequential(</span>
<span class="sd">(0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="sd">(1): Sequential(</span>
<span class="sd">    (0): Sigmoid()</span>
<span class="sd">    (1): ReLU()</span>
<span class="sd">    )</span>
<span class="sd">), Linear(in_features=2, out_features=2, bias=True), Sequential(</span>
<span class="sd">    (0): Sigmoid()</span>
<span class="sd">    (1): ReLU()</span>
<span class="sd">), Sigmoid(), ReLU()]&#39;&#39;&#39;</span>

<span class="sd">&#39;&#39;&#39;.children() will not go into the submodule&#39;&#39;&#39;</span>
<span class="nb">list</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span><span class="o">.</span><span class="n">children</span><span class="p">())</span>
<span class="sd">&#39;&#39;&#39;[Linear(in_features=2, out_features=2, bias=True), Sequential(</span>
<span class="sd">    (0): Sigmoid()</span>
<span class="sd">    (1): ReLU()</span>
<span class="sd">)]&#39;&#39;&#39;</span>
</code></pre></div></p>
</details>
<ul>
<li><code>parameters(recurse=True)</code>: Returns an iterator over module parameters</li>
</ul>
<details class="question">
<summary>how to reset a module's parameters?</summary>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;reset_parameters&#39;</span><span class="p">):</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div>
</details>
<ul>
<li><code>state_dict()</code> Returns an OrderedDict containing references to the whole state of a module.</li>
<li><code>register_buffer(name, tensor, persistent)</code>: Adds a buffer to the module</li>
</ul>
<details class="question">
<summary>Difference between <code>register_buffer</code> and <code>register_parameter</code>?</summary>
<p><a href="https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723">Forum</a></p>
</details>
<ul>
<li><code>cpu()</code>: move all model parameters and buffer to CPU</li>
<li><code>cuda(device=None)</code>: move all model parameters and buffer to GPU<br />
^Module</li>
</ul>
</details>
<details class="question">
<summary>How to save and load my Module/Tensor?</summary>
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;save &amp; load tensor&#39;&#39;&#39;</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="s1">&#39;......&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;path/name.pt&#39;</span><span class="p">)</span>
<span class="n">T_load</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/name.pt&#39;</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;save &amp; load tensor list&#39;&#39;&#39;</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span> <span class="s1">&#39;path</span><span class="se">\\</span><span class="s1">anyname&#39;</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path</span><span class="se">\\</span><span class="s1">anyname&#39;</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;save &amp; load dict&#39;&#39;&#39;</span>
<span class="n">mydict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;module&#39;</span><span class="p">:</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;param&#39;</span><span class="p">:</span><span class="n">param</span><span class="p">}</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">mydict</span><span class="p">,</span> <span class="s1">&#39;path/mydict.pt&#39;</span><span class="p">)</span>
<span class="n">mydict_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;save a moudle and its parameters&#39;&#39;&#39;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;module.params&#39;</span><span class="p">)</span>
<span class="n">clone</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">clone</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;module.params&#39;</span><span class="p">))</span>
</code></pre></div>
</details>
<details class="note">
<summary>nn.Sequential</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html">Documentation</a><br />
add modules in order<br />
You can see <code>nn.Sequential</code> as an ordered container.<br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="p">)</span>

<span class="c1"># functionally the same as above</span>
<span class="n">named_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
                <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                <span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
                <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="p">]))</span>

<span class="c1"># you can also append layers to a list, and then create a net using nn.Sequential</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

<span class="c1"># acquire specific layer</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
<span class="n">named_net</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="c1"># call by the name of the layer</span>
</code></pre></div></p>
</details>
<details class="example">
<summary>Build a simple Net: Toy MLP</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">ToyMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ToyMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<p>Another example</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ToyMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feature</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_output</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ToyMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feature</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_output</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</code></pre></div>
</details>
<details class="example">
<summary>Custom Your Network: A more flexible example</summary>
<p>Sources: <a href="https://zh.d2l.ai/chapter_convolutional-neural-networks/conv-layer.html">D2l</a></p>
</details>
<details class="note">
<summary>nn.ModuleList</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>append(module)</code>: appends a given module to the end of the list</li>
<li><code>extend(module)</code>: receive iterable of modules to append</li>
</ul>
</details>
<details class="note">
<summary>nn.Linear</summary>
<p>Linear Layer<br />
<span class="arithmatex">\(<span class="arithmatex">\(Linear\_layer(X) = W\cdot X + b\)</span>\)</span></p>
<p><code>torch.nn.Linear(in_features, out_features)</code><br />
Note that the input tensor's last dimension must be the same as the linear layer's first dimension<br />
i.e. <strong>Input Tensor</strong>: <span class="arithmatex">\(*\times2\)</span> -&gt; <strong>Linear Layer</strong>: <span class="arithmatex">\(2\times 3\)</span> -&gt; <strong>Output Tensor</strong>: <span class="arithmatex">\(*\times3\)</span><br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">lin_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span> <span class="c1"># you can simply think this as a function, with random parameters</span>

<span class="c1"># you can call the parameters and initialize them as you want</span>
<span class="n">lin_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
<span class="n">lin_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span>

<span class="n">lin_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lin_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">lin_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">lin_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>nn.LazyLinear</summary>
<div class="highlight"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="c1"># \&lt;UninitializedParameter&gt;</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># LazyLinear(in_features=0, out_features=4, bias=True)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># the framework will initialize sequentially</span>
</code></pre></div>
</details>
<details class="note">
<summary>nn.Sigmoid, torch.nn.ReLU:</summary>
<p>Common Activation Functions<br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">acti_sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">acti_Relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>nn.Softmax</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>input &amp; output: any number of additional dimensions, the shapes of input and output are same.</li>
<li><code>dim</code>(<code>int</code>): the dimension along which Softmax calculates.<br />
^Softmax</li>
</ul>
</details>
<details class="question">
<summary>How to copy my module?</summary>
<p>Python has <code>copy.deepcopy()</code> to handle this<br />
Sources: <a href="https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/">geeksforgeeks</a><br />
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">copy</span>
<span class="n">LN</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">LN_clone</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">LN</span><span class="p">)</span>
<span class="n">LN_clone</span> <span class="o">==</span> <span class="n">LN</span><span class="p">,</span> <span class="n">LN_clone</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="n">LN</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">LN_clone</span><span class="o">.</span><span class="n">bias</span> <span class="o">==</span> <span class="n">LN</span><span class="o">.</span><span class="n">bias</span>
<span class="c1"># the first will return False while others True</span>
</code></pre></div></p>
</details>
<details class="question">
<summary>Reproducibility?</summary>
<p>Sources: <a href="https://pytorch.org/docs/stable/notes/randomness.html">Documentation</a></p>
</details>
<h4 id="cnn">CNN<a class="headerlink" href="#cnn" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>nn.Conv2d</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span>
        <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>padding</code>: not that if you use <code>padding=2</code>, then all sides of the tensors will be added 2 rows(columns)<br />
^conv2d</li>
</ul>
</details>
<details class="note">
<summary>nn.MaxPool, nn.AvgPool</summary>
<p><div class="highlight"><pre><span></span><code><span class="c1"># PyTorch&#39;s MaxPool and AvgPool&#39;s output channels&#39; number is the same as input numbers</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span>

<span class="n">avg_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">avg_pool2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div><br />
^maxavgpool</p>
</details>
<details class="note">
<summary>nn.Flatten</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">strat_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="note">
<summary>nn.Unflatten</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">unflattened_size</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>dim</code>: the dimension of the input tensor to be unflattened</li>
</ul>
</details>
<details class="note">
<summary>nn.ConvTranspose2d</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> 
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">dialation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>nn.ModuleDict</summary>
<p>Sources: <a href="https://discuss.pytorch.org/t/get-key-of-nn-sequential-made-with-ordereddict/170249">PyTotch discussion</a></p>
</details>
<h4 id="rnn">RNN<a class="headerlink" href="#rnn" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>nn.RNN</summary>
<p>Sources: [Documentation], <a href="https://stackoverflow.com/questions/68220175/what-is-the-output-of-pytorch-rnn">stackoverflow</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">biadirctional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">Nonw</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div></p>
<ul>
<li>Note that all weights and biases are initialized from uniformed distribution: <span class="arithmatex">\(u(-\sqrt{k},\sqrt{k}),k=\frac{1}{\text{hidden\_size}}\)</span></li>
<li><code>batch_first</code>: If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). <u>This does not apply to hidden or cell states</u>.</li>
<li><strong>Return</strong>: 1) <code>output</code>: <span class="arithmatex">\((N,L,D*H_{out})\)</span> <span class="arithmatex">\(D=2\)</span> if <code>bidirectional=True</code> otherwise <code>1</code>; 2) <code>h_n</code>: tensor of shape <span class="arithmatex">\((D\times \text{num\_layers}, H_{out})\)</span> for unbatched input or <span class="arithmatex">\((D\times\text{num\_layers}, N, H_{out})\)</span> containing the latest hidden state.<br />
<div class="highlight"><pre><span></span><code><span class="n">rnn_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rnn_layer</span><span class="o">.</span><span class="n">all_weights</span>
<span class="n">rnn_layer</span><span class="o">.</span><span class="n">weight_ih_l0</span> <span class="c1"># ih for input-hidden, l0 for layer 0</span>
<span class="n">rnn_layer</span><span class="o">.</span><span class="n">bias_ih_l0</span>

<span class="n">rnn_layer</span><span class="o">.</span><span class="n">weight_hh_l0</span> <span class="c1"># hh for hidden-hidden</span>
<span class="n">rnn_layer</span><span class="o">.</span><span class="n">bias_hh_l0</span>

<span class="n">rnn_layer</span><span class="o">.</span><span class="n">weight_ih_l1</span> <span class="c1"># acquire the second layer</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">))</span>
</code></pre></div><br />
^RNN</li>
</ul>
</details>
<h4 id="attention">Attention<a class="headerlink" href="#attention" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>nn.Embedding</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">Documentation</a></p>
<ul>
<li>I/O: <ul>
<li>(*) The input should be IntTensor or LongTensor of <u>arbitrary shape</u>.</li>
<li>(*, H) H = <code>embedding_dim</code></li>
</ul>
</li>
<li>The weights initialized from <span class="arithmatex">\(\mathcal{N}(0,1)\)</span></li>
</ul>
<p>^Embedding</p>
</details>
<details class="note">
<summary>nn.MultiheadAttention</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_bias_kv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_zero_attn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">kdim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vdim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>embed-dim</code>: Total dimension of the <em>model</em></li>
<li><code>num_heads</code>: Each head will have dimension <code>embed_dim // numheads</code></li>
<li><code>bias</code>: Default:<code>True</code>. If specified, adds bias to input / output projection layers.</li>
<li><code>batch_first</code>: Defalut: <code>False</code> (<span class="arithmatex">\(\text{seq}, \text{batch}, \text{feature}\)</span>). If <code>True</code>, the input and output tensors are provided as <span class="arithmatex">\((\text{batch}, \text{seq}, \text{feature})\)</span></li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">forward</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">average_attn_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>query</code>:<ul>
<li><span class="arithmatex">\((\text{L}, \text{E}_q)\)</span> for unbatched input.</li>
<li><span class="arithmatex">\((\text{L}, \text{N}, \text{E}_q)\)</span> when <code>batch_first=False</code></li>
<li><span class="arithmatex">\((\text{N}, \text{L}, \text{E}_q)\)</span> when <code>batch_first=True</code></li>
</ul>
</li>
<li><code>key</code>:</li>
</ul>
<p>Examples: Singlehead Dot Scaled Attention<br />
<div class="highlight"><pre><span></span><code><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># batch, time steps, embedding&#39;s dimension</span>
<span class="n">Singlehead</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># be careful with batch_first</span>

<span class="sd">&#39;&#39;&#39;Single Attention Machinism&#39;&#39;&#39;</span>
<span class="n">Query</span> <span class="o">=</span> <span class="nb">input</span><span class="nd">@Singlehead</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">Singlehead</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span> 
<span class="c1"># note that the bias was initialized as 0 vector</span>
<span class="n">Key</span> <span class="o">=</span> <span class="nb">input</span><span class="nd">@Singlehead</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">[</span><span class="mi">4</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="n">Value</span> <span class="o">=</span> <span class="nb">input</span><span class="nd">@Singlehead</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">[</span><span class="mi">8</span><span class="p">:]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="n">Sa</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">Query</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Key</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">))</span>
<span class="n">Sa_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Sa</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Weighted_value</span> <span class="o">=</span> <span class="n">Sa_weight</span><span class="nd">@Value</span>
<span class="n">Output</span> <span class="o">=</span> <span class="n">Weighted_value</span><span class="nd">@Singlehead</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="n">a</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Singlehead</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">a</span> <span class="o">-</span> <span class="n">Output</span>
<span class="c1"># the output is really close to zero, but not equal to it, I don&#39;t see the reason </span>
</code></pre></div></p>
<ul>
<li><code>attn_mask</code>: if specified, a 2D/3D mask preventing attention to certain positions. <strong>Note: (1,X,X) won't broadcast!</strong><ul>
<li>Binary and float tensors are both supported. Binary: True to indicates that the position is <strong>not</strong> allowed to attend, while float: the value will be added to <u>the attention weight</u>.</li>
</ul>
</li>
<li><code>key_padding_mask</code>: A mask of shape (N, S) indicating which elements withn key to ignore for the purpose of attention. (i.e. ignore padding)</li>
</ul>
<details class="note">
<summary><code>attn_mask</code> and <code>key_padding_mask</code></summary>
<ul>
<li>key_padding_mask: </li>
</ul>
</details>
<p>^MultiheadAttention</p>
</details>
<details class="note">
<summary>nn.Transformer</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
        <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> 
        <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">activation</span><span class="o">=&lt;</span><span class="n">function</span> <span class="n">relu</span>\<span class="o">&gt;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> 
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">custom_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div><br />
An example: <a href="https://github.com/pytorch/examples/tree/main/word_language_model">Word Language Model</a><br />
<div class="highlight"><pre><span></span><code><span class="n">forward</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">src_is_causal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_is_causal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">memory_is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>scr</code>: <span class="arithmatex">\((S,E)\)</span> for unbatched input, <span class="arithmatex">\((N, S, E)\)</span> if <code>batch_first=True</code></li>
<li><code>tgt</code>: <span class="arithmatex">\((T,E)\)</span> ... <span class="arithmatex">\((N, T, E)\)</span> ...</li>
<li><code>src_mask</code>: <span class="arithmatex">\((S, S)\)</span> or <span class="arithmatex">\((N\cdot num_{heads}, S, S)\)</span></li>
<li><code>tgt_mask</code>: <span class="arithmatex">\((T, T)\)</span> or <span class="arithmatex">\((N\cdot num_{heads}, T, T)\)</span><br />
^Transformer</li>
</ul>
</details>
<h4 id="loss-functions">Loss Functions<a class="headerlink" href="#loss-functions" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>nn.MSELoss</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>reduction</code>: $$l(x,y)=\left{\begin{aligned}</li>
</ul>
</details>
<p>&amp;\frac{1}{N}\sum\limits_{n=1}^{N}l_n,\mbox{reduction='mean'}\<br />
&amp;\sum\limits_{n=1}^{N}l_n,\mbox{reduction='sum'}<br />
\end{aligned}\right.$$</p>
<blockquote>
<ul>
<li><code>reduce</code>: Deprecated</li>
<li><code>size_average</code>: Deprecated<br />
^MSELoss</li>
</ul>
</blockquote>
<details class="note">
<summary>nn.CrossEntropyLoss</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>weight</code>: if provided, the input should be a 1D tensor assigning weight to each of the classes. (useful when you have an unbalanced training set)</li>
<li>Note that the <strong>input</strong> has to contain the <u>unnormalized logits</u> for each class. The shape of input has to be <span class="arithmatex">\((\text{minibatch}, C)\)</span> or <span class="arithmatex">\((\text{minibatch}, C, d_1,d_2,\cdots,d_K)\)</span> (for the K-dimensional case, i.e. computing cross entropy loss per-pixel for 2D images).</li>
<li><code>reduction</code>: $$l(x,y)=\left{\begin{aligned}</li>
</ul>
</details>
<p>&amp;\frac{1}{N}\sum\limits_{n=1}^{N}l_n,\mbox{reduction='mean'}\<br />
&amp;\sum\limits_{n=1}^{N}l_n,\mbox{reduction='sum'}<br />
\end{aligned}\right.$$ If <code>reduction</code> is set to <code>none</code>, then for a <span class="arithmatex">\(N\)</span> batch size input, the loss function will return <span class="arithmatex">\(\{l_1,l_2,\cdots,l_N\}\)</span> for each sample's loss.</p>
<blockquote>
<p><div class="highlight"><pre><span></span><code><span class="n">criterion_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">criterion_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="n">criterion_none</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">17</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">])])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">criterion_mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_real</span><span class="p">)</span>
<span class="n">criterion_sum</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_real</span><span class="p">)</span>
<span class="n">criterion_none</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_real</span><span class="p">)</span>
</code></pre></div><br />
^CrossEntropyLoss</p>
</blockquote>
<details class="warning">
<summary><code>CorssEntropyLoss</code> do not calculate Cross Entropy Loss!</summary>
</details>
<details class="note">
<summary>nn.BCELoss</summary>
<p>Measures the Binary Cross between the <u>input probabilities</u>： <span class="arithmatex">\(<span class="arithmatex">\(l_n=-w_n[\hat{y}_n\log y_n+(1-\hat{y}_n)\log(1-y_n)]\)</span>\)</span></p>
</details>
<details class="note">
<summary>nn.BCEWithLogitsLoss</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">Documentation</a><br />
<span class="arithmatex">\(<span class="arithmatex">\(l_n=-w_n[y_n\log \sigma(x_n)+(1-y_n)\log(1-\sigma(x_n))]\)</span>\)</span></p>
</details>
<details class="question">
<summary>BCELoss vs BCEWithlogitsLoss</summary>
</details>
<details class="bug">
<summary>tensor(nan, grad_fn=\&lt;MseLossBackward&gt;)</summary>
<p>State: Unknown Reason<br />
Possible solution: <a href="https://www.reddit.com/r/pytorch/comments/dd0o8w/tensornan_grad_fnmselossbackward_when_training_cnn/">Raddit</a><br />
to repeat the bug:</p>
</details>
<h4 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>nn.Dropout</summary>
<p><a href="">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>inplace</code>: if set <code>True</code> this will do this operation in-place.<br />
^Dropout</li>
</ul>
</details>
<details class="note">
<summary>nn.BatchNorm2d</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div><br />
Applies Batch Normalization over <strong>a 4D input</strong> (and 2d refers to image(2d)) <br />
<div class="highlight"><pre><span></span><code><span class="n">BN</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># set channel&#39;s number to 1</span>
<span class="n">N1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">N2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">N1</span><span class="p">,</span> <span class="n">N2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">BN</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</code></pre></div></p>
</details>
<details class="note">
<summary>nn.LayerNorm</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalizaed_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-0.5</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>The mean and strandard-deviation are calculated over the last D dimension<strong>s</strong> (<code>normalizaed_shape</code>)<ul>
<li>The standard-deviation is calculated via the biased estimateor, equivalent to <code>torch.var(input, unbiased=False)</code></li>
</ul>
</li>
<li><code>elementwise_affine</code>: set <code>True</code> to use <span class="arithmatex">\(\gamma\)</span> and <span class="arithmatex">\(\beta\)</span> to shift mean and standard-deviation.<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">]]])</span> <span class="c1"># X.shape: (1,2,2)</span>
<span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">LayerNorm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">correction</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 

<span class="n">LayerNorm_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">LayerNorm_</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></li>
</ul>
</details>
<h4 id="nnfunctional">nn.functional<a class="headerlink" href="#nnfunctional" title="Permanent link">&para;</a></h4>
<details class="question">
<summary>Difference between torch.nn.functional and torch.nn?</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/63826328/torch-nn-functional-vs-torch-nn-pytorch">Stackexchange</a></p>
<ul>
<li><code>nn.functional.xxx</code>: You'll need to handle the parameters yourself (passing them to the optimizer or moving them to the GPU)</li>
<li><code>nn.xxx</code>: easy to handle parameters with <code>net.parameters()</code>, <code>net.to(device)</code> etc.</li>
<li>You can see <code>nn.functional.xxx</code> as a more flexible function than <code>nn.xxx</code>. </li>
</ul>
</details>
<details class="note">
<summary>functional.softmax</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="note">
<summary>functional.one_hot</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>num_classes</code>: set to <code>-1</code>, then the number of classes will be greater than the largest value in the input.<br />
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></li>
</ul>
</details>
<h4 id="nnparameter">nn.parameter<a class="headerlink" href="#nnparameter" title="Permanent link">&para;</a></h4>
<details class="question">
<summary>Why do we need nn.parameter.Parameter?</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter">stackoverflow</a></p>
<ul>
<li><code>Tensor</code>s are multi-dimensional matrices, while parameters are <code>Tensor</code> subclasses.When a parameter is associated with a module as its attribute, it will automatically be added to the parameter list of the module, and can be accessed using the <code>.parameters()</code> iterator <strong>???</strong>. </li>
<li>This comes with a <br />
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">simple_function</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">simple_function</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">,</span><span class="mf">6.</span><span class="p">))</span> <span class="c1"># by default, the parameter will be set `requires_grad=True`</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight1</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight0</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span>

<span class="n">sf</span> <span class="o">=</span> <span class="n">simple_function</span><span class="p">()</span>
<span class="n">sf</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">sf</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="p">)</span>

<span class="c1"># return &lt;class &#39;torch.Tensor&#39;&gt; Parameter containing: tensor([5., 6.], requires_grad=True)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">sf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="c1"># tensor(12., grad_fn=\&lt;AddBackward0&gt;)</span>

<span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">sf</span><span class="o">.</span><span class="n">weight1</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># tensor([1., 1.])</span>
<span class="n">sf</span><span class="o">.</span><span class="n">weight0</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># tensor([0.5000, 0.5000])</span>
</code></pre></div></li>
</ul>
</details>
<details class="note">
<summary>parameter.Parameter</summary>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><br />
A kind of tensor used as a module parameter. Parameters </p>
</details>
<details class="note">
<summary>nn.init</summary>
<p><a href="https://pytorch.org/docs/stable/nn.init.html">Documentation</a><br />
<div class="highlight"><pre><span></span><code><span class="sd">&#39;&#39;&#39;Normal distribution&#39;&#39;&#39;</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># default 0.0, 1.0</span>

<span class="sd">&#39;&#39;&#39;Constant&#39;&#39;&#39;</span>
<span class="n">LinearNet</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;Uniform distribution&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">init_uniform</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">isintance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">net</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">LinearNet</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_uniform</span><span class="p">)</span>
</code></pre></div><br />
Xavier Uniform: generated tensor from <span class="arithmatex">\(<span class="arithmatex">\(u(-a,a), a=\text{gain}\times \sqrt{\frac{6}{\text{fan\_in}+\text{fan\_out}}}\)</span>\)</span><br />
<div class="highlight"><pre><span></span><code><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</code></pre></div><br />
^init</p>
</details>
<h3 id="torchoptim">torch.optim<a class="headerlink" href="#torchoptim" title="Permanent link">&para;</a></h3>
<p><a href="https://pytorch.org/docs/stable/optim.html">Documentation</a></p>
<details class="note">
<summary>optim.Optimizer:</summary>
<p>base class for all optimizers<br />
<code>torch.optim.Optimizer(params, defaults)</code></p>
<ul>
<li><code>param</code> is an iterable of <code>torch.Tensor</code>s or <code>dict</code>s</li>
<li>
<p>a dict containing default values of optimization options</p>
</li>
<li>
<p><code>Optimizer.zero_grad(set_to_none=True)</code>: Resets the gradient of all optimized <code>torch.Tensor</code>s<br />
    -set to none(bool): set the grads to <code>None</code> instead of to zero.</p>
</li>
</ul>
</details>
<details class="question">
<summary>Why do we need to call zero_grad?</summary>
<p>Sources: <a href="https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch">stackoverflow</a></p>
<p>PyTroch accumulates the gradients on subsequent backward passes, which is useful when we want to sum the whole loss summed over multiple batches or training RNNs.<br />
^whyzerograd</p>
</details>
<details class="note">
<summary>optim.SGD</summary>
<p>Stomatic Gradient Descent<br />
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">foreach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">differentiable</span><span class="o">=</span><span class="kc">False</span> <span class="p">)</span>
</code></pre></div></p>
<ul>
<li><code>momentum</code>: momentum factor <span class="arithmatex">\(\mu\)</span><ul>
<li><code>damplening</code>: dampening <span class="arithmatex">\(\tau\)</span> for momentum: </li>
<li><a href="../../Math/DL/#^Momentum">Check here</a></li>
</ul>
</li>
<li><code>params</code>: iterable of parameters to optimize or dicts<br />
<div class="highlight"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">}],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="c1"># although all net.parameters are set to require gradient, you can choose which to update using optimizer</span>
<span class="n">rand_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rand_samples</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Round</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">rand_samples</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">Y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div><br />
^SGD</li>
</ul>
</details>
<details class="note">
<summary>optim.Adam</summary>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-0.8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">capturable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>amsgrad</code>: AMSGrad vairant of this algorithm <a href="https://openreview.net/pdf?id=ryQu7f-RZ">Check</a><br />
^Adam</li>
</ul>
</details>
<details class="example">
<summary>Change the learning rate based on number of epochs</summary>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.optim.lr_scheduler.StepLR</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
</details>
<h3 id="more_1">More<a class="headerlink" href="#more_1" title="Permanent link">&para;</a></h3>
<details class="note">
<summary>torch.multiprocessing</summary>
<div class="highlight"><pre><span></span><code>
</code></pre></div>
</details>
<h3 id="torchvision">torchvision<a class="headerlink" href="#torchvision" title="Permanent link">&para;</a></h3>
<h4 id="transforms">transforms<a class="headerlink" href="#transforms" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>transforms</summary>
<p>All transformations accept <u>PIL Image</u>, <u>Tensor Image</u> <span class="arithmatex">\((C,H,W)\)</span> or <u>batch of Tensor Images</u> <span class="arithmatex">\((B,C,H,W)\)</span> as input.</p>
</details>
<h3 id="torchtext">torchtext<a class="headerlink" href="#torchtext" title="Permanent link">&para;</a></h3>
<h4 id="torchtextdatautils">torchtext.data.utils<a class="headerlink" href="#torchtextdatautils" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>utils.get_tokenizer</summary>
<div class="highlight"><pre><span></span><code><span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">&#39;en&#39;</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>tokenizer</code>: if <code>None</code>, if returns <code>split()</code> function (by space)<ul>
<li><code>basic_english</code>: return <code>_basic_english_normalize()</code> function, first <u>normalizing</u> the string and then spliting it by space.</li>
</ul>
</li>
</ul>
</details>
<h4 id="torchtextvocab">torchtext.vocab<a class="headerlink" href="#torchtextvocab" title="Permanent link">&para;</a></h4>
<details class="note">
<summary>vocab.Vocab</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">vocab</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">OrderedDict</span>
</code></pre></div>
</details>
<h2 id="unsorted-notes">Unsorted Notes<a class="headerlink" href="#unsorted-notes" title="Permanent link">&para;</a></h2>
<details class="note">
<summary>The Busy Person's Intro to LLMs</summary>
<p><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Sources</a></p>
<details class="note">
<summary>Model Inference: What is a LLM?</summary>
<p>Two Files (llama-2-70b by Meta AI): llama series - 2nd iteration of it - 70 billion parameters - All access to the paper, parameter, architecture.</p>
<ul>
<li>Parameters: <span class="arithmatex">\(140\)</span>GB<ul>
<li>every one of those parameters is stored as 2 bytes (float 16 number)</li>
</ul>
</li>
<li>run.c <span class="arithmatex">\(\sim500\)</span> lines of C code</li>
</ul>
<p>You only need a device...(don't need network)</p>
</details>
<details class="note">
<summary>Model Training: How do we get the parameters?</summary>
<ul>
<li>Chunk of the internet (<span class="arithmatex">\(\sim10\)</span>TB of text)</li>
<li><span class="arithmatex">\(6000\)</span> GPUs for <span class="arithmatex">\(12\)</span> days, <span class="arithmatex">\(\sim\)</span>$2M <span class="arithmatex">\(\sim1e24\)</span> FLOPs (floating-point operations per second)<ul>
<li>like compression the chunks into a 'zip' file (but we don't have the chunks)</li>
<li>this are only <strong>rookie numbers</strong>, the state of the art models by <span class="arithmatex">\(10\)</span> or more...</li>
</ul>
</li>
</ul>
<p>The LLM is simply predicting the next word in the sequence</p>
<p>Next word prediction <strong>forces</strong> the neural network to learn a lot about the world<br />
[[DLVisual]]</p>
</details>
<details class="note">
<summary>LLM 'Dreams' / Hallucination</summary>
<ul>
<li>Fake Links</li>
<li>Fake Codes</li>
<li>...<br />
It just puts in what every it 'thinks' reasonable</li>
</ul>
</details>
<details class="note">
<summary>How do they work?</summary>
<p>We know every math operation. But <u>little is known in full detail</u></p>
<ul>
<li>We can measure that this works, but we don't really know how the billions collaborate to do it. (Or equally, we do not actually how to rectify parameters precisely to make it work, we just 'train' it, like human teachers)</li>
<li>The model's database is strange: reversal curse (one-dimensional?)</li>
</ul>
</details>
<details class="note">
<summary>Fine tuning: Training the Assistant</summary>
<p>Training will be the same, but with different datasets. </p>
<p>First you write labeling instructions, and then hire people to write ideal Q&amp;A responses, after that you have the dataset, and use it to train your dataset (fine-tuning), and deplot the model. When model running, you will get misbehaviours, then you can fine-tuning again (let people write the right Q&amp;A responses and feed it into the dataset)</p>
<ul>
<li><span class="arithmatex">\(\sim100K\)</span> conversations (people write: questions and ideal answers)</li>
<li>Quality over quantity<br />
After fine tuning you have the Assistant model.</li>
</ul>
<p>The model somehow still have access to the first-state (pretraining) knowledge.</p>
<ul>
<li>Another way to fine-tuning: compare answers and feed back.</li>
</ul>
</details>
<details class="note">
<summary>LLM Scaling Laws</summary>
<p>DO NOT SHOW SIGNS OF TOPPING OUT</p>
</details>
<details class="note">
<summary>LLMs Use tools</summary>
<ul>
<li>Browser</li>
<li>Calculator</li>
<li>Python Interpretor</li>
<li>Vision: See and Generate images</li>
<li>Audio: Hear and Speak</li>
</ul>
</details>
<details class="note">
<summary>Future</summary>
<p>LLMs only have instinctive part, cannot think reasonably.</p>
<p>Take 30minutes thinking</p>
<p>Create <u>tree of thoughts</u>.</p>
<details class="note">
<summary><strong>Self-Imporvement</strong></summary>
<p>AlphaGo</p>
</details>
</details>
<blockquote>
<p>What does the step 2 look like in LLMs? Lack of reward criterion.</p>
</blockquote>
<details class="note">
<summary>Jailbreak</summary>
</details>
<details class="note">
<summary>Data poisoning and Backdoor attacks</summary>
</details>
</details>
<h2 id="hugging-face">Hugging Face<a class="headerlink" href="#hugging-face" title="Permanent link">&para;</a></h2>
<details class="note">
<summary>pipeline</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">)</span>
<span class="n">classifier</span><span class="p">([</span><span class="s1">&#39;I miss you&#39;</span><span class="p">,</span> <span class="s1">&#39;I just miss your so much. I guess&#39;</span><span class="p">])</span>
</code></pre></div>
<ul>
<li>by default, the pipeline selects a particular trained model. The model is downloaded and cached.<br />
^pipeline</li>
</ul>
</details>
<details class="example">
<summary>Zero-shot classification</summary>
<div class="highlight"><pre><span></span><code><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;zero-shot-classification&#39;</span><span class="p">)</span>
<span class="n">classifier</span><span class="p">(</span>
    <span class="s2">&quot;this is my life&quot;</span><span class="p">,</span>
    <span class="n">candidate_labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;education&#39;</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<ul>
<li>You don't need to fine=tune the model on your data to use it.</li>
</ul>
</details>
<details class="example">
<summary>Text Generation</summary>
<div class="highlight"><pre><span></span><code><span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;text-generation&#39;</span><span class="p">)</span>
<span class="n">generator</span><span class="p">(</span><span class="s1">&#39;I miss you&#39;</span><span class="p">)</span>

<span class="c1"># specify a model</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;text-generation&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;distilgpt2&#39;</span><span class="p">)</span>
<span class="n">generator</span><span class="p">(</span>
    <span class="s1">&#39;I miss you.&#39;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
</details>
<details class="note">
<summary>Save models</summary>
</details>
<p><center><span style="font-family: Brush Script MT; font-size: 3em;">Stay Hungry, Stay Foolish</span></center></p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="最后更新">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2024年4月6日</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="创建日期">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3h-2Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2024年4月6日</span>
  </span>

    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../../Coding/Python/" class="md-footer__link md-footer__link--prev" aria-label="上一页: Python">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                Python
              </div>
            </div>
          </a>
        
        
          
          <a href="../PyDCB/" class="md-footer__link md-footer__link--next" aria-label="下一页: Python DM &amp; ML">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Python DM & ML
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy 2024 zoeminus
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:2210377@mail.nankai.edu.cn" target="_blank" rel="noopener" title="Email" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.7l167.6-182.9c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/zoeplus" target="_blank" rel="noopener" title="Github Profile" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["revision.date", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.action.edit", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.bd41221c.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="../../javascripts/tabSync.js"></script>
      
    
  </body>
</html>